<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
<meta name="google-site-verification" content="L0ZN3xykszl3pBQeJ5QFwk98KKJ8kHDzGSBLhEtwmNU" />
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"kimh060612.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="본 포스트는 Hands-on Machine learning 2nd Edition, CS231n, Tensorflow 공식 document를 참조하여 작성되었음을 알립니다. Index Basic of Neural Network   Definition of Fully Connected Neural Network(FCNN) Feed Forward of FCNN">
<meta property="og:type" content="article">
<meta property="og:title" content="Fully Connected Neural Network 강의 내용 part.B">
<meta property="og:url" content="https://kimh060612.github.io/2022/03/05/FCNN-B/index.html">
<meta property="og:site_name" content="Michael&#39;s Study Blog">
<meta property="og:description" content="본 포스트는 Hands-on Machine learning 2nd Edition, CS231n, Tensorflow 공식 document를 참조하여 작성되었음을 알립니다. Index Basic of Neural Network   Definition of Fully Connected Neural Network(FCNN) Feed Forward of FCNN">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://kimh060612.github.io/img/API.png">
<meta property="article:published_time" content="2022-03-05T08:33:29.000Z">
<meta property="article:modified_time" content="2022-05-29T15:00:32.840Z">
<meta property="article:author" content="Michael Kim">
<meta property="article:tag" content="Deep Learning">
<meta property="article:tag" content="Tensorflow2">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://kimh060612.github.io/img/API.png">

<link rel="canonical" href="https://kimh060612.github.io/2022/03/05/FCNN-B/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Fully Connected Neural Network 강의 내용 part.B | Michael's Study Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><link rel="alternate" href="/rss2.xml" title="Michael's Study Blog" type="application/rss+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Michael's Study Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Backend Engineering & Deep learning!</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>Tags<span class="badge">15</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>Categories<span class="badge">7</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>Archives<span class="badge">23</span></a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kimh060612.github.io/2022/03/05/FCNN-B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Michael Kim">
      <meta itemprop="description" content="Backend Engineering, Deep learning, Algorithm & DS, CS 전공 지식 공부 저장소">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Michael's Study Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Fully Connected Neural Network 강의 내용 part.B
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-03-05 17:33:29" itemprop="dateCreated datePublished" datetime="2022-03-05T17:33:29+09:00">2022-03-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-05-30 00:00:32" itemprop="dateModified" datetime="2022-05-30T00:00:32+09:00">2022-05-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2022/03/05/FCNN-B/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2022/03/05/FCNN-B/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><em>본 포스트는 Hands-on Machine learning 2nd Edition, CS231n, Tensorflow 공식 document를 참조하여 작성되었음을 알립니다.</em></p>
<h6 id="Index"><a href="#Index" class="headerlink" title="Index"></a>Index</h6><ol>
<li><del>Basic of Neural Network</del>  </li>
<li><del>Definition of Fully Connected Neural Network(FCNN)</del></li>
<li><del>Feed Forward of FCNN</del></li>
<li><del>Gradient Descent</del></li>
<li><del>Back Propagation of FCNN</del></li>
<li>Partice(+ 부록 Hyper parameter tuning)</li>
</ol>
<p><br></p>
<h4 id="6-Partice-부록-Hyper-parameter-tuning"><a href="#6-Partice-부록-Hyper-parameter-tuning" class="headerlink" title="6. Partice(+ 부록 Hyper parameter tuning)"></a>6. Partice(+ 부록 Hyper parameter tuning)</h4><hr>
<p>자 실습 시간이다. 왜 실습을 Part. B로 뺐느냐? FCNN이 뭐 할게 있다고?<br>뭐 할게 있겠다. Tensorflow 2가 대충 어떻게 이루어 졌는지 설명하기 위해 분량 조절을 위해서 뺀것이다.<br>무엇보다 Part. A 쓰는데 수식을 너무 많이 써서 힘들어서 분리했다. <del>Tlqkf</del><br>자, 우선 실습에 들어가기에 앞서, TF 2를 애정하는 나로써는 앞으로 이 스터디 포스트에 작성될 대부분의 소스코드를 꿰뚫는 구현 체계를 먼저 설명하고 넘어가겠다.<br>다음 사진을 보자.</p>
<p align="center"><img src="http://kimh060612.github.io/img/API.png" width="70%" height="70%"></p>

<p><em>출처: pyimagesearch blog: <a target="_blank" rel="noopener" href="https://www.pyimagesearch.com/2019/10/28/3-ways-to-create-a-keras-model-with-tensorflow-2-0-sequential-functional-and-model-subclassing/">링크</a></em></p>
<p>위 그림에서 필자는 대부분의 코드를 <strong>Model Subclassing</strong> 방식으로 구현할 것이다. 구현 하면서 설명할 터이니 잘 따라와 주기를 바란다.<br>여기서부터는 대학교 강의 수준의 <strong>객체지향프로그래밍</strong> 지식을 갖추지 않으면 읽기 힘들 수 있다. “상속”, “오버라이딩”의 개념이라도 살펴보고 오자.</p>
<h6 id="6-1-Model-Subclassing"><a href="#6-1-Model-Subclassing" class="headerlink" title="6-1. Model Subclassing"></a>6-1. Model Subclassing</h6><hr>
<p>우선 준비한 소스부터 보고 시작하자.<br><em>file: model/FCNN1.py</em><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FCNN</span>(keras.Model):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, _units = [<span class="number">56</span>, <span class="number">56</span>], _activation = [<span class="string">&#x27;relu&#x27;</span>, <span class="string">&#x27;relu&#x27;</span>], **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)</span><br><span class="line">        self.Hidden1 = keras.layers.Dense(units=_units[<span class="number">0</span>], activation=_activation[<span class="number">0</span>], kernel_initializer=<span class="string">&quot;normal&quot;</span>)</span><br><span class="line">        self.Hidden2 = keras.layers.Dense(units=_units[<span class="number">1</span>], activation=_activation[<span class="number">1</span>], kernel_initializer=<span class="string">&quot;normal&quot;</span>)</span><br><span class="line">        self._output = keras.layers.Dense(units=<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, Input</span>):</span><br><span class="line">        hidden1 = self.Hidden1(Input)</span><br><span class="line">        hidden2 = self.Hidden2(hidden1)</span><br><span class="line">        Output = self._output(hidden2)</span><br><span class="line">        <span class="keyword">return</span> Output</span><br></pre></td></tr></table></figure><br>자, 별거 없다. keras를 써봤다면 뭔지 바로 감이 올 것이다.<br>이 부분에 대해서는 함수에 대한 설명보다는 class에 대한 설명을 해야할 것 같다. 바로 FCNN class가 상속을 받은 부모 클래스인 keras.Model 클래스에 관해서이다.</p>
<p>keras.Model class는 케라스에서 Deep learning을 진행하는 모델을 정의해주는 class이다. 우리가 이미 존재하는 layer를 가져다가 특정 순서로 연산을 진행하는 graph를 만들어 내기 위한 class이다. 하지만 그렇게 어렵게 생각하지 말자. 사용하는 것을 보면 바로 답이 나온다.</p>
<p>여기서는 생성자와 call이라는 함수를 오버라이딩을 통해서 사용자가 재 정의를 해서 사용한다. call은 우리가 구현하고자 하는 model이 feed forward를 진행할때 호출되는 함수이다. 생성자는 사용할 폭이 넓다. 여기서는 model을 구성하는 layer를 정의하는데 사용하였는데, 꼭 그 역할만 할 필요는 없는 것이다.</p>
<p>가타부타 말이 많았는데, 실제 어떻게 동작을 시키는가?</p>
<p><em>file: train_MNIST.py</em><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> model.FCNN1 <span class="keyword">import</span> FCNN</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">mnist = keras.datasets.mnist</span><br><span class="line"></span><br><span class="line">(train_img, train_labels), (test_img, test_labels) = mnist.load_data()</span><br><span class="line"></span><br><span class="line">train_img, test_img = train_img.reshape([-<span class="number">1</span>, <span class="number">784</span>]), test_img.reshape([-<span class="number">1</span>, <span class="number">784</span>])</span><br><span class="line"></span><br><span class="line">train_img = train_img.astype(np.float32) / <span class="number">255.</span></span><br><span class="line">test_img = test_img.astype(np.float32) / <span class="number">255.</span></span><br><span class="line"></span><br><span class="line">model = FCNN()</span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adam&#x27;</span>, loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>, metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br><span class="line">model.fit(train_img, train_labels, batch_size = <span class="number">32</span>, epochs = <span class="number">15</span>, verbose = <span class="number">1</span>, validation_split = <span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line">model.evaluate(test_img, test_labels, verbose=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><br>간단하다. keras.Model class를 상속 받았으니, 그곳에 있는 기본 함수들을 모두 사용할 수있다. fit method로 학습을 진행하고 evalute로 test데이터로 모델을 평가한다.<br>이는 기존에 keras의 사용법과 별반 다른게 없다.</p>
<p>여기까지는 쉽다. 하지만 이러고 끝낼거면 시작도 하지 않았다.</p>
<p>다음을 진짜 자세하게 설명할 것이다.</p>
<p><em>file: model/layer.py</em><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line"><span class="comment"># Weight Sum을 진행하는 Layer를 정의하는 예제 1</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">WeightSum</span>(keras.layers.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, units=<span class="number">32</span>, input_dim=<span class="number">32</span>, trainable=<span class="literal">True</span>, name=<span class="literal">None</span>, dtype=<span class="literal">None</span>, dynamic=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(trainable=trainable, name=name, dtype=dtype, dynamic=dynamic, **kwargs)</span><br><span class="line">        <span class="comment"># tf.Variable을 활용하는 예시</span></span><br><span class="line">        <span class="comment"># 가중치 행렬을 초기화 하기 위한 객체</span></span><br><span class="line">        w_init = tf.random_normal_initializer()</span><br><span class="line">        <span class="comment"># 가중치 행렬을 tf.Variable로 정의함. 당연히 학습해야하니 training parameter를 true로 둠.</span></span><br><span class="line">        self.Weight = tf.Variable(</span><br><span class="line">            initial_value = w_init(shape=(input_dim, units), dtype=<span class="string">&quot;float32&quot;</span>),</span><br><span class="line">            trainable = <span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line">        b_init = tf.zeros_initializer()</span><br><span class="line">        <span class="comment"># 위랑 같음. 별반 다를거 X</span></span><br><span class="line">        self.Bias = tf.Variable(</span><br><span class="line">            initial_value = b_init(shape=(units, ), dtype=<span class="string">&quot;float32&quot;</span>),</span><br><span class="line">            trainable = <span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># add_weight를 활용하는 예시 - Short Cut</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        # 이는 keras.layers.Layer의 method 중에서 add_weight를 사용하는 방법임. </span></span><br><span class="line"><span class="string">        # 주로 training을 시키기 위한 행렬을 이렇게 선언해서 나중에 편하게 불러오기 위한 목적이 큼. </span></span><br><span class="line"><span class="string">        self.Weight = self.add_weight(</span></span><br><span class="line"><span class="string">            shape=(input_dim, units), initializer=&quot;random_normal&quot;, trainable=True</span></span><br><span class="line"><span class="string">        )</span></span><br><span class="line"><span class="string">        self.Bias = self.add_weight(shape=(units,), initializer=&quot;zeros&quot;, trainable=True)</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, Input</span>):</span><br><span class="line">        <span class="comment"># 행렬 곱을 위한 tf 함수임. 별거 없음</span></span><br><span class="line">        <span class="comment"># 그냥 U = WZ + B 구현한거. </span></span><br><span class="line">        <span class="keyword">return</span> tf.matmul(Input, self.Weight) + self.Bias</span><br><span class="line"></span><br><span class="line"><span class="comment"># Weight Sum을 진행하는 Layer를 정의하는 예제 2</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">WeightSumBuild</span>(keras.layers.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, _units = <span class="number">32</span>, trainable=<span class="literal">True</span>, name=<span class="literal">None</span>, dtype=<span class="literal">None</span>, dynamic=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(trainable=trainable, name=name, dtype=dtype, dynamic=dynamic, **kwargs)</span><br><span class="line">        self.units = _units</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build</span>(<span class="params">self, input_dim</span>):</span><br><span class="line">        <span class="comment"># 이 함수는 밑에서 자세히 설명함.</span></span><br><span class="line">        self.Weight = self.add_weight(</span><br><span class="line">            shape=(input_dim[-<span class="number">1</span>], self.units),</span><br><span class="line">            initializer = <span class="string">&quot;random_normal&quot;</span>,</span><br><span class="line">            trainable= <span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.Bias = self.add_weight(</span><br><span class="line">            shape = (self.units,),</span><br><span class="line">            initializer = <span class="string">&quot;random_normal&quot;</span>,</span><br><span class="line">            trainable = <span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, Input</span>):</span><br><span class="line">        <span class="comment"># 상기 동일.</span></span><br><span class="line">        <span class="keyword">return</span> tf.matmul(Input, self.Weight) + self.Bias</span><br></pre></td></tr></table></figure><br>위의 2개의 class는 하는 짓거리가 완벽하게 똑같다. 하지만 하는 짓거리는 같은데 아주 치명적인 부분이 조금 다르다. 바로 build 함수의 overwritting이다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#                입력 벡터/텐서의 크기를 받음</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build</span>(<span class="params">self, input_dim</span>):</span><br><span class="line">    <span class="comment"># 그에 따라서 Weight와 Bias의 차원을 결정함.</span></span><br><span class="line">    <span class="comment"># 물론, Bias는 차이가 없을 지언정, Weight는 크게 차이가 나게 된다.</span></span><br><span class="line">    self.Weight = self.add_weight(</span><br><span class="line">        shape=(input_dim[-<span class="number">1</span>], self.units),</span><br><span class="line">        initializer = <span class="string">&quot;random_normal&quot;</span>,</span><br><span class="line">        trainable= <span class="literal">True</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    self.Bias = self.add_weight(</span><br><span class="line">        shape = (self.units,),</span><br><span class="line">        initializer = <span class="string">&quot;random_normal&quot;</span>,</span><br><span class="line">        trainable = <span class="literal">True</span></span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>이 함수는 call이 호출되기 전에 무조건 실행되는 함수라고 생각하면 된다. 즉, 호출하기 전에 Weight를 정의하는 것이다. 즉, 입력 벡터의 크기에 따라 모델의 형태가 알아서 바꾸게 해줄 수 있는 것이다. 이는 Image를 처리할때 이점이 될 수 있다.<br>Image를 학습시킬때, 이러한 처리가 없으면 이미지를 전부 동일한 크기로 만들어 주어야 한다. 하지만 build 함수를 정의해서 그때 그때 입력 벡터/텐서에 따라 커널을 수정해 주면 굳이 그럴 필요가 없다. 전처리 비용이 줄어드는 것이다. </p>
<p>그리고, 이제 이를 학습시키기 위한 코드를 보도록 하자.</p>
<p><em>file: train_MNIST_2.py</em><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> model.FCNN2 <span class="keyword">import</span> FCNN</span><br><span class="line"></span><br><span class="line">EPOCHS = <span class="number">15</span></span><br><span class="line">LR = <span class="number">0.01</span></span><br><span class="line">BatchSize = <span class="number">32</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 이미지 불러오기</span></span><br><span class="line">mnist = keras.datasets.mnist</span><br><span class="line">(train_img, train_labels), (test_img, test_labels) = mnist.load_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 필요한 전처리</span></span><br><span class="line">train_img, test_img = train_img.reshape([-<span class="number">1</span>, <span class="number">784</span>]), test_img.reshape([-<span class="number">1</span>, <span class="number">784</span>])</span><br><span class="line">train_img = train_img.astype(np.float32) / <span class="number">255.</span></span><br><span class="line">test_img = test_img.astype(np.float32) / <span class="number">255.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Train-Validation Split</span></span><br><span class="line">validation_img = train_img[-<span class="number">18000</span>:]</span><br><span class="line">validation_label = train_labels[-<span class="number">18000</span>:]</span><br><span class="line">train_img = train_img[:-<span class="number">18000</span>]</span><br><span class="line">train_labels = train_labels[:-<span class="number">18000</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train Data의 규합. &amp; Batch 별로 쪼갬</span></span><br><span class="line">train_dataset = tf.data.Dataset.from_tensor_slices((train_img, train_labels))</span><br><span class="line">train_dataset = train_dataset.shuffle(buffer_size=<span class="number">1024</span>).batch(BatchSize)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Validation Data의 규합 &amp; Batch 별로 쪼갬</span></span><br><span class="line">validation_dataset = tf.data.Dataset.from_tensor_slices((validation_img, validation_label))</span><br><span class="line">validation_dataset = validation_dataset.batch(BatchSize)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Optimizer &amp; Loss Function 정의</span></span><br><span class="line">optimizer = keras.optimizers.Adam(learning_rate=LR)</span><br><span class="line">loss_function = keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 학습이 잘 되고 있나 확인하기 위한 지표를 확인하기 위함.</span></span><br><span class="line">train_accuracy = keras.metrics.SparseCategoricalAccuracy()</span><br><span class="line">val_acc_metric = keras.metrics.SparseCategoricalAccuracy()</span><br><span class="line"></span><br><span class="line">model = FCNN()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Custom Training을 위한 반복문</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCHS):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Epoch %d start&quot;</span>%epoch)</span><br><span class="line">    <span class="keyword">for</span> step, (x_batch, y_batch) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_dataset):</span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">            <span class="comment"># Model의 Feed Forward</span></span><br><span class="line">            logits = model(x_batch, training=<span class="literal">True</span>)</span><br><span class="line">            <span class="comment"># Feed Forward 결과를 바탕으로 Loss를 구함</span></span><br><span class="line">            loss_val = loss_function(y_batch, logits)</span><br><span class="line">        <span class="comment"># 위의 과정을 바탕으로 gradient를 구함</span></span><br><span class="line">        grad = tape.gradient(loss_val, model.trainable_weights)</span><br><span class="line">        <span class="comment"># Optimizer를 통해서 Training Variables를 업데이트</span></span><br><span class="line">        optimizer.apply_gradients(<span class="built_in">zip</span>(grad, model.trainable_weights))</span><br><span class="line">        <span class="comment"># Batch 별로 Training dataset에 대한 정확도를 구함.</span></span><br><span class="line">        train_accuracy.update_state(y_batch, logits)</span><br><span class="line">        <span class="keyword">if</span> step % <span class="number">500</span> == <span class="number">0</span> :</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Training loss at step %d: %.4f&quot;</span>%(step, loss_val))</span><br><span class="line">    <span class="comment"># 정확도를 규합해서 출력</span></span><br><span class="line">    train_acc = train_accuracy.result()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Training acc over epoch: %.4f&quot;</span> % (<span class="built_in">float</span>(train_acc),))</span><br><span class="line">    train_accuracy.reset_states()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Validation을 진행함.</span></span><br><span class="line">    <span class="keyword">for</span> x_batch_val, y_batch_val <span class="keyword">in</span> validation_dataset:</span><br><span class="line">        <span class="comment"># Validation을 위한 Feed Forward</span></span><br><span class="line">        val_logits = model(x_batch_val, training = <span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># Batch 별로 Validation dataset에 대한 정확도를 구함</span></span><br><span class="line">        val_acc_metric.update_state(y_batch_val, val_logits)</span><br><span class="line">    <span class="comment"># 구한 정확도를 규합하여 출력함.</span></span><br><span class="line">    val_acc = val_acc_metric.result()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Validation acc: %.4f&quot;</span> % (<span class="built_in">float</span>(val_acc),))</span><br><span class="line">    val_acc_metric.reset_states()</span><br></pre></td></tr></table></figure><br>이는 TF2에서 추가된 tf.GradientTape를 통해서 사용자 정의 학습 루프를 만든 것이다. 각 줄마다 주석을 달아 놓았으니 Part. A의 내용을 숙지했다면 그렇게 어렵지 않게 알아들을 수 있을 것이다. </p>
<p>이제, 한가지 의문이 든다. 그렇다면 위의 Hyper parameter들을 변화시켜가면서 model을 최적화 하려면 노가다 밖에 답이없는건가? 답은 아니다. 이번에는 맛보기만 보여줄 것이다. 이는 scikit learn의 RandomizedSearchCV를 통해서 확인할 수 있다.</p>
<p><em>file: RandomSearch.py</em><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> reciprocal</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> RandomizedSearchCV</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">mnist = keras.datasets.mnist</span><br><span class="line">(train_img, train_labels), (test_img, test_labels) = mnist.load_data()</span><br><span class="line">train_img, test_img = train_img.reshape([-<span class="number">1</span>, <span class="number">784</span>]), test_img.reshape([-<span class="number">1</span>, <span class="number">784</span>])</span><br><span class="line">train_img = train_img.astype(np.float32) / <span class="number">255.</span></span><br><span class="line">test_img = test_img.astype(np.float32) / <span class="number">255.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 여기 parameter들의 Key는 무적권 입력 함수의 parameter와 같아야 한다. 아마도 **kwargs로 한번에 보내버리는 것일거다.</span></span><br><span class="line">param_distribution = &#123;</span><br><span class="line">    <span class="string">&quot;n_hidden&quot;</span>: [<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],</span><br><span class="line">    <span class="string">&quot;n_neurons&quot;</span>: np.arange(<span class="number">1</span>,<span class="number">100</span>),</span><br><span class="line">    <span class="string">&quot;lr&quot;</span>: reciprocal(<span class="number">3e-4</span>, <span class="number">3e-2</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 이것을 사용하기 위해서는 모델을 만들어줄 함수가 하나 필요하다.</span></span><br><span class="line"><span class="comment"># 여기서는 그냥 Sequential API를 사용하였다. 생각하기 귀찮았다. ㅋ;; </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Build_model</span>(<span class="params">n_hidden = <span class="number">1</span>, n_neurons=<span class="number">30</span>, lr = <span class="number">3e-3</span>, input_shape=[<span class="number">784</span>]</span>):</span><br><span class="line">    model = keras.models.Sequential()</span><br><span class="line">    model.add(keras.layers.InputLayer(input_shape=input_shape))</span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="built_in">range</span>(n_hidden):</span><br><span class="line">        model.add(keras.layers.Dense(n_neurons, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">    model.add(keras.layers.Dense(units=<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line">    optimizer = keras.optimizers.SGD(learning_rate=lr)</span><br><span class="line">    model.<span class="built_in">compile</span>(optimizer=optimizer, loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>, metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="comment"># 이는 keras 모델을 scikit learn에서 관리하기 위해 호출하는 함수이다.</span></span><br><span class="line">keras_classify = keras.wrappers.scikit_learn.KerasClassifier(Build_model)</span><br><span class="line"><span class="comment"># 여기서부터 본론이다. parameter_distribution으로 주어진 집합 한에서 가장 좋은 성능의 모델을 탐색한다.</span></span><br><span class="line"><span class="comment"># 여기서는 Cross-Validation을 사용한다. cv 항은 몇개로 Validation-training set을 분리할지 정하는 것이다. </span></span><br><span class="line">rnd_search_model = RandomizedSearchCV(keras_classify, param_distributions=param_distribution, n_iter = <span class="number">10</span>, cv = <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 이제 주어진 데이터를 가지고 학습을 돌면서 최적의 모델을 탐색한다. </span></span><br><span class="line">rnd_search_model.fit(train_img, train_labels, epochs=<span class="number">10</span>, validation_data=(test_img,test_labels), callbacks=[keras.callbacks.EarlyStopping(patience=<span class="number">10</span>)])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(rnd_search_model.best_params_)</span><br></pre></td></tr></table></figure></p>
<p>오늘은 이것으로 끝내도록 하자.<br>다음 포스트는 Convolutional Neural Network를 오늘처럼 다뤄볼 예정이다.</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
              <a href="/tags/Tensorflow2/" rel="tag"># Tensorflow2</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/03/05/FCNN-A/" rel="prev" title="Fully Connected Neural Network 강의 내용 part.A">
      <i class="fa fa-chevron-left"></i> Fully Connected Neural Network 강의 내용 part.A
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/03/05/CNN-A/" rel="next" title="Convoltional Neural Network 강의 내용 part.A">
      Convoltional Neural Network 강의 내용 part.A <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-6"><a class="nav-link" href="#Index"><span class="nav-number">1.</span> <span class="nav-text">Index</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-Partice-%EB%B6%80%EB%A1%9D-Hyper-parameter-tuning"><span class="nav-number"></span> <span class="nav-text">6. Partice(+ 부록 Hyper parameter tuning)</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#6-1-Model-Subclassing"><span class="nav-number">1.</span> <span class="nav-text">6-1. Model Subclassing</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Michael Kim</p>
  <div class="site-description" itemprop="description">Backend Engineering, Deep learning, Algorithm & DS, CS 전공 지식 공부 저장소</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">23</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">15</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/kimh060612" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;kimh060612" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:kimh060612@khu.ac.kr" title="E-Mail → mailto:kimh060612@khu.ac.kr" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Michael Kim</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://michael-blog-1.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  var disqus_config = function() {
    this.page.url = "https://kimh060612.github.io/2022/03/05/FCNN-B/";
    this.page.identifier = "2022/03/05/FCNN-B/";
    this.page.title = "Fully Connected Neural Network 강의 내용 part.B";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://michael-blog-1.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

</body>
</html>
