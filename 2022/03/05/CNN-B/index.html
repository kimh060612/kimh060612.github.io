<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
<meta name="google-site-verification" content="L0ZN3xykszl3pBQeJ5QFwk98KKJ8kHDzGSBLhEtwmNU" />
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"kimh060612.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="본 포스트는 Hands-on Machine learning 2nd Edition, CS231n, Tensorflow 공식 document를 참조하여 작성되었음을 알립니다. Index Introduction of Convolution Operation Definition of Convolutional Neural Network(CNN) Back Propaga">
<meta property="og:type" content="article">
<meta property="og:title" content="Convoltional Neural Network 강의 내용 part.B">
<meta property="og:url" content="https://kimh060612.github.io/2022/03/05/CNN-B/index.html">
<meta property="og:site_name" content="Michael&#39;s Study Blog">
<meta property="og:description" content="본 포스트는 Hands-on Machine learning 2nd Edition, CS231n, Tensorflow 공식 document를 참조하여 작성되었음을 알립니다. Index Introduction of Convolution Operation Definition of Convolutional Neural Network(CNN) Back Propaga">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://kimh060612.github.io/img/AlexNet.jpg">
<meta property="og:image" content="http://kimh060612.github.io/img/ResNet50.png">
<meta property="og:image" content="http://kimh060612.github.io/img/GAP.jpg">
<meta property="article:published_time" content="2022-03-05T10:26:43.000Z">
<meta property="article:modified_time" content="2022-05-29T15:00:32.839Z">
<meta property="article:author" content="Michael Kim">
<meta property="article:tag" content="Deep Learning">
<meta property="article:tag" content="Tensorflow2">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://kimh060612.github.io/img/AlexNet.jpg">

<link rel="canonical" href="https://kimh060612.github.io/2022/03/05/CNN-B/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Convoltional Neural Network 강의 내용 part.B | Michael's Study Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><link rel="alternate" href="/rss2.xml" title="Michael's Study Blog" type="application/rss+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Michael's Study Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Backend Engineering & Deep learning!</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>Tags<span class="badge">15</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>Categories<span class="badge">7</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>Archives<span class="badge">21</span></a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kimh060612.github.io/2022/03/05/CNN-B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Michael Kim">
      <meta itemprop="description" content="Backend Engineering, Deep learning, Algorithm & DS, CS 전공 지식 공부 저장소">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Michael's Study Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Convoltional Neural Network 강의 내용 part.B
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-03-05 19:26:43" itemprop="dateCreated datePublished" datetime="2022-03-05T19:26:43+09:00">2022-03-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-05-30 00:00:32" itemprop="dateModified" datetime="2022-05-30T00:00:32+09:00">2022-05-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2022/03/05/CNN-B/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2022/03/05/CNN-B/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><em>본 포스트는 Hands-on Machine learning 2nd Edition, CS231n, Tensorflow 공식 document를 참조하여 작성되었음을 알립니다.</em></p>
<h3 id="Index"><a href="#Index" class="headerlink" title="Index"></a>Index</h3><ol>
<li><del>Introduction of Convolution Operation</del></li>
<li><del>Definition of Convolutional Neural Network(CNN)</del></li>
<li><del>Back Propagation of CNN</del></li>
<li>Partice</li>
</ol>
<h4 id="6-Partice"><a href="#6-Partice" class="headerlink" title="6. Partice"></a>6. Partice</h4><hr>
<p>이 파트는 Convolutional Nerual Network를 직접 구현하는 파트이다. 비단 Convolutional Neural Network 뿐만 아니라 그를 이용한 다양한 현대적인 네트워크 구조들을 구현해 보는 시간을 가지도록 하겠다. 필자의 Deep learning 구현 관련 블로그 포스팅은 전부 Model Subclassing API로 구현될 예정이다. 왜냐? 필자 맘이다 <del>(꼬우면 보지 말든가)</del> 장닌이고, 필자가 생각하기에는 Model Subclassing API의 활용 장점은 확실히 있는 것 같다.</p>
<ol>
<li><p>모델을 가독성 있게 관리할 수 있다.</p>
<p>이는 전적으로 OOP에 대한 기본 개념 및 디자인 패턴을 잘 아는 사람에 한에서 그런거다.<br>Vision Transformer쯤 가면 알겠지만, 정말 짜야하는 연산들이 엄청 많다. 그런걸 하나하나 함수로 짜거나 Sequential API로 구성하면 지옥문이 열리게 된다. 아 물론 짜는건 무리가 없겠지만, 유지보수 관점에서는 정말 지옥일 것이다.<br>그런 의미에서 Model Subclassing API는 원하는 연산을 Class단위로 묶어서 설계하고 그들을 체계적으로 관리할 수 있는 지식이 조금이라도 있다면 (복잡한 디자인 패턴까지는 필요도 없다) 훨씬 가독성이 높은 코드를 짤 수 있다.</p>
</li>
<li><p>Low Level한 연산을 자유롭게 정의할 수 있다.</p>
<p> Model Subclassing API를 사용하면 Custom Layer, Scheduler 등등 여러 연산을 사용자의 입맛에 맞게 정의할 수 있다. 이러면 내가 세운 새로운 가설, 연구 아이디어를 보다 쉽게 구현할 수 있는 판로가 열리는 것이다. 물론 이는 전적으로 자신이 새로운 연산을 구상하고 구현할만한 경지에 도달했을때의 이야기이다.</p>
</li>
<li><p>특히 Pytorch로 소스코드 전환을 비교적 쉽게 할 수 있다.</p>
<p>이건 지극히 필자의 개인적인 생각이다. 필자는 pytorch와 tensorflow를 동시에 써가면서 일을 하고 있다. 모델 개발 및 연구는 pytorch로 배포는 tensorflow를 사용하고 있는데, 모델을 tensorflow로 완전히 포팅해야할 일이 가끔씩 있다. 이때 model subclassing API를 활용하는 편이 소스코드의 구조나 뽄새가 비슷해서 편했던 기억이 난다.</p>
</li>
</ol>
<p>하지만 단점도 명확하게 있다.</p>
<ol>
<li><p>못쓰면 이도 저도 안된다.</p>
<p> 보면 알다시피, OOP의 기초 지식과 low level로 연산을 정의해서 사용할 수 있는 사람이 아니라면 굳이 Subclassing API를 쓰겠다고 깝치다가 되려 오류만 범할 가능성이 높다.</p>
</li>
</ol>
<p>하지만 필자는 앞으로 잘하고 싶어서 힘든 길을 골라 보았다. 독자들도 이에 동의하리라고 믿는다. <del>(아니면 뒤로 가든가)</del></p>
<p>사족이 길었는데, 앞으로도 계속 Model Subclassing API만을 사용해서 포스팅을 할 예정이다.</p>
<p>우선, 지난 FCNN처럼 tensorflow 2로 어떻게 CNN layer를 정의할 수 있는지부터 알아보자.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CustomConv2D</span>(keras.layers.Layer):</span><br><span class="line">    <span class="comment">#              1. output image의 채널  2. 커널의 이미지 사이즈  3. Stride를 정해줬어야함. 4. Pooling을 정해줬어야함.(Optional) 5. Padding을 정해야함.</span></span><br><span class="line">    <span class="comment">#                                                       i  x 방향으로의 stride y 방향으로의 stride      i</span></span><br><span class="line">    <span class="comment"># &quot;SAME&quot; OH = H, &quot;VALID&quot; 가능한 패딩 필터 중에서 가장 작은 패딩(양수)으로 설정  </span></span><br><span class="line">    <span class="comment"># OH = (H + 2*P - KH)/S + 1 = 15.5</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, out_channel, kernel_size, Strides = (<span class="params"><span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span></span>), Padding = <span class="string">&quot;SAME&quot;</span>, trainable=<span class="literal">True</span>, name=<span class="literal">None</span>, dtype=<span class="literal">None</span>, dynamic=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(trainable=trainable, name=name, dtype=dtype, dynamic=dynamic, **kwargs)</span><br><span class="line">        <span class="comment"># &quot;3,4&quot; </span></span><br><span class="line">        self.out_channel = out_channel</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(kernel_size) == <span class="built_in">type</span>(<span class="number">1</span>):</span><br><span class="line">            self.kernel_size = (kernel_size, kernel_size)</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">type</span>(kernel_size) == <span class="built_in">type</span>(<span class="built_in">tuple</span>()):</span><br><span class="line">            self.kernel_size = kernel_size</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Not a Valid Kernel Type&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(Strides) == <span class="built_in">type</span>(<span class="number">1</span>):</span><br><span class="line">            self.Stride = (<span class="number">1</span>, Strides, Strides, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">type</span>(Strides) == <span class="built_in">type</span>(<span class="built_in">tuple</span>()):</span><br><span class="line">            self.Stride = Strides</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Not a Valid Stride Type&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(Padding) == <span class="built_in">type</span>(<span class="built_in">str</span>()):</span><br><span class="line">            self.Padding = Padding</span><br><span class="line">        <span class="keyword">else</span> :</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Not a Valid Padding Type&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build</span>(<span class="params">self, input_shape</span>):</span><br><span class="line">        WeightShape = (self.kernel_size[<span class="number">0</span>], self.kernel_size[<span class="number">1</span>], input_shape[-<span class="number">1</span>], self.out_channel)</span><br><span class="line">        self.Kernel = self.add_weight(</span><br><span class="line">            shape=WeightShape,</span><br><span class="line">            initializer=<span class="string">&quot;random_normal&quot;</span>,</span><br><span class="line">            trainable= <span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.Bias = self.add_weight(</span><br><span class="line">            shape=(self.out_channel, ),</span><br><span class="line">            initializer=<span class="string">&quot;random_normal&quot;</span>,</span><br><span class="line">            trainable=<span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, Input</span>):</span><br><span class="line">        Out = tf.nn.conv2d(Input, self.Kernel, strides=self.Stride, padding=self.Padding)</span><br><span class="line">        Out = tf.nn.bias_add(Out, self.Bias, data_format=<span class="string">&quot;NHWC&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> Out</span><br></pre></td></tr></table></figure>
<p>이전에도 설명했듯이 build에서 필요한 Weight를 정의한 뒤에 call에서 그것을 사용한다. 다행이게도 tensorflow에서는 최소한 convolution 연산을 정의해 주었다.<br>앞으로도 필요한 연산이 있다면 이렇게 정의해 주면 된다.</p>
<p>하지만 우리는 굳이 이렇게 convolution layer를 정의해줄 필요가 없다. 왜냐면 tensorflow keras에서 이미 정의되어 있는 좋은 함수가 있기 때문이다. 이에 대한 아주 간단한 사용 예제로써 Alexnet과 ResNet을 구현해 보도록 하겠다. 부록으로 GoogLeNet을 구현한 예제도 있는데, 이는 필자의 Github에 올려 두도록 할테니 시간이 되면 가서 봐 주었으면 한다.</p>
<p>우선 AlexNet부터 가보자. 모델의 구조를 사진으로 한번 봐보도록 하자.</p>
<p align="center"><img src="http://kimh060612.github.io/img/AlexNet.jpg" width="70%" height="70%"></p>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AlexNet</span>(keras.Model):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 원래 여기에는 커널 사이즈로 (11, 11)이 들어가고 padding은 valid이다. 하지만 메모리 때문에 돌아가지 않는 관계로 이미지 크기를 줄아느라 부득이하게 모델을 조금 변경했다.</span></span><br><span class="line">        self.Conv1 = keras.layers.Conv2D(<span class="number">96</span>, (<span class="number">3</span>, <span class="number">3</span>), strides=(<span class="number">4</span>, <span class="number">4</span>), padding=<span class="string">&quot;SAME&quot;</span>, activation=<span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        <span class="comment"># LRN 1</span></span><br><span class="line">        self.BatchNorm1 = keras.layers.BatchNormalization()</span><br><span class="line">        self.MaxPool1 = keras.layers.MaxPool2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>), strides=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">&quot;VALID&quot;</span>)</span><br><span class="line"></span><br><span class="line">        self.Conv2 = keras.layers.Conv2D(<span class="number">256</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), padding=<span class="string">&quot;SAME&quot;</span>, activation=<span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        <span class="comment"># LRN2</span></span><br><span class="line">        self.BatchNorm2 = keras.layers.BatchNormalization()</span><br><span class="line">        self.MaxPool2 = keras.layers.MaxPool2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>), strides=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">&quot;VALID&quot;</span>)</span><br><span class="line"></span><br><span class="line">        self.Conv3 = keras.layers.Conv2D(<span class="number">384</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&quot;SAME&quot;</span>, activation=<span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        self.Conv4 = keras.layers.Conv2D(<span class="number">384</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&quot;SAME&quot;</span>, activation=<span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        self.Conv5 = keras.layers.Conv2D(<span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&quot;SAME&quot;</span>, activation=<span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        self.MaxPool3 = keras.layers.MaxPool2D(pool_size=(<span class="number">3</span>, <span class="number">3</span>), strides=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">        self.Flat = keras.layers.Flatten()</span><br><span class="line"></span><br><span class="line">        self.Dense1 = keras.layers.Dense(<span class="number">4096</span>, activation=<span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        self.DropOut1 = keras.layers.Dropout(<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">        self.Dense2 = keras.layers.Dense(<span class="number">4096</span>, activation=<span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        self.DropOut2 = keras.layers.Dropout(<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">        self.OutDense = keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, Input</span>):</span><br><span class="line">        X = self.Conv1(Input)</span><br><span class="line">        X = self.BatchNorm1(X)</span><br><span class="line">        X = self.MaxPool2(X)</span><br><span class="line">        X = self.Conv2(X)</span><br><span class="line">        X = self.BatchNorm2(X)</span><br><span class="line">        X = self.MaxPool2(X)</span><br><span class="line">        X = self.Conv3(X)</span><br><span class="line">        X = self.Conv4(X)</span><br><span class="line">        X = self.Conv5(X)</span><br><span class="line">        X = self.MaxPool3(X)</span><br><span class="line">        X = self.Flat(X)</span><br><span class="line">        X = self.Dense1(X)</span><br><span class="line">        X = self.DropOut1(X)</span><br><span class="line">        X = self.Dense2(X)</span><br><span class="line">        X = self.DropOut2(X)</span><br><span class="line">        X = self.OutDense(X)</span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure>
<p>자, 필자는 굳이 더럽게 짜 보았다. 왜냐? <em>이렇게 짤거면 Model Subclassing을 쓰지 말라는 의미로 이렇게 짜 보았다.</em> 진짜 이따구로 짤거면 그냥 Sequential API나 Functional API를 사용하자. 근데 이 정도면 설명이 필요 없을 정도로 그냥 무지성 구현을 시전한 것이다. 그러니 간단하게 Keras의 Conv2D를 설명하고 넘어 가도록 하겠다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Conv2D(filters, kernel_size=(kernel_sz, kernel_sz), padding=<span class="string">&quot;SAME&quot;</span>, activation=<span class="string">&quot;relu&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>이전에 이론 글에서 설명했던 부분을 다시 되짚어 보고 위 함수를 다시 살펴보자.</p>
<blockquote>
<p><em>Definition 1</em></p>
</blockquote>
<ul>
<li>$w_{ijmk}^l$: $l$번째 층의 Weight의 $k$번째 Kernel Set에 $m$번째 Channel, $i$행, $j$열의 성분</li>
</ul>
<p>위의 $w_{ijmk}^l$를 우리는 위의 Conv2D 함수로 정의한 것이다. filters는 kernel set의 개수를 의미하며, kernel_size는 Weight kernel의 이미지 크기를 의미한다. padding은 “SAME”과 “VALID”가 있는데, “SAME”으로 하면 알아서 크기를 계산해서 입력 이미지와 출력 이미지의 크기를 같게 만든다. Valid를 선택하면 그냥 padding이 없다고 판단하면 된다. </p>
<p>AlextNet에서 대충 Conv2D를 어떻게 사용하는지 감이 왔다면, ResNet을 한번 구현해 보자.</p>
<p>ResNet에 대한 자세한 설명은 다른 포스트에서 정말 이게 맞나 싶을 정도로 분해해서 설명하도록 하겠다. 지금은 그저 다음과 같은 구조가 있구나 정도만 이해하고 넘어가면 된다.</p>
<p align="center"><img src="http://kimh060612.github.io/img/ResNet50.png" width="70%" height="70%"></p>

<p>이것이 ResNet50의 구조인데, 2가지 layer를 구현해 보아야 한다. 첫번째는 Conv Block이고 두번째는 Identity Block이다. 하나는 Skip Connection에 Convolution layer를 입힌 것이고 다른 하나는 그렇지 않은 것 뿐이다.</p>
<blockquote>
<p>Residual Conv Block</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResidualConvBlock</span>(tfk.layers.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, InputChannel, OutputChannel, strides = (<span class="params"><span class="number">1</span>, <span class="number">1</span></span>), trainable=<span class="literal">True</span>, name=<span class="literal">None</span>, dtype=<span class="literal">None</span>, dynamic=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(trainable=trainable, name=name, dtype=dtype, dynamic=dynamic, **kwargs)</span><br><span class="line"></span><br><span class="line">        self.Batch1 = tfk.layers.BatchNormalization(momentum=<span class="number">0.99</span>, epsilon= <span class="number">0.001</span>)</span><br><span class="line">        self.conv1 = tfk.layers.Conv2D(filters=InputChannel, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), strides=strides)</span><br><span class="line">        self.LeakyReLU1 = tfk.layers.LeakyReLU()</span><br><span class="line">        self.Batch2 = tfk.layers.BatchNormalization(momentum=<span class="number">0.99</span>, epsilon= <span class="number">0.001</span>)</span><br><span class="line">        self.conv2 = tfk.layers.Conv2D(filters=InputChannel, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), strides=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">&quot;SAME&quot;</span>)</span><br><span class="line">        self.LeakyReLU2 = tfk.layers.LeakyReLU()</span><br><span class="line">        self.Batch3 = tfk.layers.BatchNormalization(momentum=<span class="number">0.99</span>, epsilon= <span class="number">0.001</span>)</span><br><span class="line">        self.conv3 = tfk.layers.Conv2D(filters=OutputChannel, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), strides=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.LeakyReLU3 = tfk.layers.LeakyReLU()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Skip Connection</span></span><br><span class="line">        self.SkipConnection = tfk.layers.Conv2D(filters=OutputChannel, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), strides=strides)</span><br><span class="line">        self.SkipBatch = tfk.layers.BatchNormalization(momentum=<span class="number">0.99</span>, epsilon= <span class="number">0.001</span>)</span><br><span class="line">        self.LeakyReLUSkip = tfk.layers.LeakyReLU()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, Input</span>):</span><br><span class="line">        Skip = Input</span><br><span class="line">        Skip = self.SkipConnection(Skip)</span><br><span class="line">        Skip = self.SkipBatch(Skip)</span><br><span class="line">        Skip = self.LeakyReLUSkip(Skip)</span><br><span class="line">        Z = Input</span><br><span class="line">        Z = self.conv1(Z)</span><br><span class="line">        Z = self.Batch1(Z)</span><br><span class="line">        Z = self.LeakyReLU1(Z)</span><br><span class="line">        Z = self.conv2(Z)</span><br><span class="line">        Z = self.Batch2(Z)</span><br><span class="line">        Z = self.LeakyReLU2(Z)  </span><br><span class="line">        Z = self.conv3(Z)</span><br><span class="line">        Z = self.Batch3(Z)</span><br><span class="line">        Z = self.LeakyReLU3(Z)</span><br><span class="line">        <span class="keyword">return</span> Z + Skip </span><br></pre></td></tr></table></figure>
<blockquote>
<p>Residual Identity Block</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResidualIdentityBlock</span>(tfk.layers.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, InputChannel, OutputChannel, trainable=<span class="literal">True</span>, name=<span class="literal">None</span>, dtype=<span class="literal">None</span>, dynamic=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(trainable=trainable, name=name, dtype=dtype, dynamic=dynamic, **kwargs)</span><br><span class="line">        self.Batch1 = tfk.layers.BatchNormalization(momentum=<span class="number">0.99</span>, epsilon= <span class="number">0.001</span>)</span><br><span class="line">        self.conv1 = tfk.layers.Conv2D(filters=InputChannel, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), strides=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.LeakyReLU1 = tfk.layers.LeakyReLU()</span><br><span class="line">        self.Batch2 = tfk.layers.BatchNormalization(momentum=<span class="number">0.99</span>, epsilon= <span class="number">0.001</span>)</span><br><span class="line">        self.conv2 = tfk.layers.Conv2D(filters=InputChannel, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), strides=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">&quot;SAME&quot;</span>)</span><br><span class="line">        self.LeakyReLU2 = tfk.layers.LeakyReLU()</span><br><span class="line">        self.Batch3 = tfk.layers.BatchNormalization(momentum=<span class="number">0.99</span>, epsilon= <span class="number">0.001</span>)</span><br><span class="line">        self.conv3 = tfk.layers.Conv2D(filters=OutputChannel, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), strides=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.LeakyReLU3 = tfk.layers.LeakyReLU()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, Input</span>):</span><br><span class="line">        Skip = Input</span><br><span class="line">        Z = Input</span><br><span class="line">        Z = self.conv1(Z)</span><br><span class="line">        Z = self.Batch1(Z)</span><br><span class="line">        Z = self.LeakyReLU1(Z)</span><br><span class="line">        Z = self.conv2(Z)</span><br><span class="line">        Z = self.Batch2(Z)</span><br><span class="line">        Z = self.LeakyReLU2(Z)  </span><br><span class="line">        Z = self.conv3(Z)</span><br><span class="line">        Z = self.Batch3(Z)</span><br><span class="line">        Z = self.LeakyReLU3(Z)</span><br><span class="line">        <span class="comment"># Z : 256</span></span><br><span class="line">        <span class="keyword">return</span> Z + Skip</span><br></pre></td></tr></table></figure>
<p>우선 이 또한 정말이지 Model Subclassing을 그지같이 사용한 예시중 하나이다. 부디 독자들은 이따구로 구현할거면 그냥 Functional API를 사용하기 바란다.</p>
<p>이쯤되면 이런 질문이 나올 것이다. </p>
<blockquote>
<p>Q: 왜 저게 그지같이 구현한 예시인가요?<br>A: 여러 이유가 있지만, 가장 큰 이유는 굳이 모델(Weight)의 정의와 호출을 분리할 이유가 전혀 없는 구조이기 때문입니다.</p>
</blockquote>
<p>여기까지 잘 따라왔다면 이제 이 2개의 layer를 사용해서 ResNet50을 다음과 같이 구현할 수 있다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResNet50</span>(keras.Model):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># Input Shape 224*224*3</span></span><br><span class="line">        <span class="comment"># Conv 1 Block</span></span><br><span class="line">        self.ZeroPadding1 = keras.layers.ZeroPadding2D(padding=(<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">        self.Conv1 = keras.layers.Conv2D(filters = <span class="number">64</span>, kernel_size=(<span class="number">7</span>, <span class="number">7</span>), strides=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        self.Batch1 = keras.layers.BatchNormalization()</span><br><span class="line">        self.ReLU1 = keras.layers.LeakyReLU()</span><br><span class="line">        self.ZeroPadding2 = keras.layers.ZeroPadding2D(padding=(<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        self.MaxPool1 = keras.layers.MaxPool2D(pool_size=(<span class="number">3</span>, <span class="number">3</span>), strides=<span class="number">2</span>)</span><br><span class="line">        self.ResConvBlock1 = ResidualConvBlock(<span class="number">64</span>, <span class="number">256</span>, strides = (<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.ResIdentityBlock1 = ResidualIdentityBlock(<span class="number">64</span>, <span class="number">256</span>)</span><br><span class="line">        self.ResIdentityBlock2 = ResidualIdentityBlock(<span class="number">64</span>, <span class="number">256</span>)</span><br><span class="line"></span><br><span class="line">        self.ResConvBlock2 = ResidualConvBlock(<span class="number">128</span>, <span class="number">512</span>, strides = (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        self.ResIdentityBlock3 = ResidualIdentityBlock(<span class="number">128</span>, <span class="number">512</span>)</span><br><span class="line">        self.ResIdentityBlock4 = ResidualIdentityBlock(<span class="number">128</span>, <span class="number">512</span>)</span><br><span class="line">        self.ResIdentityBlock5 = ResidualIdentityBlock(<span class="number">128</span>, <span class="number">512</span>)</span><br><span class="line"></span><br><span class="line">        self.ResConvBlock3 = ResidualConvBlock(<span class="number">256</span>, <span class="number">1024</span>, strides = (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        self.ResIdentityBlock6 = ResidualIdentityBlock(<span class="number">256</span>, <span class="number">1024</span>)</span><br><span class="line">        self.ResIdentityBlock7 = ResidualIdentityBlock(<span class="number">256</span>, <span class="number">1024</span>)</span><br><span class="line">        self.ResIdentityBlock8 = ResidualIdentityBlock(<span class="number">256</span>, <span class="number">1024</span>)</span><br><span class="line">        self.ResIdentityBlock9 = ResidualIdentityBlock(<span class="number">256</span>, <span class="number">1024</span>)</span><br><span class="line">        self.ResIdentityBlock10 = ResidualIdentityBlock(<span class="number">256</span>, <span class="number">1024</span>)</span><br><span class="line"></span><br><span class="line">        self.ResConvBlock4 = ResidualConvBlock(<span class="number">512</span>, <span class="number">2048</span>, strides = (<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.ResIdentityBlock11 = ResidualIdentityBlock(<span class="number">512</span>, <span class="number">2048</span>)</span><br><span class="line">        self.ResIdentityBlock12 = ResidualIdentityBlock(<span class="number">512</span>, <span class="number">2048</span>)</span><br><span class="line">        </span><br><span class="line">        self.GAP = keras.layers.GlobalAveragePooling2D()</span><br><span class="line">        self.DenseOut = keras.layers.Dense(<span class="number">1000</span>, activation=<span class="string">&#x27;softmax&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, Input</span>):</span><br><span class="line">        X = self.ZeroPadding1(Input)</span><br><span class="line">        X = self.Conv1(X)</span><br><span class="line">        X = self.Batch1(X)</span><br><span class="line">        X = self.ReLU1(X)</span><br><span class="line">        X = self.ZeroPadding2(X)</span><br><span class="line"></span><br><span class="line">        X = self.MaxPool1(X)</span><br><span class="line">        X = self.ResConvBlock1(X)</span><br><span class="line">        X = self.ResIdentityBlock1(X)</span><br><span class="line">        X = self.ResIdentityBlock2(X)</span><br><span class="line"></span><br><span class="line">        X = self.ResConvBlock2(X)</span><br><span class="line">        X = self.ResIdentityBlock3(X)</span><br><span class="line">        X = self.ResIdentityBlock4(X)</span><br><span class="line">        X = self.ResIdentityBlock5(X)</span><br><span class="line"></span><br><span class="line">        X = self.ResConvBlock3(X)</span><br><span class="line">        X = self.ResIdentityBlock6(X)</span><br><span class="line">        X = self.ResIdentityBlock7(X)</span><br><span class="line">        X = self.ResIdentityBlock8(X)</span><br><span class="line">        X = self.ResIdentityBlock9(X)</span><br><span class="line">        X = self.ResIdentityBlock10(X)</span><br><span class="line"></span><br><span class="line">        X = self.ResConvBlock4(X)</span><br><span class="line">        X = self.ResIdentityBlock11(X)</span><br><span class="line">        X = self.ResIdentityBlock12(X)</span><br><span class="line"></span><br><span class="line">        X = self.GAP(X)</span><br><span class="line">        Out = self.DenseOut(X)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> Out</span><br></pre></td></tr></table></figure>
<p>여기서 하나 GAP로 정의된 Global Average Pooling layer가 있다. 이것에 대해서 간단하게만 알아보자.</p>
<p>이 layer는 단순하게 말하자면 feaeture map을 1차원을 만들어 주는 layer이다. 대개, Image는 3차원인데, 차원별로 존재하는 image를 하나의 Scalar 값으로 만든다는 뜻이다. (Global Pooling) 그때, Scalar 값으로 만드는 과정에서 이미지의 각 픽셀 값을 평균을 내는 방법을 취한 것 뿐이다. (Average)</p>
<p>이를 간단히 그림으로 표현하자면 다음과 같다.</p>
<p align="center"><img src="http://kimh060612.github.io/img/GAP.jpg" width="100%" height="100%"></p>

<p>그림에서 보여지는 것과 같이, 각 채널에 있는 이미지들의 픽셀값을 평균을 내서 그것을 모으면 채널의 개수 만큼의 크기를 가지는 1-dimensional vector가 완성된다.</p>
<p>여기까지 Convolutional Neural Network의 구현 실습을 마치도록 하겠다. 부디 도움이 되었….을까?는 모르겠지만 재밌게 보았으면 좋겠다.</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a>
              <a href="/tags/Tensorflow2/" rel="tag"># Tensorflow2</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/03/05/CNN-A/" rel="prev" title="Convoltional Neural Network 강의 내용 part.A">
      <i class="fa fa-chevron-left"></i> Convoltional Neural Network 강의 내용 part.A
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/03/05/RNN-B/" rel="next" title="Recurrent Neural Network 강의 내용 part.B">
      Recurrent Neural Network 강의 내용 part.B <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#Index"><span class="nav-number">1.</span> <span class="nav-text">Index</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-Partice"><span class="nav-number">1.1.</span> <span class="nav-text">6. Partice</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Michael Kim</p>
  <div class="site-description" itemprop="description">Backend Engineering, Deep learning, Algorithm & DS, CS 전공 지식 공부 저장소</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">21</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">15</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/kimh060612" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;kimh060612" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:kimh060612@khu.ac.kr" title="E-Mail → mailto:kimh060612@khu.ac.kr" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Michael Kim</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://michael-blog-1.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  var disqus_config = function() {
    this.page.url = "https://kimh060612.github.io/2022/03/05/CNN-B/";
    this.page.identifier = "2022/03/05/CNN-B/";
    this.page.title = "Convoltional Neural Network 강의 내용 part.B";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://michael-blog-1.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

</body>
</html>
