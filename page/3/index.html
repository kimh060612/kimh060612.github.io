<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
<meta name="google-site-verification" content="L0ZN3xykszl3pBQeJ5QFwk98KKJ8kHDzGSBLhEtwmNU" />
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"kimh060612.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Backend Engineering, Deep learning, Algorithm &amp; DS, CS 전공 지식 공부 저장소">
<meta property="og:type" content="website">
<meta property="og:title" content="Michael&#39;s Study Blog">
<meta property="og:url" content="https://kimh060612.github.io/page/3/index.html">
<meta property="og:site_name" content="Michael&#39;s Study Blog">
<meta property="og:description" content="Backend Engineering, Deep learning, Algorithm &amp; DS, CS 전공 지식 공부 저장소">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Michael Kim">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://kimh060612.github.io/page/3/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Michael's Study Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><link rel="alternate" href="/rss2.xml" title="Michael's Study Blog" type="application/rss+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Michael's Study Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Backend Engineering & Deep learning!</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>Tags<span class="badge">15</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>Categories<span class="badge">7</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>Archives<span class="badge">23</span></a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kimh060612.github.io/2022/03/05/CNN-A/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Michael Kim">
      <meta itemprop="description" content="Backend Engineering, Deep learning, Algorithm & DS, CS 전공 지식 공부 저장소">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Michael's Study Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/05/CNN-A/" class="post-title-link" itemprop="url">Convoltional Neural Network 강의 내용 part.A</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-03-05 19:26:40" itemprop="dateCreated datePublished" datetime="2022-03-05T19:26:40+09:00">2022-03-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-05-30 00:00:32" itemprop="dateModified" datetime="2022-05-30T00:00:32+09:00">2022-05-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2022/03/05/CNN-A/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2022/03/05/CNN-A/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><em>본 포스트는 Hands-on Machine learning 2nd Edition, CS231n, Tensorflow 공식 document를 참조하여 작성되었음을 알립니다.</em></p>
<h3 id="Index"><a href="#Index" class="headerlink" title="Index"></a>Index</h3><ol>
<li>Introduction of Convolution Operation  </li>
<li>Definition of Convolutional Neural Network(CNN)</li>
<li>Back Propagation of CNN</li>
<li>Partice</li>
</ol>
<p>여기서는 1~3는 Part. A이고 4은 Part. B에서 다루도록 하겠다.<br><br></p>
<h3 id="Introduction-of-Convolution-Operation"><a href="#Introduction-of-Convolution-Operation" class="headerlink" title="Introduction of Convolution Operation"></a>Introduction of Convolution Operation</h3><hr>
<p>Convolutional Neural Network는 Convolution 연산을 Neural Network에 적용한 것이다. 따라서 이를 알기 위해서는 Convolution 연산을 먼저 알아야할 필요가 있다. 관련 학과 대학생이라면 아마도 신호와 시스템을 배우면서 이를 처음 접했을 것이다. Continuous domain, Discrete domian까지 이 연산을 정의될 수 있고 각각에 따라 계산 방법 또한 배웠을 것이다. 예를 들어서 2차원의 Image와 2차원의 Filter의 Convolution 연산을 수식으로 표현해 보도록 하겠다.</p>
<ul>
<li>Image 행렬 정의</li>
</ul>
<script type="math/tex; mode=display">
I(i, j)</script><p>Image의 $i$열, $j$행의 성분 </p>
<ul>
<li>Filter 행렬 정의</li>
</ul>
<script type="math/tex; mode=display">
K(i, j)</script><p>Filter의 $i$열, $j$행의 성분. 높이를 $k_1$, 너비를 $k_2$라고 가정.</p>
<ul>
<li>$I$와 $K$의 Convolution 연산</li>
</ul>
<script type="math/tex; mode=display">
(I*K)_{ij} = \sum_{m = 0}^{k_1 - 1} \sum_{n = 0}^{k_2 - 1}I(i - m, j - n)K(m, n)
\tag{equation (1)}</script><p>위 정의를 조금 틀면 다음과 같이도 표현이 가능하다.</p>
<script type="math/tex; mode=display">
(I*K)_{ij} = \sum_{m = 0}^{k_1 - 1} \sum_{n = 0}^{k_2 - 1}I(i + m, j + n)K(-m, -n)
\tag{equation (2)}</script><p>위와 같은 연산의 형태를 Correlation이라고 한다. 즉, Convolution 연산의 Filter를 $\pi$만큼 회전시킨다면 그것이 Correlation 연산인 것이다. 이는 아주 중요한 관계이므로 꼭 기억해 두도록 하자.</p>
<h3 id="Definition-of-Convolutional-Neural-Network-CNN"><a href="#Definition-of-Convolutional-Neural-Network-CNN" class="headerlink" title="Definition of Convolutional Neural Network(CNN)"></a>Definition of Convolutional Neural Network(CNN)</h3><hr>
<p>CNN의 정의는 위의 Convolution 연산을 사용하여 Weight와 Input을 계산하는 것이다. 매우 간단한 예시로 LeNet이라는 것을 보자. 너무 자주 나오는 예시라서 하품이 나올 것 같지만, 본래 기본이라는 것은 “쉬운”것이 아니라 “중요한”것이다.</p>
<p align="center"><img src="https://kimh060612.github.io/img/LeNet.png" width="100%"></p>

<p>그림에서의 각 파트를 분해해서 살펴보면 다음과 같다.</p>
<blockquote>
<p>Image Input -&gt; (Convolution) -&gt; Feature Map -&gt; (Pooling) -&gt; Feature Map -&gt; (Convolution) -&gt; Feature Map -&gt; (Pooling) -&gt; Feature Map -&gt; (Flatten) -&gt; Feature Vector -&gt; (FCNN) -&gt; Feature Vector -&gt; (FCNN) -&gt; Feature Vector -&gt; (Gaussian Connection) -&gt; Output Vector</p>
</blockquote>
<p>여기서 괄호 안에 들어 있는 것이 연산의 이름이다. FCNN은 다른 포스트에서 봤다고 가정하고, 여기서 주목해야 할 것은 Pooling Layer이다. </p>
<p>Pooling은 다양한 종류가 있는데 간단하게 한가지만 소개하자면 Max Pooling이 있다. 자세한 것은 Tensorflow 2 <a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D">공식 문서</a>를 참조하는 것이 더 좋을 것 같다.<br>여기서는 Pooling까지 자세히 다룰 이유는 없는 것 같다.</p>
<p>이제 본격적으로 Convolution 연산에 대해서 알아보도록 하겠다. 그 전에 FCNN 포스트에서도 그랬듯이, 수식 표현을 하기 위한 정의부터 하고 시작하자.</p>
<blockquote>
<p><em>Definition 1</em></p>
</blockquote>
<ul>
<li>$u_{ijm}^l$: $l$번째 층의 Feature Map의 $m$번째 Channel $i$행, $j$열의 성분</li>
<li>$x_{ijm}$: Input의 $m$번째 Channel의 $i$행, $j$열의 성분</li>
<li>$w_{ijmk}^l$: $l$번째 층의 Weight의 $k$번째 Kernel Set에 $m$번째 Channel, $i$행, $j$열의 성분</li>
<li>$b_m^l$: $l$번째 층의 $m$번째 Channel의 Bias</li>
</ul>
<p>이를 통해서 Convolution Layer를 수학적으로 표현해 보자면 다음과 같다.</p>
<script type="math/tex; mode=display">
u_{ijm}^l = \sum^{H - 1}_{p = 0} \sum^{W - 1}_{q = 0} \sum^{K - 1}_{k = 0} z^{l - 1}_{i+sp,j+sq,k}w^l_{p,q,k,m} + b^l_m
\tag{equation (3)}</script><p>여기서 $z$행렬은 $u$행렬에 activation function을 씌워 놓은 것이라고 생각하면 편하다.</p>
<p>이를 그림으로 표현해보면 다음과 같다. </p>
<p align="center"><img src="https://kimh060612.github.io/img/CNN.png" width="100%"></p>

<p>위 수식에서는 아직 정의되지 않은 부분, 설명되지 않은 부분이 많다. 첫번째로 $H$, $W$, $K$의 의미, 그리고 index 부분의 $s$의 의미이다. 또한, 위의 연산은 Correlation인데 왜 Convolution 연산이라고 하는 것일까? </p>
<p>일단 첫번째는 $H$, $W$, $K$인데, 이는 각각 Weight의 높이, 너비, 채널수이다. 그리고 index 부분의 $s$는 Stride이다. Convolution 연산의 Weight를 옮겨가면서 곱셉을 할때 얼마나 옮길지를 결정한다. 이 값을 키울수록 결과 이미지의 크기가 작아진다. 자세한 것을 하나하나 까볼려면 오래 걸리니, 이 부분은 혼자서 잘 생각해 보는게 좋을 것 같다. 어디까지나 이 문서는 입문서가 아니라는 점을 알아주었으면 좋겠다. 기존에 Tensorflow/Pytorch만을 사용하던 사람들에게 이론을 제공하고자 함이다.</p>
<h3 id="Back-Propagation-of-CNN"><a href="#Back-Propagation-of-CNN" class="headerlink" title="Back Propagation of CNN"></a>Back Propagation of CNN</h3><hr>
<p>자. 본격적으로 어려운 부분이다. 앞으로 Deep learning 강의를 써내려가면서 이보다 어려운 부분은 없다. 그리고 필자가 생각하기에도 쓸모가 없다. 단지 지적 유희를 위해서 읽어주기를 바라며 틀린 부분이 있다면 지적해 주기를 바란다. </p>
<p>그 전에, 왜 필자는 굳이 이 파트를 써내려 가는가를 적어보도록 하겠다. (잡담이니 굳이 안 읽어도 상관 없다.) 최근의 Deep learning 개발은 Auto Grad 계열의 알고리즘들을 활용하여 앞먹임 연산만을 정의하면 알아서 역전파 수식이 계산되어 BackPropagation을 편리하게 할 수 있다. 하지만 라이브러리에 모든 것을 맡기고 개발만 하는 것이 과연 좋은 개발자/연구원 이라고 할 수 있을까? 필요하다면 더 깊은 인사이트를 얻어서 문제를 해결해야할 필요가 있다. 이 글은 그런 사람들을 위함이기도 하고 나처럼 학문 변태들을 위한 것이기도 하다. 그러니 이 파트가 필요 없다고 판단되면 읽지 않는 것을 추천하고, 만약 틀린 것이 있다면 부디 연락해서 알려주었으면 좋겠다. 환영하는 마음으로 받아들이고 수정하도록 하겠다.</p>
<p>사족이 길었는데, 그래서 Back Propagation이 어떻게 정의되는 것일까? 큰 틀은 FCNN과 다를 바가 없다. Weight를 업데이트함에 있어서 Chain Rule을 활용하는 것이다. 그렇다면 어떻게 그것을 진행할 것인가?<br>우선 첫번째로 미분부터 써내려 가보자. </p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial w^l_{ijmk}}
\tag{def 2}</script><p>이것을 구해서 Weight를 업데이트하는 것이 Back Propagation의 핵심이다. 그렇다면 FCNN과 똑같이 일반화된 Delta Rule을 활용해 보는 것으로 시작하자. 이를 위해서 chain rule을 적용해 보면 다음과 같이 표현할 수 있다. </p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial w^l_{ijmk}} = \sum_{p=0}^{H'}\sum_{q=0}^{W'} \frac{\partial E}{\partial u^l_{pqm}}\frac{\partial u^l_{pqm}}{\partial w^l_{ijmk}}
\tag{equation (4)}</script><p>이때, $H’$, $W’$는 Convolution output의 결과 Feature map의 높이와 너비이다.<br>FCNN때와 똑같이 한다면 다음과 같이 Delta를 정의하고 식을 수정할 수 있다. </p>
<script type="math/tex; mode=display">
\delta_{pqm}^l = \frac{\partial E}{\partial u^l_{pqm}}
\tag{def 3}</script><p>그리고 다음과 같이 식을 유도하는 것이 가능하다. </p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial w^l_{ijmk}} = \sum_{p=0}^{H'}\sum_{q=0}^{W'} \delta_{pqm}^l  \frac{\partial u^l_{pqm}}{\partial w^l_{ijmk}} =  \sum_{p=0}^{H'}\sum_{q=0}^{W'} \delta_{pqm}^l z^{l - 1}_{i+sp, j+sq, k}
\tag{equation (5)}</script><p>결국 다음과 같이 표현 가능하다.</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial w^l_{ijmk}} = (\delta_{m}^l * z^{l-1}_k)_{ij}
\tag{equation (6)}</script><p>이제 delta를 정의했으니, 앞층의 delta와 뒷층의 delta간의 관계를 밝혀내서 연산을 효율화 하면 된다. 이 과정을 손으로 유도하는 것은 필자가 생각해도 실용적 측면에서는 정말 쓸데가 없다. 왜냐하면 현대의 신경망 구조는 너무 복잡해져서 이걸 유도했다 쳐도 다른 구조들이 정말 많이 때문에 써먹을 수가 없기 때문이다. 하지만 아주 제한적인 경우에 대해서 이걸 유도해 보도록 하겠다. </p>
<ol>
<li>Convolution - Convolution layer 에서의 Delta 점화식</li>
</ol>
<p>우선 다시 한번 delta에서 chain rule을 적용해 보도록 하겠다 . 이 과정은 FCNN에서도 했을 것이다. 따라서 최대한 간결하게 진행해 보도록 하겠다.  </p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial u^l_{pqm}} = \sum_{x=0}^{H''}\sum_{y=0}^{W''} \sum_{c=0}^{C''}  \frac{\partial E}{\partial u^{l + 1}_{p - sx,q - sy, c}} \frac{\partial u^{l + 1}_{p - sx,q - sy, c}}{\partial u^l_{pqm}}
\tag{equation (7)}</script><p>이때 $H’’$, $W’’$, $C’’$는 다음 층에서의 Feature Map의 크기이다.</p>
<p>이를 전개해 보면 다음과 같다.</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial u^l_{pqm}} = \sum_{x=0}^{H''}\sum_{y=0}^{W''} \sum_{c=0}^{C''} \delta_{p-sx,q-sy,c}^{l+1} w^{l+1}_{xycm} f'(u^{l}_{pqm})
\tag{equation (8)}</script><p>이는 다음과 같이 Convolution 연산으로 표현될 수 있다.</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial u^l_{pqm}} = (\delta^{l+1} * w_m^{l+1}) \odot f'(u^l_m)
\tag{equation (9)}</script><p>위의 $\odot$은 성분 끼리의 곱(element wise multiplication)을 의미한다. </p>
<ol>
<li>Convolution - Pooling - Convolution 에서의 Delta 점화식</li>
</ol>
<p>위에서 delta를 유도함에 있어서 한 층이 더 추가될 뿐이다. 다음과 같이 말이다. 여기서는 Max Pooling, Average Pooling을 예로 들어보겠다. </p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial u^l_{pqm}} = \sum_{x=0}^{H''}\sum_{y=0}^{W''} \sum_{c=0}^{C''}  \frac{\partial E}{\partial u^{l + 2}_{p' - sx,q' - sy, c}} \frac{\partial u^{l + 2}_{p' - sx,q' - sy, c}}{\partial u^{l + 1}_{p'q'm}} \frac{u^{l + 1}_{p'q'm}}{u^l_{pqm}}
\tag{equation (10)}</script><script type="math/tex; mode=display">
\frac{\partial E}{\partial u^l_{pqm}} = \sum_{x=0}^{H''}\sum_{y=0}^{W''} \sum_{c=0}^{C''}  \delta_{p-sx,q-sy,c}^{l+1} w^{l+1}_{xycm} \frac{\partial u^{l + 2}_{p' - sx,q' - sy, c}}{\partial u^{l + 1}_{p'q'm}} f'(u^{l}_{pqm})
\tag{equation (11)}</script><p>위 식에서 중간에 있는 $l+1$층이 Pooling 층이다. 이 미분은 다음과 같이 정의된다. </p>
<ul>
<li>Max Pooling의 경우</li>
</ul>
<script type="math/tex; mode=display">
\frac{\partial u^{l + 2}_{p' - sx,q' - sy, c}}{\partial u^{l + 1}_{p'q'm}} = 
\begin{cases}
    1 \space \space \space \space \space \text{if p, q 성분이 최댓값이었을 경우} \\
    0 \space \space \space \space \space \text{otherwise}
\end{cases} 
\tag{equation (12)}</script><ul>
<li>Average Pooling의 경우</li>
</ul>
<script type="math/tex; mode=display">
\frac{\partial u^{l + 2}_{p' - sx,q' - sy, c}}{\partial u^{l + 1}_{p'q'm}} = \frac{1}{H''' * W'''} 
\tag{equation (12)}</script><p>여기서 $H’’’$,$W’’’$는 Pooling Layer의 크기이다.</p>
<p>결국 Pooling layer까지 포함하면 다음과 같이 convolution 연산으로 정의할 수 있다.</p>
<script type="math/tex; mode=display">
\delta_{pqm}^l = \text{Upsampling}[(\delta^{l + 2} * w^{l + 2}_m)] \odot f'(u^l_m)
\tag{equation (13)}</script><p>어디까지나 이렇게 연산을 할 수 있는 이유는 Pooling layer는 업데이트를 할 필요가 없기 때문이다. 만약 업데이트를 할 피라미터가 있다면 “제대로” 다시 delta rule의 방정식을 수정해 줘야 한다.</p>
<ol>
<li>이게 정말 쓸데 없는 이유</li>
</ol>
<p>현대의 신경망은 에시당초 Convolution layer에서 탈각하는 분위기 인데다가 Convolution - Feed Forward 관계나 ResNet같은 현대의 신경망 구조에서는 이런 복잡한 수식으로 구현하는 것은 사실상 불가능에 가깝다. 그러니 우리는 Autograd를 믿고 이런건 그냥 지적 유희로만 알아 두도록 하자.</p>
<p>그리고 마지막으로 Bias의 Update 방법을 알아보면 다음과 같다.</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial b_m^l} = \sum^{H_o - 1}_{p = 0} \sum^{W_o - 1}_{q = 0} \frac{\partial E}{\partial u_{pqm}^l} \frac{\partial u_{pqm}^l}{\partial b_m^l} 
\tag{equation (14)}</script><p>이때, 곱셈 term의 뒷 항은 전부 1이므로 다음과 같은 식이 성립한다.</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial b_m^l} = \sum^{H' - 1}_{p = 0} \sum^{W' - 1}_{q = 0} \delta_{pqm}^l
\tag{equation (15)}</script><p>이걸로 CNN의 이론 부분은 끝났다. 다음에는 실습 부분으로 찾아오도록 하겠다.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kimh060612.github.io/2022/03/05/FCNN-B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Michael Kim">
      <meta itemprop="description" content="Backend Engineering, Deep learning, Algorithm & DS, CS 전공 지식 공부 저장소">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Michael's Study Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/05/FCNN-B/" class="post-title-link" itemprop="url">Fully Connected Neural Network 강의 내용 part.B</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-03-05 17:33:29" itemprop="dateCreated datePublished" datetime="2022-03-05T17:33:29+09:00">2022-03-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-05-30 00:00:32" itemprop="dateModified" datetime="2022-05-30T00:00:32+09:00">2022-05-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2022/03/05/FCNN-B/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2022/03/05/FCNN-B/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><em>본 포스트는 Hands-on Machine learning 2nd Edition, CS231n, Tensorflow 공식 document를 참조하여 작성되었음을 알립니다.</em></p>
<h6 id="Index"><a href="#Index" class="headerlink" title="Index"></a>Index</h6><ol>
<li><del>Basic of Neural Network</del>  </li>
<li><del>Definition of Fully Connected Neural Network(FCNN)</del></li>
<li><del>Feed Forward of FCNN</del></li>
<li><del>Gradient Descent</del></li>
<li><del>Back Propagation of FCNN</del></li>
<li>Partice(+ 부록 Hyper parameter tuning)</li>
</ol>
<p><br></p>
<h4 id="6-Partice-부록-Hyper-parameter-tuning"><a href="#6-Partice-부록-Hyper-parameter-tuning" class="headerlink" title="6. Partice(+ 부록 Hyper parameter tuning)"></a>6. Partice(+ 부록 Hyper parameter tuning)</h4><hr>
<p>자 실습 시간이다. 왜 실습을 Part. B로 뺐느냐? FCNN이 뭐 할게 있다고?<br>뭐 할게 있겠다. Tensorflow 2가 대충 어떻게 이루어 졌는지 설명하기 위해 분량 조절을 위해서 뺀것이다.<br>무엇보다 Part. A 쓰는데 수식을 너무 많이 써서 힘들어서 분리했다. <del>Tlqkf</del><br>자, 우선 실습에 들어가기에 앞서, TF 2를 애정하는 나로써는 앞으로 이 스터디 포스트에 작성될 대부분의 소스코드를 꿰뚫는 구현 체계를 먼저 설명하고 넘어가겠다.<br>다음 사진을 보자.</p>
<p align="center"><img src="http://kimh060612.github.io/img/API.png" width="70%" height="70%"></p>

<p><em>출처: pyimagesearch blog: <a target="_blank" rel="noopener" href="https://www.pyimagesearch.com/2019/10/28/3-ways-to-create-a-keras-model-with-tensorflow-2-0-sequential-functional-and-model-subclassing/">링크</a></em></p>
<p>위 그림에서 필자는 대부분의 코드를 <strong>Model Subclassing</strong> 방식으로 구현할 것이다. 구현 하면서 설명할 터이니 잘 따라와 주기를 바란다.<br>여기서부터는 대학교 강의 수준의 <strong>객체지향프로그래밍</strong> 지식을 갖추지 않으면 읽기 힘들 수 있다. “상속”, “오버라이딩”의 개념이라도 살펴보고 오자.</p>
<h6 id="6-1-Model-Subclassing"><a href="#6-1-Model-Subclassing" class="headerlink" title="6-1. Model Subclassing"></a>6-1. Model Subclassing</h6><hr>
<p>우선 준비한 소스부터 보고 시작하자.<br><em>file: model/FCNN1.py</em><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FCNN</span>(keras.Model):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, _units = [<span class="number">56</span>, <span class="number">56</span>], _activation = [<span class="string">&#x27;relu&#x27;</span>, <span class="string">&#x27;relu&#x27;</span>], **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)</span><br><span class="line">        self.Hidden1 = keras.layers.Dense(units=_units[<span class="number">0</span>], activation=_activation[<span class="number">0</span>], kernel_initializer=<span class="string">&quot;normal&quot;</span>)</span><br><span class="line">        self.Hidden2 = keras.layers.Dense(units=_units[<span class="number">1</span>], activation=_activation[<span class="number">1</span>], kernel_initializer=<span class="string">&quot;normal&quot;</span>)</span><br><span class="line">        self._output = keras.layers.Dense(units=<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, Input</span>):</span><br><span class="line">        hidden1 = self.Hidden1(Input)</span><br><span class="line">        hidden2 = self.Hidden2(hidden1)</span><br><span class="line">        Output = self._output(hidden2)</span><br><span class="line">        <span class="keyword">return</span> Output</span><br></pre></td></tr></table></figure><br>자, 별거 없다. keras를 써봤다면 뭔지 바로 감이 올 것이다.<br>이 부분에 대해서는 함수에 대한 설명보다는 class에 대한 설명을 해야할 것 같다. 바로 FCNN class가 상속을 받은 부모 클래스인 keras.Model 클래스에 관해서이다.</p>
<p>keras.Model class는 케라스에서 Deep learning을 진행하는 모델을 정의해주는 class이다. 우리가 이미 존재하는 layer를 가져다가 특정 순서로 연산을 진행하는 graph를 만들어 내기 위한 class이다. 하지만 그렇게 어렵게 생각하지 말자. 사용하는 것을 보면 바로 답이 나온다.</p>
<p>여기서는 생성자와 call이라는 함수를 오버라이딩을 통해서 사용자가 재 정의를 해서 사용한다. call은 우리가 구현하고자 하는 model이 feed forward를 진행할때 호출되는 함수이다. 생성자는 사용할 폭이 넓다. 여기서는 model을 구성하는 layer를 정의하는데 사용하였는데, 꼭 그 역할만 할 필요는 없는 것이다.</p>
<p>가타부타 말이 많았는데, 실제 어떻게 동작을 시키는가?</p>
<p><em>file: train_MNIST.py</em><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> model.FCNN1 <span class="keyword">import</span> FCNN</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">mnist = keras.datasets.mnist</span><br><span class="line"></span><br><span class="line">(train_img, train_labels), (test_img, test_labels) = mnist.load_data()</span><br><span class="line"></span><br><span class="line">train_img, test_img = train_img.reshape([-<span class="number">1</span>, <span class="number">784</span>]), test_img.reshape([-<span class="number">1</span>, <span class="number">784</span>])</span><br><span class="line"></span><br><span class="line">train_img = train_img.astype(np.float32) / <span class="number">255.</span></span><br><span class="line">test_img = test_img.astype(np.float32) / <span class="number">255.</span></span><br><span class="line"></span><br><span class="line">model = FCNN()</span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adam&#x27;</span>, loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>, metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br><span class="line">model.fit(train_img, train_labels, batch_size = <span class="number">32</span>, epochs = <span class="number">15</span>, verbose = <span class="number">1</span>, validation_split = <span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line">model.evaluate(test_img, test_labels, verbose=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><br>간단하다. keras.Model class를 상속 받았으니, 그곳에 있는 기본 함수들을 모두 사용할 수있다. fit method로 학습을 진행하고 evalute로 test데이터로 모델을 평가한다.<br>이는 기존에 keras의 사용법과 별반 다른게 없다.</p>
<p>여기까지는 쉽다. 하지만 이러고 끝낼거면 시작도 하지 않았다.</p>
<p>다음을 진짜 자세하게 설명할 것이다.</p>
<p><em>file: model/layer.py</em><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line"><span class="comment"># Weight Sum을 진행하는 Layer를 정의하는 예제 1</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">WeightSum</span>(keras.layers.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, units=<span class="number">32</span>, input_dim=<span class="number">32</span>, trainable=<span class="literal">True</span>, name=<span class="literal">None</span>, dtype=<span class="literal">None</span>, dynamic=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(trainable=trainable, name=name, dtype=dtype, dynamic=dynamic, **kwargs)</span><br><span class="line">        <span class="comment"># tf.Variable을 활용하는 예시</span></span><br><span class="line">        <span class="comment"># 가중치 행렬을 초기화 하기 위한 객체</span></span><br><span class="line">        w_init = tf.random_normal_initializer()</span><br><span class="line">        <span class="comment"># 가중치 행렬을 tf.Variable로 정의함. 당연히 학습해야하니 training parameter를 true로 둠.</span></span><br><span class="line">        self.Weight = tf.Variable(</span><br><span class="line">            initial_value = w_init(shape=(input_dim, units), dtype=<span class="string">&quot;float32&quot;</span>),</span><br><span class="line">            trainable = <span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line">        b_init = tf.zeros_initializer()</span><br><span class="line">        <span class="comment"># 위랑 같음. 별반 다를거 X</span></span><br><span class="line">        self.Bias = tf.Variable(</span><br><span class="line">            initial_value = b_init(shape=(units, ), dtype=<span class="string">&quot;float32&quot;</span>),</span><br><span class="line">            trainable = <span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># add_weight를 활용하는 예시 - Short Cut</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        # 이는 keras.layers.Layer의 method 중에서 add_weight를 사용하는 방법임. </span></span><br><span class="line"><span class="string">        # 주로 training을 시키기 위한 행렬을 이렇게 선언해서 나중에 편하게 불러오기 위한 목적이 큼. </span></span><br><span class="line"><span class="string">        self.Weight = self.add_weight(</span></span><br><span class="line"><span class="string">            shape=(input_dim, units), initializer=&quot;random_normal&quot;, trainable=True</span></span><br><span class="line"><span class="string">        )</span></span><br><span class="line"><span class="string">        self.Bias = self.add_weight(shape=(units,), initializer=&quot;zeros&quot;, trainable=True)</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, Input</span>):</span><br><span class="line">        <span class="comment"># 행렬 곱을 위한 tf 함수임. 별거 없음</span></span><br><span class="line">        <span class="comment"># 그냥 U = WZ + B 구현한거. </span></span><br><span class="line">        <span class="keyword">return</span> tf.matmul(Input, self.Weight) + self.Bias</span><br><span class="line"></span><br><span class="line"><span class="comment"># Weight Sum을 진행하는 Layer를 정의하는 예제 2</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">WeightSumBuild</span>(keras.layers.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, _units = <span class="number">32</span>, trainable=<span class="literal">True</span>, name=<span class="literal">None</span>, dtype=<span class="literal">None</span>, dynamic=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(trainable=trainable, name=name, dtype=dtype, dynamic=dynamic, **kwargs)</span><br><span class="line">        self.units = _units</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build</span>(<span class="params">self, input_dim</span>):</span><br><span class="line">        <span class="comment"># 이 함수는 밑에서 자세히 설명함.</span></span><br><span class="line">        self.Weight = self.add_weight(</span><br><span class="line">            shape=(input_dim[-<span class="number">1</span>], self.units),</span><br><span class="line">            initializer = <span class="string">&quot;random_normal&quot;</span>,</span><br><span class="line">            trainable= <span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.Bias = self.add_weight(</span><br><span class="line">            shape = (self.units,),</span><br><span class="line">            initializer = <span class="string">&quot;random_normal&quot;</span>,</span><br><span class="line">            trainable = <span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, Input</span>):</span><br><span class="line">        <span class="comment"># 상기 동일.</span></span><br><span class="line">        <span class="keyword">return</span> tf.matmul(Input, self.Weight) + self.Bias</span><br></pre></td></tr></table></figure><br>위의 2개의 class는 하는 짓거리가 완벽하게 똑같다. 하지만 하는 짓거리는 같은데 아주 치명적인 부분이 조금 다르다. 바로 build 함수의 overwritting이다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#                입력 벡터/텐서의 크기를 받음</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build</span>(<span class="params">self, input_dim</span>):</span><br><span class="line">    <span class="comment"># 그에 따라서 Weight와 Bias의 차원을 결정함.</span></span><br><span class="line">    <span class="comment"># 물론, Bias는 차이가 없을 지언정, Weight는 크게 차이가 나게 된다.</span></span><br><span class="line">    self.Weight = self.add_weight(</span><br><span class="line">        shape=(input_dim[-<span class="number">1</span>], self.units),</span><br><span class="line">        initializer = <span class="string">&quot;random_normal&quot;</span>,</span><br><span class="line">        trainable= <span class="literal">True</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    self.Bias = self.add_weight(</span><br><span class="line">        shape = (self.units,),</span><br><span class="line">        initializer = <span class="string">&quot;random_normal&quot;</span>,</span><br><span class="line">        trainable = <span class="literal">True</span></span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>이 함수는 call이 호출되기 전에 무조건 실행되는 함수라고 생각하면 된다. 즉, 호출하기 전에 Weight를 정의하는 것이다. 즉, 입력 벡터의 크기에 따라 모델의 형태가 알아서 바꾸게 해줄 수 있는 것이다. 이는 Image를 처리할때 이점이 될 수 있다.<br>Image를 학습시킬때, 이러한 처리가 없으면 이미지를 전부 동일한 크기로 만들어 주어야 한다. 하지만 build 함수를 정의해서 그때 그때 입력 벡터/텐서에 따라 커널을 수정해 주면 굳이 그럴 필요가 없다. 전처리 비용이 줄어드는 것이다. </p>
<p>그리고, 이제 이를 학습시키기 위한 코드를 보도록 하자.</p>
<p><em>file: train_MNIST_2.py</em><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> model.FCNN2 <span class="keyword">import</span> FCNN</span><br><span class="line"></span><br><span class="line">EPOCHS = <span class="number">15</span></span><br><span class="line">LR = <span class="number">0.01</span></span><br><span class="line">BatchSize = <span class="number">32</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 이미지 불러오기</span></span><br><span class="line">mnist = keras.datasets.mnist</span><br><span class="line">(train_img, train_labels), (test_img, test_labels) = mnist.load_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 필요한 전처리</span></span><br><span class="line">train_img, test_img = train_img.reshape([-<span class="number">1</span>, <span class="number">784</span>]), test_img.reshape([-<span class="number">1</span>, <span class="number">784</span>])</span><br><span class="line">train_img = train_img.astype(np.float32) / <span class="number">255.</span></span><br><span class="line">test_img = test_img.astype(np.float32) / <span class="number">255.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Train-Validation Split</span></span><br><span class="line">validation_img = train_img[-<span class="number">18000</span>:]</span><br><span class="line">validation_label = train_labels[-<span class="number">18000</span>:]</span><br><span class="line">train_img = train_img[:-<span class="number">18000</span>]</span><br><span class="line">train_labels = train_labels[:-<span class="number">18000</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train Data의 규합. &amp; Batch 별로 쪼갬</span></span><br><span class="line">train_dataset = tf.data.Dataset.from_tensor_slices((train_img, train_labels))</span><br><span class="line">train_dataset = train_dataset.shuffle(buffer_size=<span class="number">1024</span>).batch(BatchSize)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Validation Data의 규합 &amp; Batch 별로 쪼갬</span></span><br><span class="line">validation_dataset = tf.data.Dataset.from_tensor_slices((validation_img, validation_label))</span><br><span class="line">validation_dataset = validation_dataset.batch(BatchSize)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Optimizer &amp; Loss Function 정의</span></span><br><span class="line">optimizer = keras.optimizers.Adam(learning_rate=LR)</span><br><span class="line">loss_function = keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 학습이 잘 되고 있나 확인하기 위한 지표를 확인하기 위함.</span></span><br><span class="line">train_accuracy = keras.metrics.SparseCategoricalAccuracy()</span><br><span class="line">val_acc_metric = keras.metrics.SparseCategoricalAccuracy()</span><br><span class="line"></span><br><span class="line">model = FCNN()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Custom Training을 위한 반복문</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCHS):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Epoch %d start&quot;</span>%epoch)</span><br><span class="line">    <span class="keyword">for</span> step, (x_batch, y_batch) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_dataset):</span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">            <span class="comment"># Model의 Feed Forward</span></span><br><span class="line">            logits = model(x_batch, training=<span class="literal">True</span>)</span><br><span class="line">            <span class="comment"># Feed Forward 결과를 바탕으로 Loss를 구함</span></span><br><span class="line">            loss_val = loss_function(y_batch, logits)</span><br><span class="line">        <span class="comment"># 위의 과정을 바탕으로 gradient를 구함</span></span><br><span class="line">        grad = tape.gradient(loss_val, model.trainable_weights)</span><br><span class="line">        <span class="comment"># Optimizer를 통해서 Training Variables를 업데이트</span></span><br><span class="line">        optimizer.apply_gradients(<span class="built_in">zip</span>(grad, model.trainable_weights))</span><br><span class="line">        <span class="comment"># Batch 별로 Training dataset에 대한 정확도를 구함.</span></span><br><span class="line">        train_accuracy.update_state(y_batch, logits)</span><br><span class="line">        <span class="keyword">if</span> step % <span class="number">500</span> == <span class="number">0</span> :</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Training loss at step %d: %.4f&quot;</span>%(step, loss_val))</span><br><span class="line">    <span class="comment"># 정확도를 규합해서 출력</span></span><br><span class="line">    train_acc = train_accuracy.result()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Training acc over epoch: %.4f&quot;</span> % (<span class="built_in">float</span>(train_acc),))</span><br><span class="line">    train_accuracy.reset_states()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Validation을 진행함.</span></span><br><span class="line">    <span class="keyword">for</span> x_batch_val, y_batch_val <span class="keyword">in</span> validation_dataset:</span><br><span class="line">        <span class="comment"># Validation을 위한 Feed Forward</span></span><br><span class="line">        val_logits = model(x_batch_val, training = <span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># Batch 별로 Validation dataset에 대한 정확도를 구함</span></span><br><span class="line">        val_acc_metric.update_state(y_batch_val, val_logits)</span><br><span class="line">    <span class="comment"># 구한 정확도를 규합하여 출력함.</span></span><br><span class="line">    val_acc = val_acc_metric.result()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Validation acc: %.4f&quot;</span> % (<span class="built_in">float</span>(val_acc),))</span><br><span class="line">    val_acc_metric.reset_states()</span><br></pre></td></tr></table></figure><br>이는 TF2에서 추가된 tf.GradientTape를 통해서 사용자 정의 학습 루프를 만든 것이다. 각 줄마다 주석을 달아 놓았으니 Part. A의 내용을 숙지했다면 그렇게 어렵지 않게 알아들을 수 있을 것이다. </p>
<p>이제, 한가지 의문이 든다. 그렇다면 위의 Hyper parameter들을 변화시켜가면서 model을 최적화 하려면 노가다 밖에 답이없는건가? 답은 아니다. 이번에는 맛보기만 보여줄 것이다. 이는 scikit learn의 RandomizedSearchCV를 통해서 확인할 수 있다.</p>
<p><em>file: RandomSearch.py</em><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> reciprocal</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> RandomizedSearchCV</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">mnist = keras.datasets.mnist</span><br><span class="line">(train_img, train_labels), (test_img, test_labels) = mnist.load_data()</span><br><span class="line">train_img, test_img = train_img.reshape([-<span class="number">1</span>, <span class="number">784</span>]), test_img.reshape([-<span class="number">1</span>, <span class="number">784</span>])</span><br><span class="line">train_img = train_img.astype(np.float32) / <span class="number">255.</span></span><br><span class="line">test_img = test_img.astype(np.float32) / <span class="number">255.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 여기 parameter들의 Key는 무적권 입력 함수의 parameter와 같아야 한다. 아마도 **kwargs로 한번에 보내버리는 것일거다.</span></span><br><span class="line">param_distribution = &#123;</span><br><span class="line">    <span class="string">&quot;n_hidden&quot;</span>: [<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],</span><br><span class="line">    <span class="string">&quot;n_neurons&quot;</span>: np.arange(<span class="number">1</span>,<span class="number">100</span>),</span><br><span class="line">    <span class="string">&quot;lr&quot;</span>: reciprocal(<span class="number">3e-4</span>, <span class="number">3e-2</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 이것을 사용하기 위해서는 모델을 만들어줄 함수가 하나 필요하다.</span></span><br><span class="line"><span class="comment"># 여기서는 그냥 Sequential API를 사용하였다. 생각하기 귀찮았다. ㅋ;; </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Build_model</span>(<span class="params">n_hidden = <span class="number">1</span>, n_neurons=<span class="number">30</span>, lr = <span class="number">3e-3</span>, input_shape=[<span class="number">784</span>]</span>):</span><br><span class="line">    model = keras.models.Sequential()</span><br><span class="line">    model.add(keras.layers.InputLayer(input_shape=input_shape))</span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="built_in">range</span>(n_hidden):</span><br><span class="line">        model.add(keras.layers.Dense(n_neurons, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">    model.add(keras.layers.Dense(units=<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line">    optimizer = keras.optimizers.SGD(learning_rate=lr)</span><br><span class="line">    model.<span class="built_in">compile</span>(optimizer=optimizer, loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>, metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="comment"># 이는 keras 모델을 scikit learn에서 관리하기 위해 호출하는 함수이다.</span></span><br><span class="line">keras_classify = keras.wrappers.scikit_learn.KerasClassifier(Build_model)</span><br><span class="line"><span class="comment"># 여기서부터 본론이다. parameter_distribution으로 주어진 집합 한에서 가장 좋은 성능의 모델을 탐색한다.</span></span><br><span class="line"><span class="comment"># 여기서는 Cross-Validation을 사용한다. cv 항은 몇개로 Validation-training set을 분리할지 정하는 것이다. </span></span><br><span class="line">rnd_search_model = RandomizedSearchCV(keras_classify, param_distributions=param_distribution, n_iter = <span class="number">10</span>, cv = <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 이제 주어진 데이터를 가지고 학습을 돌면서 최적의 모델을 탐색한다. </span></span><br><span class="line">rnd_search_model.fit(train_img, train_labels, epochs=<span class="number">10</span>, validation_data=(test_img,test_labels), callbacks=[keras.callbacks.EarlyStopping(patience=<span class="number">10</span>)])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(rnd_search_model.best_params_)</span><br></pre></td></tr></table></figure></p>
<p>오늘은 이것으로 끝내도록 하자.<br>다음 포스트는 Convolutional Neural Network를 오늘처럼 다뤄볼 예정이다.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kimh060612.github.io/2022/03/05/FCNN-A/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Michael Kim">
      <meta itemprop="description" content="Backend Engineering, Deep learning, Algorithm & DS, CS 전공 지식 공부 저장소">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Michael's Study Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/05/FCNN-A/" class="post-title-link" itemprop="url">Fully Connected Neural Network 강의 내용 part.A</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-03-05 17:32:02" itemprop="dateCreated datePublished" datetime="2022-03-05T17:32:02+09:00">2022-03-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-05-30 00:00:32" itemprop="dateModified" datetime="2022-05-30T00:00:32+09:00">2022-05-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2022/03/05/FCNN-A/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2022/03/05/FCNN-A/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><em>본 포스트는 Hands-on Machine learning 2nd Edition, CS231n, Tensorflow 공식 document를 참조하여 작성되었음을 알립니다.</em></p>
<h3 id="Index"><a href="#Index" class="headerlink" title="Index"></a>Index</h3><ol>
<li>Basic of Neural Network  </li>
<li>Definition of Fully Connected Neural Network(FCNN)</li>
<li>Feed Forward of FCNN</li>
<li>Gradient Descent</li>
<li>Back Propagation of FCNN</li>
<li>Hyper parameter tuning</li>
<li>Partice</li>
</ol>
<p>여기서는 1~5는 Part. A이고 6,7은 Part. B에서 다루도록 하겠다.<br><br></p>
<h3 id="Basic-of-Neural-Network"><a href="#Basic-of-Neural-Network" class="headerlink" title="Basic of Neural Network"></a>Basic of Neural Network</h3><hr>
<p>Neural Network의 기본을 설명할때 항상 나오는 것이 바로 뉴런 구조 사진이다. 여기서는 그들을 생략하도록 하겠다. 굳이 여기서 설명할 필요를 못 느끼겠다.<br>궁금하면 SLP(Single Layer Perceptron)와 MLP(Multi Layer Perceptron)에 대해서 구글링을 해보도록 하자.</p>
<p>우선 FCNN을 본격적으로 들어가기 전에, 지도 학습에 대한 Deep learning의 학습 프로세스에 대한 개략적인 모식도를 보고 가자.</p>
<p align="center"><img src="https://kimh060612.github.io/img/DeepProcess_1.png" width="100%"></p>

<p>이러한 모식도에 개략적인 생략이 들어 갔고, 반례도 충분히 있겠지만, 지도 학습의 형태로 학습하는 대부분의 Neural Network는 이런 Process로 학습을 하게 된다.<br>앞으로 나오는 신경망 구조들 중에서 위의 “출력”, “평가”, “학습”의 3가지의 과정을 집중적으로 기술할 생각이다. </p>
<p>그렇다면 Neural Network 구조를 활용해서 풀 수 있는 문제는 무엇이 있을까?<br>여기서는 크게 2가지만 다룰 예정이다. </p>
<ol>
<li><em>Classification</em>: 어떤 입력을 특정 범주로 분류해내는 문제. </li>
<li><em>Regression</em>: 어떤 변수들 사이의 관계를 도출해내는 문제. (Modeling에 가깝다고 생각하면 편함.)</li>
</ol>
<p>이러한 2개의 문제의 범주를 설명하고 넘어가는 이유는 Neural Network로 어떤 문제를 풀 것이냐에 따라서 학습 과정에서 다양한 부분이 상이하게 변하기 때문이다. 이는 나중에 따로 자세하게 다룰 예정이다. 지금은 일단 이런 것이 있다 라는 것만 알아두고 가자.</p>
<p><br></p>
<h3 id="Definition-of-FCNN"><a href="#Definition-of-FCNN" class="headerlink" title="Definition of FCNN"></a>Definition of FCNN</h3><hr>
<p>여기서는 Fully Connected Nueral Network의 정의를 먼저 알아보도록 하겠다. 간단하게 이를 표현하자면 다음과 같은 구조를 가지는 Neural Network를 일컫는다.</p>
<p align="center"><img src="https://kimh060612.github.io/img/FCNN.png" width="100%"></p>

<p><em>Figure 1: FCNN의 모식도 1</em><br>이렇게 앞층의 뉴런과 뒷층의 뉴런이 빠짐없이 전부 연결된 구조를 FCNN이라고 한다. 여기서 만약 독자가 MLP를 모르는 상태라면 위의 구조가 상당히 추상적으로 다가올 것이다. 간단하게 생각해서, 각 뉴런들은 숫자를 가지고 있다고 생각하면 된다. 다음 그림을 보면 훨씬 쉽게 이해할 수 있다.</p>
<p align="center"><img src="https://kimh060612.github.io/img/FCNN2.png" width="100%"></p>

<p><em>Figure 2: FCNN의 모식도 2</em></p>
<p>이렇게 층의 뉴런마다 간선으로 연결되어 있고 각 뉴런들은 뒤의 뉴런들에게 어떠한 가중치를 반영해서 자신의 값을 가지고 있다고 생각하면 된다. 이제부터 우리는 이러한 구조를 가진 Neural Network가 어떻게 주어진 문제에 대해서 결과를 도출하고 학습을 진행하는지 알아보도록 하겠다. </p>
<p><br></p>
<h3 id="Feed-Forward-of-FCNN"><a href="#Feed-Forward-of-FCNN" class="headerlink" title="Feed Forward of FCNN"></a>Feed Forward of FCNN</h3><hr>
<p>이 과정은 학습이 되었건 되지 않았건 FCNN의 구조에서 입력을 통해 출력을 얻어내는 과정이다. 층층이 쌓인 Neural Network 구조에서 앞으로 계속 나아가면서 결과를 도출해 내는 모습에서 Feed Forward(앞 먹임)이라는 이름이 붙은 것이다.<br>앞으로 대부분의 예시는 <em>Figure 2</em>와 비슷한 형식으로 나올 것이다. 그리고 앞으로는 <em>뉴런</em>이라는 명칭보다는 <em>Unit</em>이라는 명칭을 사용할 것이다.<br>이제, 이전 슬라이드에 있는 것들을 기호를 명확하게 정의해 보도록 하겠다.</p>
<blockquote>
<p><em>Definition 1</em></p>
</blockquote>
<ul>
<li>$x_i$: 입력 벡터의 $i$번째 원소</li>
<li>$u_i^k$: $k$번째 Layer의 $i$번째 Hidden Units의 값.</li>
<li>$w^k_{ij}$: $k$번째 층의 $i$번째 Unit과 $k-1$번째 층의 $j$번째 Unit을 이어주는 가중치</li>
<li>$y_i$: 출력층의 $i$번째 Unit</li>
<li>$n_k$: $k$번째 층의 Unit의 갯수</li>
</ul>
<p>그렇다면 이제 Feed Forward를 위해서 가장 중요한 부분을 설명하도록 하겠다. Weighted Sum(가중합)에 대한 부분이다.<br>앞선 <em>Figure 1</em>과 <em>Figure 2</em>에서 앞층과 뒷층의 관계를 우리는 다음과 같이 정의한 것이다.</p>
<script type="math/tex; mode=display">
u_i^k = \sum_{j=1}^{n^{k-1}}w^k_{ij}u^{k-1}_j
\tag{Equation (2)}</script><p>정말 단순하게 생각해서 이전 유닛에 해당하는 가중치만큼 앞의 유닛에 반영해 주면 되는 것이다. 이렇게 뒤의 층에서 앞의 층으로 순차적으로 계산해 나가면 된다.</p>
<p>하지만 여기에는 치명적인 문제점이 있다. 바로, 이렇게 된다면 Neural Network 구조로 Linear Function 밖에 표현할 수 없다는 것이다.<br>위의  $equation (1)$의 형식으로 Feed Foward를 진행하게 된다면 층을 쌓는 것이 의미가 없어진다. 그 이유는 단순히 일변수 선형 함수를 생각해보면 알 수 있다.<br>우리가 $y = a_ix$라는 임의의 $N$개의 함수들을 층층이 쌓는다 할저언정 그것은 새로운 선형 함수 $y = a’x, \quad a’ = a_1a_2a_3\cdot\cdot\cdot a_N$ 를 만들어 내는 것과 별반 다를 것이 없다. 이렇게 되면 가장 대표적인 문제점은 바로 XOR문제를 해결할 수 없다는 것이다. 이는 너무 보편적인 문제라서 그냥 구글링 하면 나보다 설명 잘해놓은 글이 널려있다. 따라서 여기서는 생략한다.<br>똑같은 논리이다. 이에 대한 자세한 논의는 Appendix. A에 자세히 기술하도록 하겠다.</p>
<p>이러한 문제를 해결하고 더 다양한 함수를 신경망이 표현할 수 있게 하기 위해서 Activation function과 Bias를 적용하게 된다.</p>
<blockquote>
<p>Activation Function(활성 함수)</p>
</blockquote>
<p>신경망이 Linear한 함수만을 근사시키는 것을 방지하기 위해서 중간에 Non Linear Function을 Unit에 적용하게 된다. 이때 이러한 함수를 Non linear function이라고 한다. 그렇다고, 전부 Non linear인 것은 아니다. 하지만 Non linear function이여야 앞선 문제가 발생하지 않을 것이다. 그 종류는 대략 다음과 같다.</p>
<ol>
<li>Sigmoid: $\sigma(x) = \frac{1}{1+\exp(-x)}$</li>
<li>ReLU(Rectified Linear Unit): $max(0, x)$</li>
<li>Leaky ReLU: $max(0.1x, x)$</li>
<li>Hyperbolic tangent: $\tanh(x) = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(-x)}$</li>
<li>ELU(Exponential Linear Unit): $\sigma(x) = xu(x) + a(\exp(x)-1)u(-x)$</li>
</ol>
<p>이것 뿐만 아니라 진짜 개 많다. 나머지는 Tensorflow 공식 Document나 구글링을 통해서 알아보도록 하라. </p>
<p>이렇게 Activation Function을 씌운 값을 앞으로는 다음과 같이 표현하도록 하겠다.</p>
<blockquote>
<p><em>Defintion 2</em>: 활성함수를 적용한 Unit의 값.</p>
</blockquote>
<script type="math/tex; mode=display">z_i^k = f(u^k_i)</script><blockquote>
<p>Bias (편향)</p>
</blockquote>
<p>Bias로써는 Non Linear성을 신경망에 추가할 수는 없다. 하지만 Activation Function을 평행 이동 시켜주는 역할을 하게 해서 입력이 0인 지점에서의 신경망의 자율도를 높여주게 된다. 예를 들어보자. 위에 예시를 든 Activation function들에서는 예를 들어서 $(0, 32)$같은 점을 표현할 수 없다. 그렇기 때문에 Bias를 주어서 평행 이동을 시켜주는 것이다. 이것의 유무가 신경망의 학습 양상과 성능이 크게 차이를 주는 경우가 많다. </p>
<p>즉, 최대한 많은 함수를 근사할 수 있도록 위와 같은 설정을 추가해 주는 것이다.<br>이러면 어떤 잼민이는 이런 질문을 할 수 있을 것이다.</p>
<blockquote>
<p>???: 아 ㅋㅋㅋㅋ 그런 근사 못 하는 함수는 어쩔건데요 ㅋㅋ루삥뽕</p>
</blockquote>
<p>걱정 마라. 이미 <em>만능 근사 정리</em>(<em>Universal Approximation Theorem</em>, <em>시벤코 정리</em>)에 의해서 증명되었다. 그냥 안심하고 쓰면 된다.</p>
<p>그렇다면 Bias를 적용해서 다시금 Unit의 값을 작성해 보자.</p>
<blockquote>
<p><em>Definition 3</em>: Bias를 적용한 Unit의 값  </p>
</blockquote>
<script type="math/tex; mode=display">
u_i^k = \sum_{j=1}^{n_{k-1}}w^k_{ij}z^{k-1}_j + b^k_i
\tag{Equation (2)}</script><p><br></p>
<p align="center"><img src="https://kimh060612.github.io/img/FCNN3.png" width="100%"></p>

<p><em>Figure 3: Bias를 반영한 Neural Network의 표현</em></p>
<p>이제 이를 행렬로 표현해보자. 행렬로써 표현하여 식을 짧고 간편하게 정리할 수 있으며, 컴퓨터에서의 구현을 다소 직관적이고 편하게 할 수 있다.</p>
<script type="math/tex; mode=display">
U^k = 
\begin{bmatrix}
    u^k_1 && u^k_2 && \dots && u^k_{n_k}
\end{bmatrix} 
^T
\tag{Def. 4}</script><p><br></p>
<script type="math/tex; mode=display">
W^k = \begin{bmatrix}
    w^k_{11} && w^k_{12} && \cdots && w^k_{1n_{k-1}} \\
    w^k_{21} && w^k_{22} && \cdots && w^k_{2n_{k-1}} \\
    \vdots && \ddots && \ddots && \vdots \\
    w^k_{n_k1} && w^k_{n_k2} && \cdots && w^k_{n_kn_{k-1}} \\
\end{bmatrix}
\tag{Def. 5}</script><p><br></p>
<script type="math/tex; mode=display">
Z^{k-1} = 
\begin{bmatrix}
    z^{k-1}_1 && z^{k-1}_2 && \dots && z^{k-1}_{n_{k-1}}
\end{bmatrix}^T
\tag{Def. 6}</script><p><br></p>
<script type="math/tex; mode=display">
B^k = 
\begin{bmatrix}
    b^k_1 && b^k_2 && \dots && b^k_{n_k}
\end{bmatrix}^T
\tag{Def. 7}</script><p><br></p>
<script type="math/tex; mode=display">
\therefore U^k = W^kZ^{k-1} + B^k
\quad
(Z^k = f(U^k), \quad where \space f: \mathbb{R}^{n^k} \to \mathbb{R}^{n^k})</script><p><br></p>
<h3 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h3><hr>
<p>이번 장에서는 Gradient Descent에 대해서 다룰 예정이다. 하지만 너무 깊게는 안 다룰 생각이다. 그렇다고 걱정할 필요는 없다. 나중에 존나 자세하게 다룰거니깐 걱정은 붙들어 매도록 하자. </p>
<p align="center"><img src="https://kimh060612.github.io/img/GradientDescent.gif" width="100%"></p>

<p><em>Figure 4. Gradient Descent을 묘사하는 Figure</em><br>Gradient Descent는 간단하게 설명해서 Gradient를 활용해서 Objective Function(목적 함수)의 최저점을 찾는 것이 목표이다. 위의 사진처럼 말이다. 이의 명확한 표현을 직관적으로 납득시키기 위해서 다음의 사진을 보자.</p>
<p align="center"><img src="https://kimh060612.github.io/img/GD.jpg" width="70%"></p>

<p><em>Figure 5. Gradient Descent를 직관적으로 설명하기 위한 그림</em></p>
<p>이처럼 목적 함수의 미분값을 활용하여 목적 함수의 최저점을 찾는 것이 가능하다. 이에 대한 자세한 설명은 진짜 나중에 질리도록 해주도록 하겠다. 걱정하지 말아달라.</p>
<p>그렇다면 이제 Neural Network에서 목적 함수는 어떤 것을 사용하는지 알아봐야한다. 이것도 지금은 그냥 “그런 것이 있다”라고만 생각하자. 이 파트는 신경망을 이해하는데 있어서 중요하고 심도있는 내용이기 때문에 나중에 Post 하나를 통으로 열어서 집중적으로 설명할 것이다.</p>
<p>이번 포스트에서는 목적함수로써 주로 2가지를 다룰 것이다.</p>
<ol>
<li>Mean Squared Error</li>
</ol>
<script type="math/tex; mode=display">
E = \frac{1}{2N}\sum_i(y_i - t_i)^2
\tag{Equation (3)}</script><ol>
<li>Cross-Entropy</li>
</ol>
<script type="math/tex; mode=display">
E = -\sum_c t_c\ln y_c
\tag{Equation (4)}</script><p>$Equation 3$은 주로 regression 문제에서 목적 함수로 쓰이고, $Equation 4$는 주로 classification 문제에서 쓰인다.<br>이렇게 Error function이 있는데 Update를 어떻게 한다고 하는 것일까? 다음과 같은 공식으로 Update 하면 된다</p>
<script type="math/tex; mode=display">
w_{t+1} \gets w_t - \alpha \frac{\partial E}{\partial w_t}
\quad
\alpha: learning \space rate
\tag{Equation (5)}</script><p>직관적으로 생각해보면, 그래프의 미분값이 음수이면 아래로 볼록한 이차 함수에서 극소점이 오른쪽에 있다는 것이다. 그렇다면 변수에 양수를 더해줘야 극소점에 다가갈 것이다. 반대도 마찬가지이다. 그리고 한번 Update를 할때 너무 변화폭을 급하게 하지 않기 위해서 learning rate를 곱해줘서 update를 해준다. 그렇게 적절한 learning rate를 지정하여 몇번 반복해주면 최저점에 수렴해 있을 것이다.</p>
<p>집요하지만, 이번 주제에 관한 내용은 나중에 “Optimization, Error function, and Problem”에 관한 포스트에서 자세하게 다룰 것이다. 그러니 지금은 직관적으로만 학습의 Process를 이해하는데 사력을 다해주기를 바란다.</p>
<h3 id="Back-propagation-of-FCNN"><a href="#Back-propagation-of-FCNN" class="headerlink" title="Back-propagation of FCNN"></a>Back-propagation of FCNN</h3><hr>
<p>이번에는 어떻게 신경망에 Gradient Descent를 적용할 것인지에 대한 내용이다. 위의 Gradient Descent를 배운 사람이라면 <script type="math/tex">Equation 5</script>를 활용하여 가중치의 Update를 다음과 같이 해줘야 할 것이라는데 이견을 가지지는 않을 것이다.</p>
<script type="math/tex; mode=display">
w^m_{ij} \gets w^m_{ij} - \alpha \frac{\partial E}{\partial w^m_{ij}}
\tag{Equation (6)}
\quad
, m \in [1, K] \cap \N</script><p>여기서 $K$는 층의 갯수이다. </p>
<p>이때 수치해석을 공부한 사람이라면, 다음과 같은 말을 할 수 있다.</p>
<blockquote>
<p>적당한 $\epsilon$을 선택하고, 수치 미분을 해주면 되지 않을까?</p>
</blockquote>
<p>뭐, 틀린 말은 아니다. 실제로도 좋은 방법이 될 수 있다.(<em>신경망이 아니라면 말이지</em>) 하지만 이건 Neural Network이다. 미친 연산량을 자랑하는 이쪽 바닥에서 안이하게 접근했다가는 피를 볼 수 있다.<br>요즘은 많은 알고리즘들이 발달해서 어떨지는 모르지만, 기본적인 수치 미분은 여기서 좋은 방법이 아니다. 우선 수치 미분을 진행하는 방법은 다음과 같다.</p>
<script type="math/tex; mode=display">
    \frac{df}{dx} \approx \frac{f(x+\epsilon) - f(x)}{\epsilon}</script><p>우선 이걸 신경망에 적용시키기 위해 가장 먼저 떠오르는 방법은 다음과 같을 것이다.</p>
<ol>
<li>$f(x+\epsilon)$를 계산한다.</li>
<li>$f(x)$를 계산한다.</li>
<li>미분을 계산한다.</li>
</ol>
<p>자, 1,2번의 과정을 신경망에 적용해 주려면 당신은 2번의 함수 값을 계산해 줘야한다. 만약, 계산량이 크지 않은 함수라면 빨리 되겠지만, 이건 신경망이다. <em>Figure 3</em>을 보자. 저기 그려져 있는 모든 간선에 대해서 Update를 해줘야하는데 진짜 어림도 없는 방법이 아닐 수 없다. 이걸 parameter마다 해준다? 진짜 어림도 없는 소리가 된다.</p>
<p>그렇다면 다음으로 패기 넘치는 잼민이가 다음과 같이 말한다.</p>
<blockquote>
<p>그냥 간단한 함수만 쓰고 손으로 계산하면 안됨? 그것도 못하누 ㅋㅋ루삥뽕</p>
</blockquote>
<p>^^ 열심히 해보십셔 잼민님^^(<em>말도 안되는 소리는 집어 치우자</em>)</p>
<p>여기서 tensorflow의 자동 미분(Autograd)가 나오는데, 우리는 이 전에 수학적인 부분으로 이를 접근해 보도록하자.</p>
<p>가장 먼저, 이는 엄청나게 곂곂이 쌓인 함수이다. 일변수 Scalar 함수로 예시를 들어보면 다음과 비슷한 것이라고 생각할 수 있다.</p>
<script type="math/tex; mode=display">y = f(f(f( \cdots f(x) \cdots)))</script><p>이를 미분해야한다고 생각해보자. Calculus를 배웠다면 가장 먼저 떠오르는 것은 Chain Rule 일 것이다.</p>
<p>하지만, 어디서부터 어떻게 Chain Rule을 적용해야하는지 막막하다. 그 전에 우선 <strong>상대적으로 구히기 쉬울 것 같은</strong> 출력층 바로 이전의 가중치의 미분을 Chaine Rule을 통해서 구해보자.</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial w^K_{ij}} = \frac{\partial E}{\partial y_i}\frac{\partial y_i}{\partial u^K_i}\frac{\partial u^K_i}{\partial w^K_{ij}}
\tag{Equation (6)}</script><p>이는 출력층 바로 이전의 층의 가중치이기에, Chain Rule을 통해서 <strong>비교적 쉽게</strong> 구할 수 있다. 다음과 같이 말이다.<br>(Error function(목적 함수)는 MSE를 사용하였다고 생각하자.)</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial y_i} = \frac{1}{N}(y_i - t_i)
\quad  
\frac{\partial y_i}{\partial u^K_i} = f'(u^K_i)
\quad
\frac{\partial u^K_i}{\partial w^K_{ij}} = z^{K-1}_j
\tag{Equation (7)}</script><script type="math/tex; mode=display">
\therefore \frac{\partial E}{\partial w^K_{ij}} = \frac{1}{N}(y_i - t_i)f'(u^K_i)z^{K-1}_j
\tag{Equation (8)}</script><p>이렇게 성공적으로 $W^K$의 미분은 구했다고 치고, 어떻게 그 뒤의 층을 구할 것인가?<br>우선, 다음 그림을 보자.</p>
<p align="center"><img src="https://kimh060612.github.io/img/Layer.png" width="70%"></p>

<p><em>Figure 7. Neural Network 구조에서 일부를 떼어서 표현한 그림</em><br>이는 Neural Network 구조에서 일부를 떼어낸 것을 표현한 그림이다. 위의 $\vdots$가 그려진 Unit은 그냥 거기에 많은 수의 Unit들이 있다고 생각하면 될 것이다.<br>우리는 여기서 $W^k$의 $E$에 대한 미분을 구하는 것이 목표이다.<br>즉, $Equation (9)$를 구하는 것이 목표이다.</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial w^k_{ip}} \space = \space ?</script><p>그것을 위해서 우선, Chaine Rule을 진행해 보자.</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial w^k_{ip}} = \frac{\partial E}{\partial u^k_i}\frac{\partial u^k_i}{\partial w^k_{ip}}
\tag{Equation (9)}</script><p>여기까지만 봐서는 잘 모르겠다. 하지만 다음의 그림을 보고, $\frac{\partial E}{\partial u^k_i}$ 에 한번 더 Chain Rule을 적용해 보자.</p>
<p align="center"><img src="https://kimh060612.github.io/img/Layer2.png" width="70%"></p>

<p><em>Figure 6. $u^k_i$가 영향을 주는 앞 층의 Unit들</em><br>FCNN의 정의에 의해, $u^k_i$는 앞층의 모든 뉴런에 연결되어 있다. 그리고 그 앞 층의 뉴런들은 또 그 다음층으로 값을 전달하며, 점점 출력층에 가까워 지고, Error에 영향을 미치게 된다. 한마디로, 다음과 같이 Chain Rule을 펼칠 수 있다.</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial u^k_i} = \sum_{j=1}^{n_{k+1}} \frac{\partial E}{\partial u^{k+1}_j}\frac{\partial u^{k+1}_j}{\partial u^k_i}
\tag{Equation (10)}</script><p>이때 우리는 위의 $Equation (10)$에서 유사한 부분을 찾을 수 있다.</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial u^k_i} \quad and \quad \frac{\partial E}{\partial u^{k+1}_j}</script><p>그렇다면 우리는 다음과 같이 정의를 해보자.</p>
<script type="math/tex; mode=display">
\delta^k_i = \frac{\partial E}{\partial u^k_i}
\tag{Def. 8}</script><p>그렇다면 $Equation (10)$을 다음과 같이 작성할 수 있다.</p>
<script type="math/tex; mode=display">
\delta^k_i = \sum_{j=1}^{n_{k+1}} \delta^{k+1}_j\frac{\partial u^{k+1}_j}{\partial u^k_i}
\tag{Equation (11)}</script><p><strong>이 점화식이 이번 포스트의 핵심이다.</strong><br>그렇다면 식을 다시 정리해서, 최종적인 미분 값을 구해보자.</p>
<script type="math/tex; mode=display">
\frac{\partial u^{k+1}_j}{\partial u^k_i} = \frac{\partial u^{k+1}_j}{\partial z^k_i}\frac{\partial z^{k}_i}{\partial u^k_i} = w^{k+1}_{ji}f'(u^k_i)
\tag{Equation (12)}</script><p>그리고, $Equation (12)$를 통해서 $Equation (10)$을 다시 써보면 다음과 같다.</p>
<script type="math/tex; mode=display">
\delta^k_i = \sum_{j=1}^{n_{k+1}} \delta^{k+1}_jw^{k+1}_{ji}f'(u^k_i)
\tag{Equation (13)}</script><p>자! 다 왔다. 이제 최종 미분식을 다시 써보자.</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial w^k_{ip}} = \delta^k_i\frac{\partial u^k_i}{\partial w^k_{ip}} = \left(\sum_{j=1}^{n_{k+1}} \delta^{k+1}_jw^{k+1}_{ji}f'(u^k_i)\right )\frac{\partial u^k_i}{\partial w^k_{ip}}</script><script type="math/tex; mode=display">
\therefore \frac{\partial E}{\partial w^k_{ip}} = \left(\sum_{j=1}^{n_{k+1}} \delta^{k+1}_jw^{k+1}_{ji}f'(u^k_i)\right )z^{k-1}_p
\tag{Equation (14)}</script><p>이 포스트의 핵심은 이것이다.<br>결국 우리는 <strong>상대적으로 구하기 쉬운 출력층의 미분을 구하는 과정에서 미분을 구하기 어려운 층의 미분을 구할 수 있는 실마리를 얻은 것이다.</strong><br>무슨 말이냐? 우리는 $\delta^k_i$를 정의했다면, 출력층의 $\delta^K$도 구할 수 있을 것이다. 그렇다면 $K-1$층의 $\delta^{K-1}$는 $\delta^K$를 통해서 구할 수 있으니, 결국 $W^{K-1}$층의 미분도 구할 수 있다는 뜻이 된다.</p>
<p>본격적으로 $\delta^K$를 구해보자.</p>
<script type="math/tex; mode=display">
\delta^K_i = \frac{\partial E}{\partial y_i}\frac{\partial y_i}{\partial u^K_i} = \frac{1}{N}(y_i - t_i)f'(u^K_i)</script><p>그렇다. 바로 구할 수 있다. 그렇다면 이를 통해서 뒤에 있는 층의 미분을 차례로 구할 수 있는 것이다.<br>Feed Forward와는 달리, 이 과정은 방향이 반대로 진행된다. 따라서 이는 Back Propagation이라고 하며, 이렇게 $\delta$를 정의하여 미분을 구하는 방식을 <strong>일반화된 델타 규칙</strong>이라고 한다. </p>
<p>자, Weight의 미분은 구했으니, Bias의 미분도 구해야한다. 이는 매우 간단하다.</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial b^k_i} = \frac{\partial E}{\partial u^k_i}\frac{\partial u^k_i}{\partial b^k_i} = \delta^k_i\cdot 1
\tag{Equation (15)}</script><p>이렇게 우리는 $\delta$에 관한 값만 구하면 신경망을 Update 하기 위한 미분을 전부 구할 수 있다. 그렇기에, <em>핵심</em>이라고 할 수 있는 방정식 $Def. 8$, $Equation (13)$, $Equation (15)$, $Equation (14)$에 대해서 행렬로써 표현해 보자.</p>
<p><em>Definition of Delta</em></p>
<script type="math/tex; mode=display">
\Delta^k = \nabla_UE = \nabla_ZE \odot f'(U^k)
\tag{Def. 8}</script><p><em>Delta Matrix Relation</em></p>
<script type="math/tex; mode=display">
\Delta^k = \left( (W^{k+1})^T\Delta^k \right) \odot f'(U^k)
\tag{Equation (13)}</script><p><em>Bias Update Equation</em></p>
<script type="math/tex; mode=display">
\nabla_B E = \Delta^k
\tag{Equation (15)}</script><p><em>Weight Update Equation</em></p>
<script type="math/tex; mode=display">
\nabla_WE = \Delta^kZ^{k-1}
\tag{Equation (14)}</script><p>이걸로 FCNN의 Feed Forward, Back Propagation의 이론적인 부분이 끝났다. 이제 실습은 Part. B에서 마저 다룰 예정이다.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Michael Kim</p>
  <div class="site-description" itemprop="description">Backend Engineering, Deep learning, Algorithm & DS, CS 전공 지식 공부 저장소</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">23</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">15</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/kimh060612" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;kimh060612" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:kimh060612@khu.ac.kr" title="E-Mail → mailto:kimh060612@khu.ac.kr" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Michael Kim</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://michael-blog-1.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>

</body>
</html>
