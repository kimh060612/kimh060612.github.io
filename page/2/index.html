<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
<meta name="google-site-verification" content="L0ZN3xykszl3pBQeJ5QFwk98KKJ8kHDzGSBLhEtwmNU" />
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"kimh060612.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Backend Engineering, Deep learning, Algorithm &amp; DS, CS 전공 지식 공부 저장소">
<meta property="og:type" content="website">
<meta property="og:title" content="Michael&#39;s Study Blog">
<meta property="og:url" content="https://kimh060612.github.io/page/2/index.html">
<meta property="og:site_name" content="Michael&#39;s Study Blog">
<meta property="og:description" content="Backend Engineering, Deep learning, Algorithm &amp; DS, CS 전공 지식 공부 저장소">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Michael Kim">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://kimh060612.github.io/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Michael's Study Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><link rel="alternate" href="/rss2.xml" title="Michael's Study Blog" type="application/rss+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Michael's Study Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Backend Engineering & Deep learning!</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>Tags<span class="badge">15</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>Categories<span class="badge">7</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>Archives<span class="badge">21</span></a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kimh060612.github.io/2022/03/05/Transformer-C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Michael Kim">
      <meta itemprop="description" content="Backend Engineering, Deep learning, Algorithm & DS, CS 전공 지식 공부 저장소">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Michael's Study Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/05/Transformer-C/" class="post-title-link" itemprop="url">Transformer 강의 내용 part.C</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-03-05 19:28:31" itemprop="dateCreated datePublished" datetime="2022-03-05T19:28:31+09:00">2022-03-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-05-30 00:00:32" itemprop="dateModified" datetime="2022-05-30T00:00:32+09:00">2022-05-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2022/03/05/Transformer-C/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2022/03/05/Transformer-C/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><em>본 포스트는 해당 링크의 <a target="_blank" rel="noopener" href="https://wikidocs.net/22893">블로그</a>와 논문 “Attention is all you need”를 참조하여 작성되었음을 알립니다.</em></p>
<h3 id="Index"><a href="#Index" class="headerlink" title="Index"></a>Index</h3><ol>
<li><del>Attention!</del><ul>
<li><del>Seq2Seq</del></li>
<li><del>Attention in Seq2Seq</del></li>
</ul>
</li>
<li><del>Transformer Encoder</del><ul>
<li><del>Attention in Transformer</del></li>
<li><del>Multi-Head Attention</del></li>
<li><del>Masking</del></li>
<li><del>Feed Forward</del></li>
</ul>
</li>
<li><del>Transformer Decoder</del></li>
<li><del>Positional Encoding</del></li>
<li>Partice<ul>
<li>Seq2Seq Attention</li>
<li>Transformer</li>
</ul>
</li>
</ol>
<p><br></p>
<h3 id="Partice"><a href="#Partice" class="headerlink" title="Partice"></a>Partice</h3><hr>
<p>이 파트에서는 지난 시간부터 쭉 다뤄온 Attention과 Transformer를 구현해 보는 시간을 가질 것이다. 본격적으로 Model Subclassing API를 제대로 활용하는 시간이 될 것이다.</p>
<h4 id="Seq2Seq-Attention"><a href="#Seq2Seq-Attention" class="headerlink" title="Seq2Seq Attention"></a>Seq2Seq Attention</h4><p>Transformer 구조를 보기 전에 Attention을 Seq2Seq 모델에 적용시켜 보도록 하겠다. 먼저 소스코드부터 보고 가자.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(keras.layers.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Attention, self).__init__()</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, q, k, v, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># q: (batch_size, seq_len, d_model)</span></span><br><span class="line">        <span class="comment"># k: (batch_size, seq_len, d_model)</span></span><br><span class="line">        <span class="comment"># v: (batch_size, seq_len, d_model)</span></span><br><span class="line"></span><br><span class="line">        matmul_qk = tf.matmul(q, k, transpose_b=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># qk^T : (batch_size, seq_len, seq_len)</span></span><br><span class="line"></span><br><span class="line">        dk = tf.cast(tf.shape(k)[-<span class="number">1</span>], tf.float32)</span><br><span class="line">        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)</span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            scaled_attention_logits += (mask * -<span class="number">1e9</span>)</span><br><span class="line">        </span><br><span class="line">        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-<span class="number">1</span>)</span><br><span class="line">        output = tf.matmul(attention_weights, v)</span><br><span class="line">        <span class="comment"># output: (batch_size, seq_len, d_model)</span></span><br><span class="line">        <span class="keyword">return</span> output, attention_weights</span><br></pre></td></tr></table></figure>
<p>우선 우리가 사용할 Attention 구조이다. 지난 Transformer part.A에서 정의한 연산(masking 포함)을 고대로 구현한 것이다.<br>이를 어떻게 Seq2Seq 모델에 적용하는지는 밑의 그림과 소스코드를 보자.</p>
<p align="center"><img src="https://kimh060612.github.io/img/Attention.png" width="100%"></p>

<p>위 그림에 따르면 일단 Seq2Seq는 Encoder와 Decoder가 각기 다른 모델로 있다. 그리고 Encoder에서 나온 모든 time step의 출력과 Decoder의 출력을 같이 사용해서 연산을 해야할 필요가 있다.</p>
<p>필자는 일단 Decoder에 Encoder의 모든 Time step의 출력을 넣는 형식으로 구현을 하였다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(keras.Model):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, WORDS_NUM, emb_dim, hidden_unit, batch_size, *args, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(*args, **kwargs)</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.hidden_unit = hidden_unit</span><br><span class="line">        self.embedding = keras.layers.Embedding(WORDS_NUM, emb_dim)</span><br><span class="line">        self.LSTM = keras.layers.LSTM(hidden_unit, return_state=<span class="literal">True</span>, return_sequences=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, inputs, enc_hidden = <span class="literal">None</span></span>):</span><br><span class="line">        x = self.embedding(inputs)</span><br><span class="line">        y, h, _ = self.LSTM(x, initial_state=enc_hidden)</span><br><span class="line">        <span class="keyword">return</span> y, h</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(keras.Model):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, WORDS_NUM, emb_dim, hidden_unit, *args, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(*args, **kwargs)</span><br><span class="line">        self.embedding = keras.layers.Embedding(WORDS_NUM, emb_dim)</span><br><span class="line">        self.lstm = tf.keras.layers.LSTM(hidden_unit, return_sequences=<span class="literal">True</span>, return_state=<span class="literal">True</span>)</span><br><span class="line">        self.attention = Attention()</span><br><span class="line">        self.dense = tf.keras.layers.Dense(WORDS_NUM, activation=<span class="string">&#x27;softmax&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, x, hidden, mask=<span class="literal">None</span></span>):</span><br><span class="line">        x = self.embedding(x)</span><br><span class="line">        _, h, c = self.lstm(x)</span><br><span class="line">        context_vec, attention_weight = self.attention(h, hidden, hidden, mask=mask)</span><br><span class="line">        </span><br><span class="line">        x_ = tf.concat([context_vec, c], axis=-<span class="number">1</span>)</span><br><span class="line">        out = self.dense(x_)</span><br><span class="line">        <span class="keyword">return</span> out, h, attention_weight</span><br></pre></td></tr></table></figure>
<p>소스코드를 보기 전에 Attention의 $Q$, $K$, $V$의 정의를 먼저 상기하고 가자.</p>
<p>$Q$ : 특정 시점의 디코더 셀에서의 은닉 상태<br>$K$ : 모든 시점의 인코더 셀의 은닉 상태들<br>$V$ : 모든 시점의 인코더 셀의 은닉 상태들  </p>
<p>Encoder에서는 Decoder에서 활용하기 위해서 2개의 출력을 내놓는다. 본인의 출력값과 hidden state의 값이다.</p>
<p>Decoder에서는 Encoder에서의 Hidden state를 입력으로 받아서 자신의 hidden state와 같이 attention! 을 진행한다.</p>
<p>이렇게 만들어진 vector와 마지막 출력 값을 concatenation한 값을 dense에 넣어서 최종적인 출력을 내 놓는다.</p>
<p>이의 training loop를 보면 더 이해가 잘 될 것이다. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@tf.function</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_step</span>(<span class="params">inp, targ, enc_hidden</span>):</span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">        _, enc_hidden = encoder(inp, enc_hidden)</span><br><span class="line">        dec_hidden = enc_hidden</span><br><span class="line">        dec_input = tf.expand_dims([targ_lang.word_index[<span class="string">&#x27;&lt;start&gt;&#x27;</span>]] * BATCH_SIZE, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, targ.shape[<span class="number">1</span>]):</span><br><span class="line">            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden)</span><br><span class="line">            loss += loss_function(targ[:, t], predictions)</span><br><span class="line">            dec_input = tf.expand_dims(targ[:, t], <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        batch_loss = (loss / <span class="built_in">int</span>(targ.shape[<span class="number">1</span>]))</span><br><span class="line">        </span><br><span class="line">        variables = encoder.trainable_variables + decoder.trainable_variables</span><br><span class="line">        gradients = tape.gradient(loss, variables)</span><br><span class="line">        optimizer.apply_gradients(<span class="built_in">zip</span>(gradients, variables))</span><br><span class="line">        <span class="keyword">return</span> batch_loss</span><br></pre></td></tr></table></figure>
<h4 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h4><p>이제 대망의 Transformer이다. 먼저 Transformer를 구현하기 위해서 어떤 Component들이 필요했는지 알아보자.</p>
<ol>
<li>Multi-head Attention</li>
<li>Positional Encoding</li>
<li>Encoder Blocks</li>
<li>Decoder Blocks</li>
</ol>
<p>우리는 이 4가지 Component들을 각각 순차적으로 구현하고 이를 통해서 최종 모델을 구현할 것이다.</p>
<p><em>file: model/layer.py</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(keras.layers.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, num_head</span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttention, self).__init__()        </span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> d_model % num_head  == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid head and d_model!&quot;</span>)</span><br><span class="line">        self.d_k = d_model // num_head</span><br><span class="line">        self.num_head = num_head</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        </span><br><span class="line">        self.Wq = keras.layers.Dense(self.d_model)</span><br><span class="line">        self.Wk = keras.layers.Dense(self.d_model)</span><br><span class="line">        self.Wv = keras.layers.Dense(self.d_model)</span><br><span class="line">        </span><br><span class="line">        self.dense = keras.layers.Dense(self.d_model, activation=<span class="string">&quot;relu&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">split_head</span>(<span class="params">self, batch_size, x</span>):</span><br><span class="line">        x = tf.reshape(x, (batch_size, -<span class="number">1</span>, self.num_head, self.d_k))</span><br><span class="line">        <span class="keyword">return</span> tf.transpose(x, perm=[<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, q, k, v, mask = <span class="literal">None</span></span>):</span><br><span class="line">        batch_size = q.shape[<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line">        q, k, v = self.Wq(q), self.Wk(k), self.Wv(v)</span><br><span class="line">        <span class="comment"># q: (batch_size, seq_len, d_model)</span></span><br><span class="line">        <span class="comment"># k: (batch_size, seq_len, d_model)</span></span><br><span class="line">        <span class="comment"># v: (batch_size, seq_len, d_model)</span></span><br><span class="line">        </span><br><span class="line">        q, k, v = self.split_head(batch_size, q), self.split_head(batch_size, k), self.split_head(batch_size, v)</span><br><span class="line">        <span class="comment"># q: (batch_size, num_head, seq_len, d_k)</span></span><br><span class="line">        <span class="comment"># k: (batch_size, num_head, seq_len, d_k)</span></span><br><span class="line">        <span class="comment"># v: (batch_size, num_head, seq_len, d_v)</span></span><br><span class="line">        </span><br><span class="line">        qkT = tf.matmul(q, k, transpose_b=<span class="literal">True</span>) <span class="comment"># (batch_size, num_head, seq_len, seq_len)</span></span><br><span class="line">        d_k = tf.cast(self.d_k, dtype=tf.float32)</span><br><span class="line">        scaled_qkT = qkT / tf.math.sqrt(d_k)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> mask == <span class="literal">None</span>:</span><br><span class="line">            scaled_qkT += (mask * -<span class="number">1e9</span>)</span><br><span class="line">        </span><br><span class="line">        attention_dist = tf.nn.softmax(scaled_qkT, axis=-<span class="number">1</span>)</span><br><span class="line">        attention = tf.matmul(attention_dist, v) <span class="comment"># (batch_size, num_head, seq_len, d_k)</span></span><br><span class="line">        </span><br><span class="line">        attention = tf.transpose(attention, perm=[<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>]) <span class="comment"># (batch_size, seq_len, num_head, d_k)</span></span><br><span class="line">        concat_attention = tf.reshape(attention, (batch_size, -<span class="number">1</span>, self.d_model)) <span class="comment"># (batch_size, seq_len, d_model)</span></span><br><span class="line">        output = self.dense(concat_attention)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p>각 연산 뒤에 차원을 적어 두었으니 이해하기는 어렵지 않을 것이다. 구체적인 정의가 떠오르지 않을 독자들을 위해서 Multi-Head Attention의 수식을 적어두고 가겠다.</p>
<script type="math/tex; mode=display">
\text{Attention}(Q, K, V) = \text{Softmax}(\frac{Q_iK_i^T}{\sqrt{d_k}})V_i
\tag{definition 3}</script><script type="math/tex; mode=display">
M_{out} = \text{Concat}(O_1, O_2, ..., O_{H_n}) W^O
\tag{equation 1}</script><p>위 수식을 고대~로 구현한 것 밖에 되지 않는다.</p>
<p>그리고 positional Encoding은 다음과 같이 구현했다.</p>
<p><em>file: model/layer.py</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_angles</span>(<span class="params">pos, i, d_model</span>):</span><br><span class="line">    angle_rates = <span class="number">1</span> / np.power(<span class="number">10000</span>, (<span class="number">2</span> * (i//<span class="number">2</span>)) / np.float32(d_model))</span><br><span class="line">    <span class="keyword">return</span> pos * angle_rates</span><br><span class="line">    </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">positional_encoding</span>(<span class="params">position, d_model</span>):</span><br><span class="line">    angle_rads = get_angles(np.arange(position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model)</span><br><span class="line"></span><br><span class="line">    angle_rads[:, <span class="number">0</span>::<span class="number">2</span>] = np.sin(angle_rads[:, <span class="number">0</span>::<span class="number">2</span>])</span><br><span class="line">    angle_rads[:, <span class="number">1</span>::<span class="number">2</span>] = np.cos(angle_rads[:, <span class="number">1</span>::<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">    pos_encoding = angle_rads[np.newaxis, ...]</span><br><span class="line">    <span class="keyword">return</span> tf.cast(pos_encoding, dtype=tf.float32)</span><br></pre></td></tr></table></figure>
<p>이것 또한 정의를 고대로 구현한 것이다. <del>(참 쉽죠?)</del></p>
<script type="math/tex; mode=display">
\begin{aligned}
    PE_{(pos, 2i)} = \sin(pos/10000^{2i/d_{model}}) \\
    PE_{(pos, 2i + 1)} = \cos(pos/10000^{2i/d_{model}})
\end{aligned}</script><p>이제 Encoder와 Decoder Block을 구현할 차례이다. 귀찮으니 한번에 소스를 적어 두도록 하겠다.</p>
<p><em>file: model/layer.py</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(keras.layers.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, num_head, d_ff, drop_out_prob = <span class="number">0.2</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.num_head = num_head</span><br><span class="line">        </span><br><span class="line">        self.MultiHeadAttention = MultiHeadAttention(d_model=d_model, num_head=num_head)</span><br><span class="line">        </span><br><span class="line">        self.dense_1 = keras.layers.Dense(d_ff, activation=<span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        self.dense_2 = keras.layers.Dense(d_model, activation=<span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        self.layernorm1 = keras.layers.LayerNormalization(epsilon=<span class="number">1e-6</span>)</span><br><span class="line">        self.layernorm2 = keras.layers.LayerNormalization(epsilon=<span class="number">1e-6</span>)</span><br><span class="line"></span><br><span class="line">        self.dropout1 = keras.layers.Dropout(drop_out_prob)</span><br><span class="line">        self.dropout2 = keras.layers.Dropout(drop_out_prob)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, x, training=<span class="literal">True</span>, mask=<span class="literal">None</span></span>):</span><br><span class="line">        </span><br><span class="line">        out_atten = self.MultiHeadAttention(x, x, x, mask)</span><br><span class="line">        out_atten = self.dropout1(out_atten, training=training)</span><br><span class="line">        x = self.layernorm1(out_atten + x)</span><br><span class="line">        </span><br><span class="line">        out_dense = self.dense_1(x)</span><br><span class="line">        out_dense = self.dense_2(out_dense)</span><br><span class="line">        out_dense = self.dropout2(out_dense, training=training)</span><br><span class="line">        x = self.layernorm2(out_dense + x)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">        </span><br><span class="line">    </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(keras.layers.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, num_head, d_ff, drop_out_prob = <span class="number">0.2</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.num_head = num_head</span><br><span class="line">        self.MultiHeadAttention1 = MultiHeadAttention(d_model, num_head)</span><br><span class="line">        self.MultiHeadAttention2 = MultiHeadAttention(d_model, num_head)</span><br><span class="line">        </span><br><span class="line">        self.dense_1 = keras.layers.Dense(d_ff, activation=<span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        self.dense_2 = keras.layers.Dense(d_model, activation=<span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=<span class="number">1e-6</span>)</span><br><span class="line">        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=<span class="number">1e-6</span>)</span><br><span class="line">        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=<span class="number">1e-6</span>)</span><br><span class="line">        self.dropout1 = tf.keras.layers.Dropout(drop_out_prob)</span><br><span class="line">        self.dropout2 = tf.keras.layers.Dropout(drop_out_prob)</span><br><span class="line">        self.dropout3 = tf.keras.layers.Dropout(drop_out_prob)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, x, enc_output, training, look_ahead_mask, padding_mask</span>):</span><br><span class="line">        </span><br><span class="line">        out_atten1 = self.MultiHeadAttention1(x, x, x, look_ahead_mask)</span><br><span class="line">        out_atten1 = self.dropout1(out_atten1, training=training)</span><br><span class="line">        x = self.layernorm1(out_atten1 + x)</span><br><span class="line">        </span><br><span class="line">        out_atten2 = self.MultiHeadAttention2(enc_output, enc_output, x, padding_mask)</span><br><span class="line">        out_atten2 = self.dropout2(out_atten2, training=training)</span><br><span class="line">        x = self.layernorm2(out_atten2 + x)</span><br><span class="line">        </span><br><span class="line">        out_dense = self.dense_1(x)</span><br><span class="line">        out_dense = self.dense_2(out_dense)</span><br><span class="line">        out_dense = self.dropout3(out_dense, training=training)</span><br><span class="line">        x = self.layernorm3(out_dense + x)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>Encoder는 별로 볼게 없고, Decoder를 보자면 지난번 정의에서 다뤘듯이, 첫번째 Multi-Head Attention과 두번째 Mutli-Head Attention은 다른 입력이 주어지고, 다른 masking이 주어진다.</p>
<p>따라서 Decoder의 입력으로 출력 sequence, encoder 출력, look ahead masking, padding masking이 들어가야 한다.</p>
<p>자세한 정의는 part.B를 다시 참고하자.</p>
<p>참고로, masking은 다음 함수로 만들었다.</p>
<p><em>file: model/model.py</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_padding_mask</span>(<span class="params">seq</span>):</span><br><span class="line">    seq = tf.cast(tf.math.equal(seq, <span class="number">0</span>), tf.float32)</span><br><span class="line">    <span class="comment"># add extra dimensions to add the padding</span></span><br><span class="line">    <span class="comment"># to the attention logits.</span></span><br><span class="line">    <span class="keyword">return</span> seq[:, tf.newaxis, tf.newaxis, :]  <span class="comment"># (batch_size, 1, 1, seq_len)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_look_ahead_mask</span>(<span class="params">size</span>):</span><br><span class="line">    mask = <span class="number">1</span> - tf.linalg.band_part(tf.ones((size, size)), -<span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> mask  <span class="comment"># (seq_len, seq_len)</span></span><br></pre></td></tr></table></figure>
<p>그리고 최종적으로 이렇게 만든 Encoder, Decoder Block을 쌓아서 Transformer 모델을 만들어야 한다.</p>
<p>편의상, Encoder모델, Decoder 모델을 만들고 이를 하나로 합쳐서 Transformer 모델을 만들었다.</p>
<p><em>file: model/model.py</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderModel</span>(keras.Model):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_voc_size, num_layers, max_seq_len, d_model, num_head, d_ff, drop_out_prob = <span class="number">0.2</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderModel, self).__init__()</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.embedding = tf.keras.layers.Embedding(input_voc_size, d_model)</span><br><span class="line">        self.pos_encoding = positional_encoding(max_seq_len, self.d_model)</span><br><span class="line">        self.enc_layers = [ Encoder(d_model, num_head, d_ff, drop_out_prob) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_layers) ]</span><br><span class="line">        self.dropout = tf.keras.layers.Dropout(drop_out_prob)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, x, training, mask</span>):</span><br><span class="line">        </span><br><span class="line">        seq_len = tf.shape(x)[<span class="number">1</span>]</span><br><span class="line">        x = self.embedding(x)</span><br><span class="line">        x += self.pos_encoding[:, :seq_len, :]</span><br><span class="line">        x = self.dropout(x, training=training)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_layers):</span><br><span class="line">            x = self.enc_layers[i](x, training, mask)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">        </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderModel</span>(keras.Model):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, output_voc_size, num_layers, max_seq_len, d_model, num_head, d_ff, drop_out_prob = <span class="number">0.2</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(DecoderModel, self).__init__()</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.embedding = tf.keras.layers.Embedding(output_voc_size, d_model)</span><br><span class="line">        self.pos_encoding = positional_encoding(max_seq_len, self.d_model)</span><br><span class="line">        </span><br><span class="line">        self.dec_layers = [ Decoder(d_model, num_head, d_ff, drop_out_prob) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_layers) ]</span><br><span class="line">        self.dropout = tf.keras.layers.Dropout(drop_out_prob)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, x, enc_output, training, look_ahead_mask, padding_mask</span>):</span><br><span class="line">        </span><br><span class="line">        seq_len = tf.shape(x)[<span class="number">1</span>]</span><br><span class="line">        x = self.embedding(x)</span><br><span class="line">        x += self.pos_encoding[:, :seq_len, :]</span><br><span class="line">        x = self.dropout(x, training=training)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.num_layers):</span><br><span class="line">            x = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(keras.Model):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_voc_size, output_voc_size, num_layers, max_seq_len_in, max_seq_len_out, d_model, num_head, d_ff, drop_out_prob = <span class="number">0.2</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.Encoder = EncoderModel(input_voc_size, num_layers, max_seq_len_in, d_model, num_head, d_ff, drop_out_prob)</span><br><span class="line">        self.Decoder = DecoderModel(output_voc_size, num_layers, max_seq_len_out, d_model, num_head, d_ff, drop_out_prob)</span><br><span class="line">        </span><br><span class="line">        self.final_layer = tf.keras.layers.Dense(output_voc_size)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, inputs, training</span>):</span><br><span class="line">        inp, tar = inputs</span><br><span class="line">        </span><br><span class="line">        enc_padding_mask, look_ahead_mask, dec_padding_mask = self.create_masks(inp, tar)</span><br><span class="line">        enc_output = self.Encoder(inp, training, enc_padding_mask)</span><br><span class="line">        dec_output = self.Decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)</span><br><span class="line">        </span><br><span class="line">        final_output = self.final_layer(dec_output)</span><br><span class="line">        <span class="keyword">return</span> final_output</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">create_masks</span>(<span class="params">self, inp, tar</span>):</span><br><span class="line">        enc_padding_mask = create_padding_mask(inp)</span><br><span class="line">        dec_padding_mask = create_padding_mask(inp)</span><br><span class="line"></span><br><span class="line">        look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[<span class="number">1</span>])</span><br><span class="line">        dec_target_padding_mask = create_padding_mask(tar)</span><br><span class="line">        look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> enc_padding_mask, look_ahead_mask, dec_padding_mask</span><br></pre></td></tr></table></figure>
<p>그리고 Transformer에서 보면 Embedding layer를 사용하는데, 이는 Dense와 결과적인 측면에서는 다를 것이 없디. 하지만 token을 vector로 Embedding하는데 있어서 훨씬 효율적이니 되도록 token을 vector로 임베딩할때 이걸 사용하도록 하자.</p>
<p>각 Encoder와 Decoder는 num_layer(hyper parameter)만큼 Encoder, Decoder 블록을 반복해서 쌓고, 그것들을 차례로 출력으로 내놓는다. 이제 2개를 합쳐서 간단하게 Transformer 모델이 완성되는 것이다.</p>
<p>이쯤되면 필자들도 느낄 것이다. 이럴때 Model Subclassing API가 빛을 발휘한다. 이 각각의 Component들을 함수로만 구현하는 것 보다는 class로 묶어서 관리하는 것이 가독성, 유지보수 측면에서 훨씬 이득이다.</p>
<p>그리고 논문에 의하면, Transformer는 독자적인 learning rate scheduler를 가진다. 이 수식은 다음과 같다.</p>
<p align="center"><img src="https://kimh060612.github.io/img/lr_schedule.png" width="50%"></p>

<p>이를 구현하기 위해서 다음과 같이 Custom Scheduler를 만들 수 있다.</p>
<p><em>file: model/model.py</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CustomSchedule</span>(keras.optimizers.schedules.LearningRateSchedule):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, warmup_steps=<span class="number">4000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(CustomSchedule, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.d_model_save = d_model</span><br><span class="line">        self.d_model = tf.cast(d_model, tf.float32)</span><br><span class="line"></span><br><span class="line">        self.warmup_steps = warmup_steps</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, step</span>):</span><br><span class="line">        arg1 = tf.math.rsqrt(step)</span><br><span class="line">        arg2 = step * (self.warmup_steps ** -<span class="number">1.5</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_config</span>(<span class="params">self</span>):</span><br><span class="line">        config = &#123;</span><br><span class="line">            <span class="string">&#x27;d_model&#x27;</span>: self.d_model_save,</span><br><span class="line">            <span class="string">&#x27;warmup_steps&#x27;</span>: self.warmup_steps,</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> config</span><br></pre></td></tr></table></figure>
<p>여기까지 구현하면 Transformer의 구현은 끝이 난다. 굳이 여기서는 Training loop까지는 다루지 않겠다. 한번 필자가 직접 완성해 보도록 하자. 정 모르겠다면 필자의 github에 완성판이 있으니 직접 가서 찾아 보기를 바란다.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kimh060612.github.io/2022/03/05/Transformer-B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Michael Kim">
      <meta itemprop="description" content="Backend Engineering, Deep learning, Algorithm & DS, CS 전공 지식 공부 저장소">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Michael's Study Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/05/Transformer-B/" class="post-title-link" itemprop="url">Transformer 강의 내용 part.B</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-03-05 19:28:27" itemprop="dateCreated datePublished" datetime="2022-03-05T19:28:27+09:00">2022-03-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-05-30 00:00:32" itemprop="dateModified" datetime="2022-05-30T00:00:32+09:00">2022-05-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2022/03/05/Transformer-B/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2022/03/05/Transformer-B/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><em>본 포스트는 해당 링크의 <a target="_blank" rel="noopener" href="https://wikidocs.net/22893">블로그</a>와 논문 “Attention is all you need”를 참조하여 작성되었음을 알립니다.</em></p>
<h3 id="Index"><a href="#Index" class="headerlink" title="Index"></a>Index</h3><ol>
<li><del>Attention!</del><ul>
<li><del>Seq2Seq</del></li>
<li><del>Attention in Seq2Seq</del></li>
</ul>
</li>
<li>Transformer Encoder<ul>
<li>Attention in Transformer</li>
<li>Multi-Head Attention</li>
<li>Masking</li>
<li>Feed Forward</li>
</ul>
</li>
<li>Transformer Decoder</li>
<li>Positional Encoding</li>
<li>Partice<ul>
<li>Seq2Seq Attention</li>
<li>Transformer</li>
</ul>
</li>
</ol>
<p>여기서는 1은 Part. A이고 2~4는 Part. B에서, 5는 part. C에서 다루도록 하겠다.<br><br></p>
<h3 id="Transformer-Encoder"><a href="#Transformer-Encoder" class="headerlink" title="Transformer Encoder"></a>Transformer Encoder</h3><hr>
<p>자 드디어 대망의 Transformer이다. 현대 Deep learning model의 기초라고도 할 수 있을 만큼 많이 쓰이는 모델이므로 블로그 주제에 선정되었다. 이를 위해서 하고 싶지도 않았던 Attention 공부를 해왔는데, 공부하고 보니 보람 넘치는 시간이었던 것 같다.</p>
<h4 id="Attention-in-Transformer"><a href="#Attention-in-Transformer" class="headerlink" title="Attention in Transformer"></a>Attention in Transformer</h4><p>우선 Transformer를 제시한 논문의 이름이 “Attention is all you need”라는 것을 파악하고 가자. (필자 생각에는 이때부터 살짝 논문 이름이 제목학원이 된 것 같기도 하다.. ㅋㅋ;;) 한마디로, 기존에 Seq2Seq 같은 경우에는 RNN/LSTM/GRU의 보완제로써 Attention을 사용했다면, Transformer에서는 Attention만을 사용하여 신경망을 구성한다.</p>
<p align="center"><img src="https://kimh060612.github.io/img/Transformer.png" width="50%"></p>

<p>일단, 양산형 블로그글의 클리셰답게, Transformer의 논문에서 Model Architecture의 모식도를 가져와 봤다. 여기에 들어가는 Component를 하나하나 분해해 보면 다음과 같이 정리할 수 있다.</p>
<ol>
<li>Multi-Head Attention</li>
<li>Feed Forward Network</li>
<li>Masked Multi-Head Attention</li>
<li>Positional Encoding</li>
<li>*$N$ ?</li>
<li>Residual Connection</li>
</ol>
<p>우리는 이 모두를 하나하나 분해해서 살펴볼 것이다. (5번인 Residual Connection은 다른 Post에서 집중적으로 다루어 볼 것이다.) 그 전에 먼저 Transformer에서 Attention이 어떻게 정의되고 쓰이는지를 알아볼 것이다. 우선, 우리는 Attention을 정의하려면 먼저 $Q$, $K$, $V$를 정의해야한다는 것을 이전 포스트에서 다루었다.</p>
<p>위 행렬의 크기/정의는 어떤 attention이냐에 따라서 달라진다. 이때 attention의 종류는 이전 포스트에서 언급했던 “누가 만든 attention”이냐가 아니라 어떤 식으로 Attention 연산이 이루어 지냐이다. Transformer에서는 아래의 3가지 타입의 attention이 사용된다.</p>
<p align="center"><img src="https://kimh060612.github.io/img/attention_type.png" width="50%"></p>

<p>출처: 위 링크에 나와있는 블로그</p>
<p>위 3가지 attention에 따라서 $Q$, $K$, $V$의 출처가 달라진다.<br>정리하자면 다음과 같다.</p>
<p>Encoder의 Self Attention : Query = Key = Value<br>Decoder의 Masked Self Attention : Query = Key = Value<br>Decoder의 Encoder-Decoder Attention : Query : 디코더 벡터 / Key = Value : 인코더 벡터  </p>
<p>자, 이제 어떻게 $Q$, $K$, $V$가 정의되는지 파트별로 나눠놨으니 남은 것은 본격적으로 연산에 들어가는 것이다.</p>
<p>그 전에 Transformer에서 사용되는 Hyper Parameter를 먼저 알아보도록 하겠다.</p>
<ol>
<li>$d_{model}$ = 512: Transformer의 인코더와 디코더의 입력과 출력의 크기를 결정하는 parameter이다.</li>
<li>$N$ = 6: Transformer에서 Encoder와 Decoder가 총 몇 개의 층으로 이루어져 있는지를 나타내는 기준이다.</li>
<li>$H_n$ = 8: Multi head Attention에서 Attention을 병렬로 수행하고 합치는 방식을 사용하는데, 그때 몇개로 분할할지를 결정하는 hyper parameter이다.</li>
<li>$d<em>{ff}$ = 2048: Transformer의 내부에 Feed Forward 신경망이 존재하는데, 이 신경망의 은닉 unit의 개수를 의미함. 물론, 이 layer의 출력은 당연히 $d</em>{model}$ 이 된다.</li>
</ol>
<h4 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h4><p>또 양산형 블로그 클리셰답게 논문에 있는 Figure를 가지고 왔다. </p>
<p align="center"><img src="https://kimh060612.github.io/img/MHAttention.png" width="75%"></p>

<p>위 그림을 보면서 설명을 읽으면 이해가 잘 될 수 있도록 글을 구성해 보도록 하겠다.<br>우선 가장 보편적으로 활용되는 자연어 처리를 예로 들고 싶지만, 필자의 성격상 그런 것 보다는 조금 일반적으로 이야기를 하도록 하겠다. 어떤 연속열 데이터 $x<em>1, x_2, x_3 …x_n$ 이 있다고 가정해 보자. 이들 각각이 $x_i \in \mathbb{R}^{d</em>{model}}$ 인 벡터라고 해보자. 자연어 처리라면 Word Embedding으로 생성된 단어 벡터라고 할 수 있고 설령 다른 문제더라도 이것과 비슷하게 representation이 가능하다면 뭐든 학습이 가능하다는 것이다. 이러한 input을 활용해서 $Q$,$K$,$V$를 얻어 보도록 하자.</p>
<p>우리는 3개의 FCNN Layer를 활용해서 입력으로부터 $Q$,$K$,$V$를 얻어낼 것이다.<br>다음과 같이 FCNN Layer의 Weight를 정의해 보도록 하겠다.</p>
<script type="math/tex; mode=display">
\begin{aligned}
    W^Q_i \in \mathbb{R}^{d_{model} * d_k} \\
    W^K_i \in \mathbb{R}^{d_{model} * d_k} \\
    W^V_i \in \mathbb{R}^{d_{model} * d_v} 
\end{aligned}
\tag{definition 1}</script><p>여기서 $d<em>k = d_v = d</em>{model}/H_n$이다. 그리고 sequence length를 $n$이라고 가정하자. 그렇다면 우리는 결과로써 나오는 각 $Q$,$K$,$V$ 행렬의 결과의 크기를 다음과 같다고 생각할 수 있다. 그리고 여기서 $i$는 $1 \leq i \leq H_n$이다. 나중에 각 i별로 병렬로 계산하고 Concatenation을 진행하는 과정을 거칠 것이다. 그래서 Multi Head Attention이다.</p>
<script type="math/tex; mode=display">
\begin{aligned}
    Q_i \in \mathbb{R}^{n * d_k} \\
    K_i \in \mathbb{R}^{n * d_k} \\
    V_i \in \mathbb{R}^{n * d_v} 
\end{aligned}
\tag{definition 2}</script><p>이 상태에서 지난 포스트에서 주로 다뤘던 Scaled Dot Attention에 대해서 다시 한번 상기하고, 적용해 보자.</p>
<script type="math/tex; mode=display">
\text{Attention}(Q, K, V) = \text{Softmax}(\frac{Q_iK_i^T}{\sqrt{d_k}})V_i
\tag{definition 3}</script><p>이 Attention의 결과 행렬을 $O_i$라고 했을때, $O_i \in \mathbb{R}^{n * d_v}$이다.<br>이제 각기 계산한 $O_i$들에 대해서 concatenation을 진행한다. 그 후에 다시 한번 FCNN Layer에 통과시키게 된다. </p>
<script type="math/tex; mode=display">
M_{out} = \text{Concat}(O_1, O_2, ..., O_{H_n}) W^O
\tag{equation 1}</script><p>그렇다면 결과적으로는 다음이 성립한다.</p>
<script type="math/tex; mode=display">
\begin{aligned}
    W^O \in \mathbb{R}^{H_nd_v * d_{model}} \\ 
    M_{out} \in \mathbb{R}^{n * d_{model}}
\end{aligned}
\tag{definition 4}</script><p>여기까지가 Encoder에 적용시킬 Multi Head Self Attention 이다.</p>
<h4 id="Masking"><a href="#Masking" class="headerlink" title="Masking"></a>Masking</h4><p>Masking은 Sequential한 데이터에서 쓸데없는 데이터까지 학습되지 않도록, 또는 원하는 방향으로 데이터를 학습시킬 수 있게 해주는 좋은 도구이다. Transformer에서는 $Q_iK_i^T$룰 계산하면서 나오는 $\mathbb{R}^{n*n}$ 행렬에 적용시키게 된다. 몇가지 예시를 들면서 설명해 보도록 하겠다.</p>
<ol>
<li>padding의 학습을 막기 위한 용도 (Padding Mask)</li>
</ol>
<p>sequence의 길이를 통일시키기 위해, 길이가 모자란 데이터에는 으례 padding을 넣게 된다. 하지만 padding은 실질적인 의미를 가진 데이터가 아니기에 학습에서 배제를 해야한다. 그래서 우리는 $Q_iK_i^T$의 행렬에서 Softmax 함수를 거치기 전에 $-\inf$ 값을 곱하므로써 Attention Distribution에서 Padding에 해댱하는 위치의 값을 0으로 만들어 버린다. 학습에서 배제를 시키는 것이다.</p>
<p>해설이 어렵다면 다음 그림을 보자.</p>
<p align="center"><img src="https://kimh060612.github.io/img/softmax.png" width="50%"></p>

<ol>
<li>자신보다 앞의 데이터를 학습하지 않게 하는 용도</li>
</ol>
<p>이 Masking은 Decoder에서 사용된다. $Q_iK_i^T$ 행렬에서 자기 자신보다 앞서 있는 데이터를 참조하는 현상이 일어난다. 이는 Transformer가 데이터를 순차적으로 받는 것이 아닌, 한번에 받기 때문에 그런 것이다. 이 문제를 해결하기 위해서 $Q_iK_i^T$ 행렬에 자신보다 앞선 값에는 $-\inf$를 곱해서 Attention Distribution의 값을 0으로 만들어 버린다.</p>
<p>이것도 해석이 어렵다면 다음 그림을 보자.</p>
<p align="center"><img src="https://kimh060612.github.io/img/lookahead.png" width="50%"></p>

<p>이를 look ahead masking 이라고 한다.</p>
<h4 id="Feed-Forward"><a href="#Feed-Forward" class="headerlink" title="Feed Forward"></a>Feed Forward</h4><p>Transformer에서는 다음과 같은 형태의 FCNN Layer 2개를 겹쳐서 Encoder/Decoder에 사용하고 있다.</p>
<script type="math/tex; mode=display">
\begin{aligned}
    W_1 \in \mathbb{R}^{d_{model} * d_{ff}} \\
    W_2 \in \mathbb{R}^{d_{ff} * d_{model}}
\end{aligned}</script><p>위 2개의 $W$행렬은 각 FCNN layer의 가중치 행렬의 크기이다. 이 2개의 FCNN Layer에 사이에 ReLU activation function을 끼워 넣어서 사용한다.</p>
<h3 id="Transformer-Decoder"><a href="#Transformer-Decoder" class="headerlink" title="Transformer Decoder"></a>Transformer Decoder</h3><hr>
<p>자, 이제 Decoder에 대한 이야기를 해볼 것이다. 하지만 여기까지 읽었다면 Decoder를 굳이 다뤄야 할지도 의문일 정도로 유추하기 쉬워졌다. </p>
<p align="center"><img src="https://kimh060612.github.io/img/Decoder.png" width="50%"></p>

<p>Decoder와 Encoder가 다른 점은 위 그림에서 있는 것이 전부이다.</p>
<ol>
<li>첫번째 Multi Head Attention에서 Look Ahead Masking을 사용했다는점</li>
<li>2번째 Multi Head Attention에서 Encoder의 Output을 Key와 Value 행렬로써 사용 한다는점 </li>
</ol>
<p>Residual Connection 특성상, 같은 차원으로 입/출력 행렬이 일괄되기 때문에 위화감 없이 전개가 가능할 것이다. 이 부분은 독자에게 어떤 식으로 연산이 이루어지는지 생각할 시간을 가져보는 것을 추천한다.</p>
<h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><hr>
<p>마자믹으로 Positional Encoding에 대해서 알아볼 것이다. Positional Encoding은 입력의 데이터의 위치 정보를 신경망에 알려주는 한가지의 수단이다. 이것이 필요한 이유는 다양하다. 대표적으로 NLP에서는 어순 또한 언어의 뜻을 이해하는데 중요한 정보이기 때문이다. </p>
<p>대표적인 몇가지 예시를 들자면 다음과 같다.</p>
<ol>
<li>Simple Indexing</li>
</ol>
<p>이는 단순하게 입력이 들어온 순서대로 0, 1, 2, 3, … 의 값을 할당하여 Embedding Layer 같은 것을 거쳐서 벡터로 만드는 방법이다. 가장 단순하지만 딱히 추천하지는 않는다고 한다.</p>
<ol>
<li>Sin 함수를 이용한 Positional Encoding</li>
</ol>
<p>이는 position에 따라서 다음과 같은 수식으로 계산되는 값을 사용하는 방법이다.</p>
<script type="math/tex; mode=display">
\begin{aligned}
    PE_{(pos, 2i)} = \sin(pos/10000^{2i/d_{model}}) \\
    PE_{(pos, 2i + 1)} = \cos(pos/10000^{2i/d_{model}})
\end{aligned}</script><p>이때, $pos$는 Sequence 내에서 입력의 위치를 나타내며, $i$는 Embedding vector 내에서의 위치를 나타낸다.</p>
<p>자세한 것은 다음 그림을 보면 더 자세히 알 수 있다.</p>
<p align="center"><img src="https://kimh060612.github.io/img/transformer6_final.png" width="50%"></p>

<p>이걸로 기나긴 Transformer의 이야기는 끝났다. 다음 포스팅은 이것을 구현해 보는 시간을 가질 것이다.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kimh060612.github.io/2022/03/05/Transformer-A/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Michael Kim">
      <meta itemprop="description" content="Backend Engineering, Deep learning, Algorithm & DS, CS 전공 지식 공부 저장소">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Michael's Study Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/05/Transformer-A/" class="post-title-link" itemprop="url">Transformer 강의 내용 part.A</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-03-05 19:28:17" itemprop="dateCreated datePublished" datetime="2022-03-05T19:28:17+09:00">2022-03-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-05-30 00:00:32" itemprop="dateModified" datetime="2022-05-30T00:00:32+09:00">2022-05-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2022/03/05/Transformer-A/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2022/03/05/Transformer-A/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><em>본 포스트는 해당 링크의 <a target="_blank" rel="noopener" href="https://wikidocs.net/22893">블로그</a>을 참조하여 작성되었음을 알립니다.</em></p>
<h3 id="Index"><a href="#Index" class="headerlink" title="Index"></a>Index</h3><ol>
<li>Attention!<ul>
<li>Seq2Seq</li>
<li>Attention in Seq2Seq</li>
</ul>
</li>
<li>Transformer Encoder<ul>
<li>Attention in Transformer</li>
<li>Multi-Head Attention</li>
<li>Masking</li>
<li>Feed Forward</li>
</ul>
</li>
<li>Transformer Decoder</li>
<li>Positional Encoding</li>
<li>Partice<ul>
<li>Seq2Seq Attention</li>
<li>Transformer</li>
</ul>
</li>
</ol>
<p>여기서는 1은 Part. A이고 2~4는 Part. B에서, 5는 part. C에서 다루도록 하겠다.<br><br></p>
<h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention!"></a>Attention!</h3><hr>
<h4 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h4><p>자~ 주목! ㅋㅋㅋ;;<br>우선 몇몇 사람들은 의아해 할 것이다. 순서상으로는 다음과 같이 가는 것이 국룰이기 때문이다.</p>
<blockquote>
<p>RNN -&gt; LSTM -&gt; (GRU) -&gt; Seq2Seq -&gt; Attention</p>
</blockquote>
<p>흠… 일단 필자가 생각하기에는 LSTM과 GRU는 RNN에서 구조를 바꾼 것이다. 그리고 이들끼리는 언제나 서로 바뀔 수 있는, 마치 기계 공장의 나사와 같은 존재이기에, 기본이 되는 RNN만을 다루고 넘어간 것이다. 물론! Gradient 입장에서 말한다면 할 말은 많다. 이들이 나온 이론적인 토대는 명확하지만, 굳이? 이걸? 필자의 블로그에서? 다루기에는 너무 흔해 빠진 내용이라 바로 Attention 부터 죠지고 가도록 하겠다. (물론 Attention 또한 요즘은 흔해 빠진 내용 맞다 ㅋ;;)</p>
<p>그래서 왜 Attention에 part 하나를 다 써먹나? 얼마나 중요한 내용이길래?</p>
<p>중요한 내용인 것도 맞지만, 필자가 더 자세히 공부하려고 이런 것이다. 그리고 필자는 이 이후 포스팅에서는 일반화된 델타 규칙을 내세우지 않을 것이다. 왜냐하면 이제부터는 진짜 의미가 없다고 생각하기 때문이다. 이걸 굳이 일반화된 델타 규칙으로 풀어서 구현할 바에는 그냥 AutoGrad를 직접 구현하는게 빠르다.</p>
<p>우선 그렇다면 Attention이 왜 나왔나부터 생각해 보도록 하자.<br>그러기 위해서는 우선 Seq2Seq부터 알아야 하는데, 간단하게만 알아보도록 하자. 다음 그림으로 Seq2Seq의 구조를 파악할 수 있다.</p>
<p align="center"><img src="https://kimh060612.github.io/img/Seq2Seq.png" width="100%"></p>

<p>이 Seq2Seq는 그림과 같이 동작한다.<br>Encoder에서는 입력 벡터의 분석을, Decoder에서는 출력을 결정하는 역할을 한다. 여기서 Encoder는 입력의 Context를 분석하는 역할을 한다고 한다. </p>
<p>Seq2Seq의 단점 </p>
<ol>
<li>입력을 Encoder에서 고정된 크기로 압축하여 context vector로 만든다. 그런 구조는 정보 손실을 가져오기 마련이다.</li>
<li>이러한 Context Vector는 Encoder의 마지막 출력인데 이것만을 사용하게 된다면 초반의 정보는 유실되기 마련이다.  </li>
<li>RNN의 고질적인 문제인 Gradient Vanishing 문제가 발생한다.</li>
</ol>
<p>이를 보완하기 위해서 나온 것이 Attention 구조이다.</p>
<h4 id="Attention-in-Seq2Seq"><a href="#Attention-in-Seq2Seq" class="headerlink" title="Attention in Seq2Seq"></a>Attention in Seq2Seq</h4><p>먼저, Attention의 큰 구조 부터 알고 넘어가자. Attention은 다음과 같은 함수로 표현될 수 있다.</p>
<p>Attention($Q$, $K$, $V$) = Attention Value</p>
<p>그렇다면 우리는 Attention 함수 자체에 대해서 알기 전에 먼저 $Q$, $K$, $V$를 먼저 정의할 필요가 있다.<br>Seq2Seq 모델에서 $Q$, $K$, $V$는 다음과 같이 정의될 수 있다.</p>
<p>$Q$ : 특정 시점의 디코더 셀에서의 은닉 상태<br>$K$ : 모든 시점의 인코더 셀의 은닉 상태들<br>$V$ : 모든 시점의 인코더 셀의 은닉 상태들  </p>
<p>Seq2Seq 모델에서 순전파를 한다면 위의 행렬들을 전부 구할 수 있을 것이다. 그렇다면 우리는 Attention 연산을 정의할 수 있어야 한다. Attention 연산은 되게 다양한 종류가 있다. 간단하게 정리해보자면 다음과 같다.</p>
<ol>
<li>Dot Attention</li>
<li>Scaled Dot Attention</li>
<li>Bahdanau Attention</li>
</ol>
<p>우리는 이 중에서 Scaled Dot Attention을 다뤄볼 것이다. 왜냐? Transformer에 쓰이니깐.</p>
<p>Scaled Dot Attention의 수식을 써보자면 다음과 같이 간단하게 쓸 수 있다.</p>
<script type="math/tex; mode=display">
\text{Attention}(Q, K, V) = \text{Softmax}(\frac{QK^T}{\sqrt{n}})V
\tag{definition 1}</script><p>여기서 $n$은 RNN(LSTM이든 뭐든)의 출력 벡터의 크기이다. 즉, $Q$의 크기와 같다.<br>결국 Attention 구조는 다음과 같은 형식을 띄고 Seq2Seq와 결합한다.</p>
<p align="center"><img src="https://kimh060612.github.io/img/Attention.png" width="100%"></p>

<p>출처: Luong’s <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1508.04025v5">paper</a></p>
<p>위 그림에서 볼 수 있듯이, 각 $Q$ 벡터들은 RNN(또는 LSTM 또는 GRU)의 Decoder 부분의 은닉 상태이다. 각 시간 스텝에서의 출력을 바탕으로 위의 Attention 함수를 Encoder의 출력 값과 같이 계산할 수 있고, 그를 이용해서 Attention Vector를 만들고 다시 $Q$와 결합한 것을 FCNN에 입력하여 최종 출력 vector를 얻어낸 뒤에 Softmax를 씌워서 예측 단어를 분류한다.</p>
<p>말로 길게 설명하였는데, 이를 수식으로 표현해 보자.</p>
<script type="math/tex; mode=display">
Q^t \in \mathbb{R}^n \\ 
K \in \mathbb{R}^{n * T} \\
V \in \mathbb{R}^{T * n}
\tag{definition 2}</script><p>우선 각 행렬들의 크기는 위와 같이 표현이 가능하다. $Q^t$는 Decoder에서 time step $t$에서의 출력이다. 여기까지 이해가 되었으면 다시 Scaled Dot Attention의 정의에서 성분별로 하나씩 뜯어서 살펴보자.</p>
<ol>
<li>$Q^tK^{\textbf{T}}$: $K$의 경우, input의 각 time step의 출력을 한데 모아놓은 Matrix이다. 이를 $Q^t$ 벡터와 곱한다. 이를 앞으로 편의상 $e^t$라고 정의하도록 하겠다.</li>
<li>$\text{Softmax}$: 위의 $e^t$에 $\sqrt{n}$로 나눈 값을 Softmax에 집어 넣으므로써 Attention Distribution을 뽑아낸다. 이를 편의상 $\alpha^t$라고 부르도록 하겠다.</li>
<li>$V$: 위의 $\alpha^t$는 $\mathbb{R}^{T}$인 벡터이다. 이 벡터의 각 성분으로 Input Encoder의 각 출력을 가중합한다. 이 결과를 Context vector라고도 부른다.</li>
</ol>
<p>이렇게 Context Vector를 얻은 후에 우리는 $Q^t$와 Context vector를 Concatenation한 뒤에 FCNN Layer에 입력으로 넣는다.</p>
<script type="math/tex; mode=display">
W_c \in \mathbb{R}^{n * 2n}: \text{FCNN의 가중치 벡터}
\tag{definition 3}</script><p>Context vector와 $Q^t$를 concatenation한 vector의 크기가 $2n$이니, FCNN의 가중치 벡터가 위와 같이 정의되어야 한다.<br>그리고 그 결과 출력을 Softmax 함수에 넣으면 timestep $t$의 출력이 된다.</p>
<p>이렇게 Attention을 알아 보았는데 다음 파트에서는 Attention이 어떻게 Transformer에 사용되는지를 알아보도록 하겠다.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kimh060612.github.io/2022/03/05/Optimization-B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Michael Kim">
      <meta itemprop="description" content="Backend Engineering, Deep learning, Algorithm & DS, CS 전공 지식 공부 저장소">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Michael's Study Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/05/Optimization-B/" class="post-title-link" itemprop="url">Optimization 강의 내용 part.B</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-03-05 19:27:39" itemprop="dateCreated datePublished" datetime="2022-03-05T19:27:39+09:00">2022-03-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-05-30 00:00:32" itemprop="dateModified" datetime="2022-05-30T00:00:32+09:00">2022-05-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2022/03/05/Optimization-B/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2022/03/05/Optimization-B/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><em>본 포스트는 Hands-on Machine learning 2nd Edition, CS231n, Tensorflow 공식 document, Pattern Recognition and Machine Learning, Deep learning(Ian Goodfellow 저)을 참조하여 작성되었음을 알립니다.</em></p>
<h3 id="Index"><a href="#Index" class="headerlink" title="Index"></a>Index</h3><ol>
<li><del>Essential Mathematics</del>  <ul>
<li><del>Basic of Bayesian Statistics</del></li>
<li><del>Information Theory</del></li>
<li><del>Gradient</del></li>
</ul>
</li>
<li><del>Loss Function Examples</del></li>
<li><del>What is Optimizer?</del></li>
<li><del>Optimizer examples</del></li>
<li>Partice</li>
</ol>
<p><br></p>
<h3 id="Partice"><a href="#Partice" class="headerlink" title="Partice"></a>Partice</h3><hr>
<p>자, 이제 구현의 난이도 측면에서는 가장 어렵다고 말할 수 있는 파트가 왔다. 이번에도 Model Subclassing API를 활용하여 Custom Optimizer를 만들어 보도록 하겠다. Custom이라고 해서 뭔가 새로운건 아니고, 간단하게 SGD를 구현해 볼 것이다. 근데 솔직히 이건 별로 쓸데가 없다. 이것까지 건드려야하는 사람은 아마도 직접 짜는게 빠르지 않을까 싶다.<br>여기서 가장 중요한 것은 Custom Training loop와 Custom loss이다. SGD는 간단하게 소스코드만 보고 Training loop와 Custom loss를 자세히 설명하도록 하겠다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CustomSGDOptimizer</span>(keras.optimizers.Optimizer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, learning_rate = <span class="number">0.001</span>, name = <span class="string">&quot;CustomSGDOptimizer&quot;</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(name, **kwargs)</span><br><span class="line">        self._set_hyper(<span class="string">&quot;learning_rate&quot;</span>, kwargs.get(<span class="string">&quot;lr&quot;</span>, learning_rate))</span><br><span class="line">        self._is_first = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_create_slots</span>(<span class="params">self, var_list</span>):</span><br><span class="line">        <span class="keyword">for</span> var <span class="keyword">in</span> var_list:</span><br><span class="line">            self.add_slot(var, <span class="string">&quot;pv&quot;</span>) <span class="comment"># previous variable</span></span><br><span class="line">        <span class="keyword">for</span> vat <span class="keyword">in</span> var_list:</span><br><span class="line">            self.add_slot(var, <span class="string">&quot;pg&quot;</span>) <span class="comment"># previous gradient</span></span><br><span class="line">    </span><br><span class="line"><span class="meta">    @tf.function</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_resource_apply_dense</span>(<span class="params">self, grad, var</span>):</span><br><span class="line">        var_dtype = var.dtype.base_dtype</span><br><span class="line">        lr_t = self._decayed_lr(var_dtype)</span><br><span class="line"></span><br><span class="line">        new_var_m = var - lr_t * grad</span><br><span class="line"></span><br><span class="line">        pv_var = self.get_slot(var, <span class="string">&quot;pv&quot;</span>)</span><br><span class="line">        pg_var = self.get_slot(var, <span class="string">&quot;pg&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self._is_first :</span><br><span class="line">            self._is_first = <span class="literal">False</span></span><br><span class="line">            new_var = new_var_m</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            cond = grad * pg_var &gt;= <span class="number">0</span></span><br><span class="line">            avg_weight = (pv_var + var) / <span class="number">2.0</span></span><br><span class="line">            new_var = tf.where(cond, new_var_m, avg_weight)</span><br><span class="line">        </span><br><span class="line">        pv_var.assign(var)</span><br><span class="line">        pg_var.assign(grad)</span><br><span class="line"></span><br><span class="line">        var.assign(new_var)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_resource_apply_sparse</span>(<span class="params">self, grad, var</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_config</span>(<span class="params">self</span>):</span><br><span class="line">        base_config = <span class="built_in">super</span>().get_config()</span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            **base_config,</span><br><span class="line">            <span class="string">&quot;learning_rate&quot;</span> : self._serialize_hyperparameter(<span class="string">&quot;lr&quot;</span>)</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>그만 알아보자.</p>
</blockquote>
<p>장난 안치고 이건 나중에 하나의 포스트를 다 써서 설명할 것이다. 그 정도로 다른 중요한 topic과 같이 다루기에는 무겁다.</p>
<p>일단, Custom Training loop를 어떻게 만드는지를 알아보도록 하겠다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">train_dataset = tf.data.Dataset.from_tensor_slices((train_img, train_labels))</span><br><span class="line">                            <span class="comment"># 섞어.                        # 배치 사이즈 만큼 나눠.</span></span><br><span class="line">train_dataset = train_dataset.shuffle(buffer_size=<span class="number">1024</span>).batch(BatchSize)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Optimizer &amp; Loss Function 정의</span></span><br><span class="line">optimizer = keras.optimizers.Adam(learning_rate=LR)</span><br><span class="line">loss_function = keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">train_accuracy = keras.metrics.SparseCategoricalAccuracy()</span><br><span class="line">val_acc_metric = keras.metrics.SparseCategoricalAccuracy()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCHS):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Epoch %d start&quot;</span>%epoch)</span><br><span class="line">    <span class="comment"># step, 1개의 batch ==&gt; 의사 코드에서 batch 뽑는 역할</span></span><br><span class="line">    <span class="keyword">for</span> step, (x_batch, y_batch) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_dataset):</span><br><span class="line">        <span class="comment"># **********************************************************************************************</span></span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">            logits = model(x_batch, training=<span class="literal">True</span>)</span><br><span class="line">            loss_val = loss_function(y_batch, logits)</span><br><span class="line">            <span class="comment"># 여기서 신경망의 Feed Forward &amp; Expectation of Loss 계산을 진행함.</span></span><br><span class="line">        <span class="comment"># tape.gradient를 호출하면 ==&gt; gradient 계산이 진행됨.</span></span><br><span class="line">        grad = tape.gradient(loss_val, model.trainable_weights)</span><br><span class="line">        <span class="comment"># 정의된 Optimizer를 이용해서 Update를 진행함.</span></span><br><span class="line">        optimizer.apply_gradients(<span class="built_in">zip</span>(grad, model.trainable_weights))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># train에서의 정확도를 계산함.</span></span><br><span class="line">        train_accuracy.update_state(y_batch, logits)</span><br><span class="line">        <span class="keyword">if</span> step % <span class="number">500</span> == <span class="number">0</span> :</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Training loss at step %d: %.4f&quot;</span>%(step, loss_val))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 정확도 뽑아 보겠다.</span></span><br><span class="line">    train_acc = train_accuracy.result()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Training acc over epoch: %.4f&quot;</span> % (<span class="built_in">float</span>(train_acc),))</span><br><span class="line">    train_accuracy.reset_states()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> x_batch_val, y_batch_val <span class="keyword">in</span> validation_dataset:</span><br><span class="line">        val_logits = model(x_batch_val, training = <span class="literal">False</span>)</span><br><span class="line">        val_acc_metric.update_state(y_batch_val, val_logits)</span><br><span class="line">    val_acc = val_acc_metric.result()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Validation acc: %.4f&quot;</span> % (<span class="built_in">float</span>(val_acc),))</span><br><span class="line">    val_acc_metric.reset_states()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>기본적으로 Custom Training Loop는 위와 같이 구현된다. 이걸 하나하나 뜯어서 설명해 보도록 하겠다.</p>
<ol>
<li><p>tf.GradientTape()</p>
<p> tensorflow 2의 핵심인 AutoGrad를 구동시켜주는 친구이다. 이 tape scope 안에서 실행된 tensorflow operation들은 back propagation을 위한 AutoGrad 미분 그래프의 구축이 시작된다.  </p>
</li>
<li><p>tape.gradient</p>
<p> 이 함수를 실행하면 구축된 AutoGrad 미분 그래프를 따라서 미분이 시작된다. </p>
</li>
<li><p>apply_gradients</p>
<p> 이는 optimizer(ex. Adam)의 method이고 tape.gradient에서 구한 gradient를 사용자가 정한 optimizing algorithm을 통해서 Weight를 업데이트 해준다.</p>
</li>
<li><p>update_state</p>
<p>이는 학습 과정중에서 측정할 수 있는 Metric들을 구하는데 사용된다. (ex. 정확도) keras에서 제공하거나 직접 만든 Metric 객체를 새로 Nerual Network에서 계산된 batch에 적용하고 싶을때 사용한다.</p>
</li>
</ol>
<p>이렇게 Custom Training loop는 크게 4가지 요소로 구성된다.<br>필자는 이것을 보통 템플릿으로 가지고 개발할때마다 조금씩 바꿔서 사용하는 편이다. 독자들도 조금 복잡한 트릭이 필요한 Neural Network를 구현할때 본인만의 Training loop를 구성하고 조금씩 바꿔가면서 사용하는 것을 추천한다. </p>
<p>다음으로 다룰 것은 Custom loss이다. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Custom Loss</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomMSE</span>(keras.losses.Loss):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, y_true, y_pred</span>):</span><br><span class="line">        <span class="comment"># tf.math.square : 성분별 제곱</span></span><br><span class="line">        <span class="comment"># [1,2,3] ==&gt; [1,4,9]</span></span><br><span class="line">        L = tf.math.square(y_true - y_pred)</span><br><span class="line">        <span class="comment"># tf.math.reduce_sum: 벡터 합.</span></span><br><span class="line">        <span class="comment"># [1,2,3] ==&gt; 6</span></span><br><span class="line">        L = tf.math.reduce_mean(L)</span><br><span class="line">        <span class="keyword">return</span> L</span><br><span class="line"></span><br><span class="line"><span class="comment"># Custom Regularizer(규제 ==&gt; MAP)</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomRegularizer</span>(keras.regularizers.Regularizer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, _factor</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.factor = _factor</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, weights</span>):</span><br><span class="line">        <span class="keyword">return</span> tf.math.reduce_sum(tf.math.<span class="built_in">abs</span>(self.factor * weights))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_config</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&quot;factor&quot;</span> : self.factor&#125; <span class="comment"># 모델을 저장할때 custom layer, loss, 등등을 저장할때 같이 저장해 주는 역할.</span></span><br></pre></td></tr></table></figure>
<p>tensorflow는 사용자가 Loss나 Regularizer까지도 자유롭게 구성할 수 있도록 허락해준다. 사용 방법은 기존 loss들과 똑같다. 그저 자신이 원하는 연산들을 생각해서 구현하면 된다.</p>
<p>이걸로 Optimization part.B를 마치도록 하겠다. 이 정도까지 Custom해서 사용할 사람들은 많이 없겠지만 혹시라도 필요한 사람들이 있을까 싶어서 적어 보았다.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kimh060612.github.io/2022/03/05/Optimization-A/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Michael Kim">
      <meta itemprop="description" content="Backend Engineering, Deep learning, Algorithm & DS, CS 전공 지식 공부 저장소">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Michael's Study Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/05/Optimization-A/" class="post-title-link" itemprop="url">Optimization 강의 내용 part.A</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-03-05 19:27:37" itemprop="dateCreated datePublished" datetime="2022-03-05T19:27:37+09:00">2022-03-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-05-30 00:00:32" itemprop="dateModified" datetime="2022-05-30T00:00:32+09:00">2022-05-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2022/03/05/Optimization-A/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2022/03/05/Optimization-A/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><em>본 포스트는 Hands-on Machine learning 2nd Edition, CS231n, Tensorflow 공식 document, Pattern Recognition and Machine Learning, Deep learning(Ian Goodfellow 저)을 참조하여 작성되었음을 알립니다.</em></p>
<h3 id="Index"><a href="#Index" class="headerlink" title="Index"></a>Index</h3><ol>
<li>Essential Mathematics  <ul>
<li>Basic of Bayesian Statistics</li>
<li>Information Theory</li>
<li>Gradient</li>
</ul>
</li>
<li>Loss Function Examples</li>
<li>What is Optimizer?</li>
<li>Optimizer examples</li>
<li>Partice</li>
</ol>
<p>여기서는 1~4는 Part. A이고 5는 Part. B에서 다루도록 하겠다.<br><br></p>
<h3 id="Essential-Mathematics"><a href="#Essential-Mathematics" class="headerlink" title="Essential Mathematics"></a>Essential Mathematics</h3><hr>
<h4 id="Basic-of-Bayesian-Statistics"><a href="#Basic-of-Bayesian-Statistics" class="headerlink" title="Basic of Bayesian Statistics"></a>Basic of Bayesian Statistics</h4><p>이 파트에서는 Bayesian Statistics에 대해서 매~우 간단하게 다루어 보도록 하겠다. 실은 다룬다고 말하는 것도 부끄러울 정도록만 할 예정이니 너무 기대는 하지 않아 주었으면 한다.</p>
<p>여기서는 조금 익숙하지 않은 통계학, Bayesian 통계학을 소개하도록 할 것이다. 이 분야는 패턴 인식에서 아주 많이 활용되며, 기존의 확률론에 익숙해져 있다면 이를 받아들이기 매우 힘들다.</p>
<p>우선, “확률”이란 무엇인지에 대해 논하고 넘어가자.<br>Frequentist – “빈도”에 대한 척도, 어떤 일이 앞으로 얼마나 일어날 수 있겠는가?<br>Bayesian – 불확실성에 대한 척도, 이 가설에 대해 얼마나 확신할 수 있는가?</p>
<p>Bayesian 관점에서의 확률을 그림으로 표현하자면 대충 아래와 같다</p>
<p align="center"><img src="https://kimh060612.github.io/img/BP.png" width="100%"></p>

<p>간단하게만 말하자면, 확률이 1에 가까울수록 “명제의 참에 대한 확신”을 가질 수 있고 0에 가까울 수록 그 반대인 것이다. 그리고 0.5에 가까워 질수록 점점 애매해 지는 것이다.</p>
<p>그리고 이 관점에서 우리는 다음 3가지 개념을 다뤄 볼 것이다.</p>
<ol>
<li>Prior Probability</li>
<li>Likelihood</li>
<li>Posterior Probability</li>
</ol>
<p>이 3가지를 말로 풀어서 설명하면 다음과 같다.</p>
<ol>
<li>Prior Probability: 데이터가 주어지기 전에 우리의 가설은 얼마나 타당한가?</li>
<li>Likelihood: 어떤 데이터들이 특정 확률 분포에서 추출되었을 확률</li>
<li>Posterior Probability: 주어진 데이터에 한에서 우리의 가설이 얼마나 타당한가?</li>
</ol>
<p>우리는 이들애 대해서 엄밀한 정의를 다루는 것이 아닌, 좀 더 실용적인 측면에서 예제와 함께 다루어 볼 것이다.</p>
<p>이를 활용하는 예제로써 한번 Curve fitting 문제를 생각해 보자. Curve fitting을 하기 위해서 우리는 어떤 parameter $w$를 가지고 이를 우리가 원하는 곡선에 맞게 fitting하는 과정을 거칠 것이다.</p>
<p>우리는 이때 이런 가정을 할 수 있다.<br>“parameter $𝑤$로 생성된 곡선 $𝐶$는 주어진 데이터 $𝐷$를 잘 설명할 수 있다.”<br>이 문장은 특정한 불확실성을 가지고 있다. 당연히 처음부터 Fitting이 잘 될 리도 없고, 특정한 에러를 가지고 있음이 분명하다.</p>
<p>이렇다면 우리는 그 불확실성을 어떻게 확률로 표현 가능하겠는가? 한번 다음과 같이 해보자.</p>
<p>$Pr(w)$: 데이터가 주어지기 이전에 위의 가설이 얼마나 확실한가<br>$Pr(D|w)$: 주어진 모델에서 데이터가 얼마나 잘 설명되는 가의 척도<br>$Pr(w|D)$: 데이터가 주어졌을 때, 이 모델이 데이터를 잘 설명하는 가의 척도</p>
<p>이때, 좋은 모델을 만드는 방법은 크게 2가지가 있다.</p>
<ol>
<li>Likelihood를 최대화하는 parameter 만들기 ==&gt; MLE(Maximum Likelihood Estimation)</li>
<li>Posterior Probability를 최대화하는 parameter 만들기 ==&gt;  MAP (Maximum A Posterior)</li>
</ol>
<p>즉, 1번 방법은 최대한 좋은 주어진 모델에서 최대한 좋은 데이터의 설명을 얻어내는 방법이고, 2번 방법은 데이터를 가지고 최대한 좋은 모델을 얻어내는 방법이다.</p>
<p>이 관점에서 1번부터 차근차근 설명해 보도록 하겠다. 어떻게 하면 likelihood를 최대화할 수 있을까?</p>
<p>우선 $Pr(D|w)$의 함수를 찾아서 이를 최대로 만들어 주면 되는 것이다. 최대로 만들어 주는 방법은 여러가지 방법이 있겠지만, 우선 그것보다 $Pr(D|w)$를 찾는 것부터 진행하는  것이 순서일 것이다.</p>
<p>그 전에 표현을 좀 정리하고 가겠다.</p>
<script type="math/tex; mode=display">
\begin{aligned}
    x_i, t_i \text{: 주어진 데이터의 점들. 즉,} (x_i, t_i) \in D \\
    y(x_i, w) \text{: parameter가 $w$일 때의 모델의 예측 값.}
\end{aligned}
\tag{definition 1}</script><p>자, 그리고 우리는 다음과 같이 “가정”을 해보자.<br><em>데이터에서 주어진 target값과 우리의 예측 값의 차이가 Gaussian Distribution을 따른다</em></p>
<p>이를 수식으로 표현하면 다음과 같다.</p>
<script type="math/tex; mode=display">
\epsilon_i \sim \mathcal{N}(0, \beta^{-1})
\tag{definition 2}</script><p>여기서 $\epsilon_i$는 실측값과 데이터 간의 오차이다. ($\epsilon_i = t_i - y(x_i;w)$) </p>
<p>즉, 우리가 고등학교에서 확률과 통계를 착실히 배웠다면 다음과 같이 변형이 가능할 것이다.</p>
<script type="math/tex; mode=display">
\begin{aligned}
    t_i \sim \mathcal{N}(y(x_i;w), \beta^{-1}) \\
    Pr(t_i | x_i, w, \beta) = \mathcal{N}(y(x_i;w), \beta^{-1})
\end{aligned}
\tag{equation 1}</script><p>위의 내용을 시각화 하면 대충 다음과 같다.</p>
<p align="center"><img src="https://kimh060612.github.io/img/CF.png" width="100%"></p>

<p>이제 각각의 데이터에 대해서 이를 정의해 놓았으니, 데이터 전체에 대해서는 각각의 데이터가 i.i.d라는 가정 하게 그냥 곱해주면 된다. 다음과 같이 말이다.</p>
<script type="math/tex; mode=display">
\begin{aligned}
    D = \{(x_i, t_i) | 1 \leq i \leq N\} \\
    Pr(\textbf{t} | \textbf{x}, w, \beta) = \prod^{N}_{n = 1} \mathcal{N}(t_n | y(x_n;w), \beta^{-1})
\end{aligned}
\tag{equation 2}</script><p>자, 이제야 뭔가 손에 잡히는 느낌이 든다(아닌가?!?)<br>자세히 보면 위 식이 likelihood 아닌가? 정의 그대로이다.</p>
<p>이제 위의 Probability를 최대화 하면 되는 것이다.<br>근데 product는 다루기 어려우니 만능 툴인 로그를 씌워 보자.</p>
<script type="math/tex; mode=display">
\ln Pr(\textbf{t} | \textbf{x}, w, \beta) = -\frac{\beta}{2} \sum^{N}_{n = 1} (y(x_n;w) - t_n)^2 + \frac{N}{2} \ln \beta - \frac{N}{2} \ln 2\pi
\tag{equation 3}</script><p>이때, 우리는 2가지 parameter에 대해서 위 식의 최대값을 구해야 한다. 첫번째가 $w$이고 두번째가 $\beta$이다.<br>우선 $\beta$부터 해보자면 다음과 같이 가능하다.</p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial \beta} \ln Pr(\textbf{t} | \textbf{x}, w, \beta) = -\frac{1}{2} \sum^{N}_{n = 1} (y(x_n;w) - t_n)^2 + \frac{N}{2} \frac{1}{\beta}
\tag{equation 4}</script><p>결국 이 미분 값을 0으로 만드는 값이 최대값일 것이므로 다음과 같이 식을 풀어낼 수 있다.</p>
<script type="math/tex; mode=display">
\frac{1}{\beta_{ML}} = \frac{1}{2} \sum^{N}_{n = 1} (y(x_n;w) - t_n)^2
\tag{equation 5}</script><p>이제 $w$에 대해서 최대값을 구해보면 다음과 같다.</p>
<script type="math/tex; mode=display">
w_{ML} = \argmax_w \ln Pr(\textbf{t} | \textbf{x}, w, \beta) = \argmin_w \sum^{N}_{n = 1} (y(x_n;w) - t_n)^2</script><p>어..? 어디서 많이 본 것 같지만 일단 넘어가자. (실은 나중에 loss function 부분에서 한번 더 다루겠다.)</p>
<p>여기서의 최적화는 다양한 방법으로 시도할 수 있다.</p>
<p>이의 원리를 다시 한번 떠올려보자.</p>
<ol>
<li>에러가 Gaussian Distribution을 따를 때,</li>
<li>주어진 데이터 𝐷를 우리의 model이 데이터를 잘 설명할 수 있다는 가설이</li>
<li>참일 것이라는 확률(확실성)을 높이는 과정 </li>
</ol>
<p>이것이 바로 우리가 진행한 것이다.  </p>
<p>그렇다면, 이제 MAP를 활용해서 이를 구해보면 어떻게 될까?<br>Bayes’ Theorem에 따라 구하면 다음과 같이 작성 가능하다.</p>
<script type="math/tex; mode=display">
Pr(w | \textbf{t}, \textbf{x}, \alpha, \beta) \propto Pr(\textbf{t} | \textbf{x}, w, \beta) Pr(w|\alpha) 
\tag{eqaution 6}</script><p>위 식에서 뭔가 낯설지만 익숙한 것이 보인다. 바로 $Pr(w|\alpha)$이다. 이것이 바로 “사전 확률”이다.</p>
<p>사전 확률 또한 우리가 모르지만, 일단 만능인 Gaussian 이라고 가정해 보자. (이 가정이 나름의 타당성을 갖춘다는 것을 알고 싶으면 확률 및 랜덤 변수를 조금 깊게 공부해 보자.)</p>
<script type="math/tex; mode=display">
\begin{aligned}
    w \sim \mathcal{N}(0, \alpha^{-1}\textbf{I}) \\ 
    Pr(w|\alpha) = \mathcal{N}(w|0, \alpha^{-1}\textbf{I}) = (\frac{\alpha}{2\pi})^{\frac{M + 1}{2}} e^{-\frac{\alpha}{2}w^{\textbf{T}}w}
\end{aligned}
\tag{equation 7}</script><p>그렇다면, 우리는 이를 통해서 최대 (로그)사후 확률을 마찬가지로 미분을 통해서 구해보면 다음과 같다.</p>
<script type="math/tex; mode=display">
w_{ML} = \argmax_w \ln Pr(w | \textbf{t}, \textbf{x}, \alpha, \beta) = \argmin_w \frac{\beta}{2} \sum^{N}_{n = 1} (y(x_n;w) - t_n)^2 + \frac{\alpha}{2}w^{T}w
\tag{equation 8}</script><p>이것도 뭔가 어디서 본 것 같은데..? 라고 생각한다면 당신은 멋진 사람 :)</p>
<p>이걸로 Bayesian Statistics에서 말하는 likelihood와 prior/posterior 확률이 어떤 개념인지 조금이라도 와 닿았으면 좋겠다 ㅎㅎ.</p>
<h4 id="Information-Theory"><a href="#Information-Theory" class="headerlink" title="Information Theory"></a>Information Theory</h4><p>이 파트에서는 정보 이론을 매~~~~우 간단하게만 다루어볼 예정이다. 이것도 다룬다고 말하기도 부끄럽다고 할 수 있을 정도로만 다루도록 하겠다.</p>
<p>정보 이론은 “정보량”의 개념으로부터 시작한다. 이것이 현대에 들어서 통신, 패턴 인식 등 여러 분야에 사용되고 있다.<br>그리고 이러한 정보량의 평균을 “엔트로피”라고 부른다.</p>
<p>정보량은 다음과 같이 정의된다.</p>
<script type="math/tex; mode=display">
h(x) = -\log_2 p(x) \space \text{bits}</script><script type="math/tex; mode=display">
h(x) = -\ln p(x) \space \text{nats}</script><p>한가지 예를 들어 보겠다. 다음과 같이 정의되는 랜덤 변수가 있다고 가정해 보자.</p>
<p align="center"><img src="https://kimh060612.github.io/img/PVR.png" width="100%"></p>

<p>이의 엔트로피를 구해보면 다음과 같다.</p>
<p align="center"><img src="https://kimh060612.github.io/img/PVR_Entropy.png" width="100%"></p>

<p>이 내용을 갑자기 왜 하느냐? 우리는 다른 2개의 확률분포의 상대적 엔트로피와 상호 정보량을 계산하여 2개의 확률분포의 차이를 정량화 할 수 있기 때문이다.</p>
<p>우선, 우리가 근사를 목표로 하는 확률 분포를 $𝑝(𝑥)$라고 해보자.<br>이를 근사하기 위해서 모델을 학습시켜서 확률 분포 $𝑞(𝑥)$를 얻어 냈다고 치자.<br>이때, $𝑞(𝑥)$를 통해서 얻어낸 정보는 원래 $𝑝(𝑥)$를 통해서 얻을 수 있는 정보와 상이할 것이고, 우리는 이에 추가로 필요한 정보량의 평균을 다음과 같이 구할 수 있을 것이다.</p>
<script type="math/tex; mode=display">
KL(p||q) = - \int p(x)\ln q(x) dx - ( - \int p(x)\ln p(x)dx) \\
= - \int p(x)\ln q(x) - H_p[x]
\tag{definition 3}</script><p>자, 그렇다면 우리는 대충 궤가 보인다. 우리가 결국 학습시켜야 하는 분포 $𝑞(𝑥)$와 정답인 분포 $𝑝(𝑥)$사이의 차이를 정량화 해주는 함수가 바로 이것인 것이다.<br>이를 “쿨백-라이브러리 발산”이라고 하며, 이를 통해서 우리는 두 확률 분포의 차이를 알 수 있는데,<br>자세히 보니, 뒤의 목표 분포의 엔트로피는 그냥 상수나 다름이 없다. 따라서 앞의 항만을 따서 따로 “Cross-Entropy”라고 하며, 다음과 같이 표현할 수 있다.</p>
<script type="math/tex; mode=display">
KL(p||q) = CE(p||q) - H_p[x]
\tag{definition 3}</script><h4 id="Gradient"><a href="#Gradient" class="headerlink" title="Gradient"></a>Gradient</h4><p>이 파트에서는 대학교에서 Calculus를 배우지 않은 (배웠다 하더라도 다변수 함수의 미분 파트를 안 들은 이들) 사람들이 꼭 보아 주었으면 한다. Gradient의 기초 중의 기초를 다룰 예정이다.</p>
<p>우리는 이전 시간에 결국에는 미분을 구해서 가중치를 update하는 것이라고 배워왔다.<br>근데, 이때 “Gradient”라는 것에 대한 명확한 설명 없이, 그냥 하면 된다는 식으로 짚고 넘어갔던 기억이 난다.</p>
<p>우선, 함수에서 몇가지 예시부터 생각하고 넘어가자.</p>
<ol>
<li>$f: \mathbb{R}^1 \rightarrow \mathbb{R}^1$</li>
<li>$f: \mathbb{R}^N \rightarrow \mathbb{R}^1$</li>
<li>$f: \mathbb{R}^1 \rightarrow \mathbb{R}^N$</li>
<li>$f: \mathbb{R}^N \rightarrow \mathbb{R}^M$</li>
</ol>
<p>이때, $N,M \geq 2$이다.</p>
<p>$\mathbb{R}^1$을 따로 분리한 이유가 있다. 함수에서 입/출력이 벡터(또는 행렬)인 것과 입/출력이 Scalar인 것은 다소 상이한 과정을 도입해야 하기 때문이다. 우리는 이 중에서도 2번을 특히 중요하게 다룰 것이다.</p>
<p>이렇게 함수는 여러가지 형태가 있을 수 있는데 이때, 각각 정의역에 대한 미분이 어떻게 정의될까?<br>이 파트에서는 그 예시를 들어볼 것이다. </p>
<p>우선, $f: \mathbb{R}^N \rightarrow \mathbb{R}^1$ 에서의 경우를 보자. 일 변수 스칼라 함수의 경우는 빼겠다. 그걸 모르면 이걸 들을 자격이 없다.</p>
<p>우리는 이러한 함수를 vector-scalar함수라고 부르겠다.<br>이러한 함수의 미분은 다음과 같이 정의할 수 있다. </p>
<script type="math/tex; mode=display">
\textbf{x} \in \mathbb{R}^N, y \in \mathbb{R}^1</script><script type="math/tex; mode=display">
\textbf{x} = [x_1, x_2, ..., x_N], y = f(\textbf{x})</script><p>이렇게 정의되어 있을 때, 함수 $f$에 대한 Gradient는 다음과 같이 표현되고 정의된다.</p>
<script type="math/tex; mode=display">
\nabla_{\textbf{x}} f(\textbf{x}) = [\frac{\partial y}{\partial x_1}, \frac{\partial y}{\partial x_2}, ...,\frac{\partial y}{\partial x_N}]</script><p>여기서 만약 입력이 행렬이면 어떻게 되어야 할까?<br>즉, 다음과 같이 출력이 Scalar이고 입력이 벡터인 함수이다. (Matrix-Scalar함수)</p>
<script type="math/tex; mode=display">
\textbf{x} \in \mathbb{R}^{N*M}, y \in \mathbb{R}^1</script><script type="math/tex; mode=display">
\nabla_{\textbf{x}} f(\textbf{x}) = 
\begin{bmatrix}
    \frac{\partial y}{\partial x_{11}} \cdots \frac{\partial y}{\partial x_{1M}} \\
    \vdots \space \space \space \ddots \space \space \space \vdots \\
    \frac{\partial y}{\partial x_{N1}} \cdots \frac{\partial y}{\partial x_{NM}}
\end{bmatrix}</script><h3 id="Loss-Function-Examples"><a href="#Loss-Function-Examples" class="headerlink" title="Loss Function Examples"></a>Loss Function Examples</h3><hr>
<p>이 파트에서는 위에서 배운 수학적인 기초를 토대로 Loss function의 예시를 한번 볼 것이다. </p>
<p>우선 MSE부터 생각해 보자. 결국 MSE는 MLE/MAP를 이론적인 기저로 두고 있었다는 것을 위 글을 읽었다면 이해할 수 있었을 것이다. $equation 5$를 다시 한번 봐보자.</p>
<p>그렇다면 Cross-Entropy를 어떻게 사용할 것인가?<br>신경망에서는 이를 분류 문제를 어떻게 풀까? 우리는 Softmax 함수를 같이 사용하여 이를 해결할 수 있다.</p>
<p>먼저 softmax 함수부터 생각해 보자.</p>
<script type="math/tex; mode=display">
y_i = \frac{e^{u^K_i}}{\sum^{n_{out}}_{j = 1}e^{u^K_j}}</script><p>위 함수를 Neural Network에서 output layer의 각 출력 값을 확률 분포로 바꾸어 준다고 생각하면 된다. 실제로 모든 unit의 값을 다 더하면 1이 되니깐 말이다.</p>
<p>이제 이러한 확률 분포를 토대로 실제 우리가 원하는 target 확률 분포와의 거리를 구하는 과정을 Cross entropy로 진행할 수 있을 것이다. 다음과 같이 말이다.</p>
<script type="math/tex; mode=display">
E = -\sum^{n_{out}}_{i = 1} t_i \ln y_i</script><p>이때, target 분포는 class에 따라 one-hot encoding이 되어 있음<br>이때, 이 함수를 미분하면 어떻게 될까? 다음을 한번 보자.</p>
<p align="center"><img src="https://kimh060612.github.io/img/CE_Loss.png" width="100%"></p>

<p>결국 형태가 One hot encoding이 되어 있다면, MSE와 딱히 다를 것이 없다는 것을 알 수 있다.</p>
<h3 id="What-is-Optimizer"><a href="#What-is-Optimizer" class="headerlink" title="What is Optimizer?"></a>What is Optimizer?</h3><hr>
<p>이 파트에서는 위에서 배운 수학적 기초를 토대로 Optimizer가 어떻게 동작을 하는 놈들인지 배워 보도록 하겠다.</p>
<p>그래서 Deep learning에서의 Optimizer는 어떤 역할인지 알아보자. 결국 Deep learning은 해당 문제를 해결하는 것에 중심을 두고 있다. 이때, 우리는 특정한 문제를 해결한다고 했을 때, 특정 지표(ex. 정확도)를 최대화 한다는 것이다.<br>우리는 이때 정확도를 최대화 하기 위해서 데이터에서 얻을 수 있는 손실(Loss)를 최소화하는 것을 진행한다. 이는 기존 최적화와는 조금 다르다. 직접 지표를 최대화하는 것이 아닌, 간접적인 지표를 최소화하는 방향으로 가는 것이니 말이다. </p>
<p>그래서, 결국 Optimizer의 역할이 무엇인가?<br>다음과 같다.</p>
<script type="math/tex; mode=display">
\min_w \mathbf{E}_{x,y \sim \hat{P}(x,y)} [L(f(x^i;w), y^i)]</script><p>이게 뭐냐? 즉 데이터에 대한 손실을 최소화 하는 것이다. 이것이 Optimizer 의 궁극적인 역할인 것이다.<br>이 문제는 1줄만 생각해 보면 간단해 보이지만 실은 겁나게 어렵고 복잡한 문제이다.<br>이 문제를 풀기 위해서 우리는 Gradient Descent라는 방법을 사용하는 것이다. </p>
<p>여기서 Gradient Descent의 수렴성을 설명하고 싶지만…. 강의에서는 생략하도록 하겠다. 더 자세히 알고 싶으면 Talyor 급수를 키워드로 잘 찾아보기를 바란다. 만약 나중에 시간이 된다면 다시 다루어 보도록 하겠다.</p>
<p>이쯤에서 미니 배치의 의미를 설명하고 가도록 하겠다.<br>이 것은 다음 그림으로 설명이 될 수 있다.</p>
<p align="center"><img src="https://kimh060612.github.io/img/MiniBatch.png" width="100%"></p>

<p>~역시 필자는 필자 스스로 생각해도 발 그림이다. 당도췌 어떤 정신머리로 이딴걸 그리는지 모르겠다.~</p>
<p>자 각설하고, 전체 데이터에서 정해진 Batch Size 만큼을 뽑아서 만든 데이터를 Mini Batch라고 한다. 이를 신경망에 넣어서 Feed Forward를 하고 Back propagation 연산을 하면서 가중치를 업데이트한다. 이것이 학습의 1개의 step이다. 그리고 이 mini batch set으로 전부의 데이터를 학습 했을때, 그것을 1개의 epoch이라고 한다. (통상적으로) 이렇게만 간단하게 생각하고 나머지는 Optimizer 별로 따로 생각하면 편할 것이다.</p>
<h3 id="Optimizer-examples"><a href="#Optimizer-examples" class="headerlink" title="Optimizer examples"></a>Optimizer examples</h3><hr>
<p>이 파트에서는 Optimizer의 종류가 어떤 것들이 있는지 알아보도록 하겠다.</p>
<ol>
<li>Stochastic Gradient Descent</li>
</ol>
<p>이 Optimizer는 가장 간단한 Optimizer라고 생각해도 된다. 위에서 설명한 대로, 간단하게 Mini Batch를 뽑아서 Update를 진행한다.</p>
<p align="center"><img src="https://kimh060612.github.io/img/SGD.png" width="100%"></p>

<ol>
<li>Adagrad</li>
</ol>
<p>이 방법은 이전에 사용했던 gradient들을 축적해서 사용하는 방법이다. gradient를 원소별로 제곱해서 step별로 더해가는 행렬을 하나 만들고, 그 행렬을 update할때 gradient에 원소별로 곱해주는 것이다. 제목 그대로 Adaptive gradient 방법인 것이다.</p>
<p align="center"><img src="https://kimh060612.github.io/img/Adagrad.png" width="100%"></p>

<ol>
<li>RMSProp</li>
</ol>
<p>이 방법은 위의 Adagrad 방법에서 조금 더 나아가, gradient를 더해나갈때 가중치를 두는 방법이다. </p>
<p align="center"><img src="https://kimh060612.github.io/img/RMSProp.png" width="100%"></p>

<ol>
<li>Adam</li>
</ol>
<p>현재 가장 많이들 쓰는 Optimizer이다. 이것도 별거 없다. RMSProp 처럼 가중합을 진행하는데, 이번에는 원소별로 제곱하지 않은 gradient도 누적 합을 저장해서 사용한다. 자세한 것을 수식을 보면 바로 감이 올 것이다.</p>
<p align="center"><img src="https://kimh060612.github.io/img/Adam.png" width="100%"></p>

<p>이걸로 Optimization 파트 A는 끝이 났다. 다음 파트는 위에서 배운 것들을 구현하면서 찾아 오도록 할 것이다.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kimh060612.github.io/2022/03/05/RNN-A/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Michael Kim">
      <meta itemprop="description" content="Backend Engineering, Deep learning, Algorithm & DS, CS 전공 지식 공부 저장소">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Michael's Study Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/05/RNN-A/" class="post-title-link" itemprop="url">Recurrent Neural Network 강의 내용 part.A</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-03-05 19:26:50" itemprop="dateCreated datePublished" datetime="2022-03-05T19:26:50+09:00">2022-03-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-05-30 00:00:32" itemprop="dateModified" datetime="2022-05-30T00:00:32+09:00">2022-05-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2022/03/05/RNN-A/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2022/03/05/RNN-A/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><em>본 포스트는 Hands-on Machine learning 2nd Edition, CS231n, Tensorflow 공식 document를 참조하여 작성되었음을 알립니다.</em></p>
<h3 id="Index"><a href="#Index" class="headerlink" title="Index"></a>Index</h3><ol>
<li>Definition of Recurrent Neural Network(RNN)</li>
<li>Back Propagation of RNN</li>
<li>Partice</li>
</ol>
<p>여기서는 1,2는 Part. A이고 3은 Part. B에서 다루도록 하겠다.<br><br></p>
<h3 id="Definition-of-Recurrent-Neural-Network-RNN"><a href="#Definition-of-Recurrent-Neural-Network-RNN" class="headerlink" title="Definition of Recurrent Neural Network(RNN)"></a>Definition of Recurrent Neural Network(RNN)</h3><hr>
<p>RNN(Recurrent Neural Network)은 시계열 데이터를 처리하는데 특화된 신경망 구조이다.<br>이는 전의 입력이 연속해서 다음 입력에 영향을 주는 신경망 구조로써, 같은 구조가 계속 순환되어 나타나기 때문에 이러한 이름이 붙어져 있다.<br>다음 그림을 보자. </p>
<p align="center"><img src="https://kimh060612.github.io/img/RNN.png" width="100%"></p>

<p>하지만 이 그림으로는 자세한 구조까지는 잘 모르겠다. 그래서 필자가 직접 그린 그림을 보면서 설명하도록 하겠다. 말해두겠지만 필자는 그림을 심각하게 못 그리니 양해 바란다.</p>
<p align="center"><img src="https://kimh060612.github.io/img/RNND.png" width="100%"></p>

<p>이처럼 각 RNN Cell에 FCNN의 Unit이 들어가 있는 형식으로 구현된다. 이의 Feed Forward 연산을 생각해 보면 정말 간단하다.</p>
<script type="math/tex; mode=display">
z_t = f(z_{t - 1}W_{re} + x_tW_{in} + b)
\tag{equation 1}</script><p>$equation\space 1$에서 각 변수들의 정의는 다음과 같다.</p>
<blockquote>
<p>Definition 1</p>
</blockquote>
<ul>
<li>$z_t$: time step $t$에 unit의 값에 activation function에 넣은 값</li>
<li>$W_{re}$: Reccurent 가중치</li>
<li>$W_{in}$: input layer에서의 가중치</li>
<li>$x_t$: input vector</li>
</ul>
<p>그림으로 정리하면 대충 다음과 같다.</p>
<p align="center"><img src="https://kimh060612.github.io/img/RNND2.png" width="100%"></p>

<p>결국 이렇게 Sequential한 데이터 $x$에 대해서 Sequential한 output $y$를 뽑을 수 있는 것이다.</p>
<h3 id="Back-Propagation-of-RNN"><a href="#Back-Propagation-of-RNN" class="headerlink" title="Back Propagation of RNN"></a>Back Propagation of RNN</h3><hr>
<p>RNN의 Back Propagation 방법은 다음의 2가지가 있다. </p>
<ul>
<li>Back Propagation Through Time - (BPTT)</li>
<li>Real Time Recurrent Learning (RTRL)</li>
</ul>
<p>여기서는 BPTT만을 다루도록 하겠다. RNN을 그렇게 자세하게 다루지 않는 이유는 FCNN의 연장선 느낌이 강해서이고 또한 현대에서는 딱히 잘 쓰이지 않기 때문이다. 그냥 지적 유희를 위해서 또는 기본기를 잘 닦기 위해서로만 읽어주기를 바란다.</p>
<p>또는 AutoGrad 계열의 알고리즘을 공부하는 사람들은 미분 그래프를 사용한 AutoGrad이전에는 이런 식으로 역전파 알고리즘을 구현했었구나 라고 역사책 읽는 느낌으로 읽어주면 매우 감사하겠다. </p>
<p>이러한 RNN의 Back Propagation을 진행하기 위해서는 다음을 구하면 된다.</p>
<script type="math/tex; mode=display">
\begin{aligned}
    \frac{\partial E}{\partial w^{out}_{ij}} =\space ? \\
    \frac{\partial E}{\partial w^{re}_{ij}} = \space ? \\
    \frac{\partial E}{\partial w^{in}_{ij}} = \space?
\end{aligned}
\tag{definition 2}</script><p>이제 위 미분들에 chain rule을 적용해 보자. 그렇다면 다음과 같이 표현할 수 있다.</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial w^{out}_{ij}} = \sum^T_{t = 1} \frac{\partial E}{\partial v^{out}_{i}} \frac{\partial v^{out}_{i}}{\partial w^{out}_{ij}}
\tag{equation 2}</script><script type="math/tex; mode=display">
\frac{\partial E}{\partial w^{re}_{ij}} = \sum^T_{t = 1} \frac{\partial E}{\partial u^{t + 1}_{i}} \frac{\partial u^{t + 1}_{i}}{\partial w^{re}_{ij}}
\tag{equation 3}</script><script type="math/tex; mode=display">
\frac{\partial E}{\partial w^{in}_{ij}} = \sum^T_{t = 1} \frac{\partial E}{\partial u^{in}_{i}} \frac{\partial u^{in}_{i}}{\partial w^{out}_{ij}}
\tag{equation 4}</script><p>자, 그렇다면 여기서 delta를 정의해서 일반화된 delta 규칙을 적용해 보아야 Back Propagation이 효율적으로 될 것이다. </p>
<p>그렇다면 각 미분들에 대한 delta는 다음과 같이 정의될 수 있다.</p>
<script type="math/tex; mode=display">
\delta^{out}_{k,t} = \frac{\partial E}{\partial v^t_k} = \frac{\partial E}{\partial y^t_k} \frac{\partial y^t_k}{\partial v^t_k} = \frac{\partial E}{\partial y^t_k} f'(v^t_k)
\tag{definition 3}</script><p>input layer에서의 가중치는 output layer처럼 FCNN과 완전히 같다. 굳이 적어두지는 않도록 하겠다. 그렇다면 남은 것은 recurrent layer에서의 delta이다. 전개해보면 대략 다음과 같이 표현할 수 있다.</p>
<p align="center"><img src="https://kimh060612.github.io/img/RNNDBPTT.png" width="100%"></p>

<p>이를 수식으로 표현하면 다음과 같이 표현할 수 있다.</p>
<script type="math/tex; mode=display">
\delta^{re}_{j,t} = \sum^{n_{out}}_{i = 1} \frac{\partial E}{\partial v^t_i} \frac{\partial v^t_i}{\partial u^t_j} + \sum^{n_{re}}_{i = 1} \frac{\partial E}{\partial u^{t + 1}_i} \frac{\partial u^{t + 1}_i}{\partial u^t_j}
\tag{definition 4}</script><p>각 Summation Term들의 delta를 이용해서 표현해 보면 다음과 같다.</p>
<script type="math/tex; mode=display">
\delta^{re}_{j,t} = \sum^{n_{out}}_{i = 1} \delta^{out}_{i,t} \frac{\partial v^t_i}{\partial u^t_j} + \sum^{n_{re}}_{i = 1} \delta^{re}_{i, t} \frac{\partial u^{t + 1}_i}{\partial u^t_j}  \\ 
= \sum^{n_{out}}_{i = 1} \delta^{out}_{i,t} w^{out}_{ij} f'(u^t_j) + \sum^{n_{re}}_{i = 1} \delta^{re}_{i, t} w^{re}_{ij} f'(u^t_j)
\tag{equation 4}</script><p>즉, 이와 같이 RNN Cell의 기본 형태의 BPTT는 정말 단순하게도 FCNN의 연장선이다. 별게 없다.</p>
<p>그리고 이의 변형판으로 시간을 분할해서 Update하는 방법도 있다. 이를 Truncated BPTT라고 하는데 관심이 있다면 찾아보도록 하자. 이것도 진짜 별거 없다.</p>
<p>그저 위의 미분에서 시간 term을 잘라서 update 해주면 된다.</p>
<p>이걸로 RNN 또한 끝이 났다. 다음 파트에서는 이를 구현해 보도록 하겠다.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kimh060612.github.io/2022/03/05/RNN-B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Michael Kim">
      <meta itemprop="description" content="Backend Engineering, Deep learning, Algorithm & DS, CS 전공 지식 공부 저장소">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Michael's Study Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/05/RNN-B/" class="post-title-link" itemprop="url">Recurrent Neural Network 강의 내용 part.B</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-03-05 19:26:47" itemprop="dateCreated datePublished" datetime="2022-03-05T19:26:47+09:00">2022-03-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-05-30 00:00:32" itemprop="dateModified" datetime="2022-05-30T00:00:32+09:00">2022-05-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2022/03/05/RNN-B/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2022/03/05/RNN-B/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><em>본 포스트는 Hands-on Machine learning 2nd Edition, CS231n, Tensorflow 공식 document를 참조하여 작성되었음을 알립니다.</em></p>
<h3 id="Index"><a href="#Index" class="headerlink" title="Index"></a>Index</h3><ol>
<li><del>Definition of Recurrent Neural Network(RNN)</del></li>
<li><del>Back Propagation of RNN</del></li>
<li>Partice</li>
</ol>
<p><br></p>
<h3 id="Partice"><a href="#Partice" class="headerlink" title="Partice"></a>Partice</h3><hr>
<p>진짜 여기까지 과연 읽은 사람이 있을까 싶다. 있다면 압도적 감사의 의미로 그랜절을 박고 싶은 마음이다. ㅋㅋ </p>
<p>장난은 그만하고 오늘도 시작하자. 오늘도 역시 Model Subclassing API를 활용하여 간단하게 RNN을 활용하는 예제를 구현해볼 것이다.</p>
<p>우선 RNN을 tensorflow에서 어떻게 사용할 수 있는지부터 보자.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RNNLayer</span>(keras.Model):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_hidden=<span class="number">128</span>, num_class=<span class="number">39</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(RNNLayer, self).__init__()</span><br><span class="line">        self.RNN1 = keras.layers.SimpleRNN(num_hidden, activation=<span class="string">&#x27;tanh&#x27;</span>, return_sequences=<span class="literal">True</span>)</span><br><span class="line">        self.out = keras.layers.Dense(num_class, activation=<span class="string">&quot;softmax&quot;</span>)</span><br><span class="line">        self.out = keras.layers.TimeDistributed(self.out)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.RNN1(x)</span><br><span class="line">        out = self.out(x)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<p>보면 알다시피 아주 간단하다. 그래서 이번 포스트에서는 간단하게 2개의 관전 포인트만을 다루려고 한다.</p>
<ol>
<li>SimpleRNN layer의 특징</li>
<li>TimeDistributed layer의 특징</li>
</ol>
<p>1번부터 차례로 시작하자.</p>
<p>우선 SimpleRNN을 이해하려면 입력으로는 어떤 tensor를 받아먹어서 출력으로는 어떤 tensor를 뱉어내는지를 알아야 한다.<br>지난 포스트에서 RNN의 구조를 보았다면 당연히 입력은 시간순서대로 벡터가 들어가니 적어도 3차원(배치까지 포함해서) 일 것이고 출력도 시간 순서대로 나와야 하니 같은 3차원이라는 것 쯤은 유추가 가능할 것이다. 하지만 실제로는 이것 또한 조절이 가능하다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.SimpleRNN(</span><br><span class="line">    units, activation=<span class="string">&#x27;tanh&#x27;</span>, use_bias=<span class="literal">True</span>,</span><br><span class="line">    kernel_initializer=<span class="string">&#x27;glorot_uniform&#x27;</span>,</span><br><span class="line">    recurrent_initializer=<span class="string">&#x27;orthogonal&#x27;</span>,</span><br><span class="line">    bias_initializer=<span class="string">&#x27;zeros&#x27;</span>, kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">    recurrent_regularizer=<span class="literal">None</span>, bias_regularizer=<span class="literal">None</span>, activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">    kernel_constraint=<span class="literal">None</span>, recurrent_constraint=<span class="literal">None</span>, bias_constraint=<span class="literal">None</span>,</span><br><span class="line">    dropout=<span class="number">0.0</span>, recurrent_dropout=<span class="number">0.0</span>, return_sequences=<span class="literal">False</span>, return_state=<span class="literal">False</span>,</span><br><span class="line">    go_backwards=<span class="literal">False</span>, stateful=<span class="literal">False</span>, unroll=<span class="literal">False</span>, **kwargs</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>이는 tensorflow 공식 문서에 적혀 있는 SimpleRNN에 관한 내용이다. 여기서 우리가 관심 있게 봐야 할 것은 return_sequences와 return_state이다.</p>
<p>만약, return_sequences가 False면, SimpleRNN layer는 맨 마지막의 출력만을 뱉어낸다. 즉, 2차원 출력이 되는 것이다. ([Batch_size, output_dim]) 하지만 이 파라미터가 True이면 모든 시간의 출력을 출력으로 내뱉는다. 즉, 3차원 출력이 되는 것이다. </p>
<p>그리고 return_state의 경우, 이것이 True이면 출력의 Hidden State를 출력으로 같이 내뱉는다. 즉, 출력이 tuple이 되는 것이다. (출력 벡터, hidden 벡터)</p>
<p>이렇게 되면 우리는 여러가지 경우의 수를 고려해서 모델을 만드는 것이 가능해진다.</p>
<ol>
<li>맨 마지막 출력만을 고려하여 예측하는 모델</li>
<li>입력마다 다음에 나올 출력을 예측하는 모델</li>
</ol>
<p>전자를 Many to One, 후자를 Many to Many라고 한다.</p>
<p>그렇다면 Many to Many를 해주기 위해서는 시간 순서에 맞춰서 <em>같은</em> Dense layer를 적용해줄 필요가 있다. 이를 위해서 필요한 것이 바로 TimeDistributed layer이다. 이는 각 시간 step에 대해서 같은 Dense layer의 weight로 연산을 진행해 준다.</p>
<p>하지만 기본적으로 tensorflow에서 Dense layer는 broadcast 기능을 제공한다. 따라서 굳이 TimeDistributed layer 없이도 3차원 텐서가 들어오면 맨 마지막 차원에 대해서 Dense 연산을 진행하게 된다. </p>
<p>따라서, Dense만을 사용할거면 굳이 TimeDistributed layer가 필요 없다.</p>
<p>RNN의 포스팅은 이걸로 마치도록 하겠다. 딱히 크게 어려운 점도 없고 나중에 나올 Attention이 훨씬 더 어그로가 끌려야할 주제이기 때문이다.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kimh060612.github.io/2022/03/05/CNN-B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Michael Kim">
      <meta itemprop="description" content="Backend Engineering, Deep learning, Algorithm & DS, CS 전공 지식 공부 저장소">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Michael's Study Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/05/CNN-B/" class="post-title-link" itemprop="url">Convoltional Neural Network 강의 내용 part.B</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-03-05 19:26:43" itemprop="dateCreated datePublished" datetime="2022-03-05T19:26:43+09:00">2022-03-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-05-30 00:00:32" itemprop="dateModified" datetime="2022-05-30T00:00:32+09:00">2022-05-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2022/03/05/CNN-B/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2022/03/05/CNN-B/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><em>본 포스트는 Hands-on Machine learning 2nd Edition, CS231n, Tensorflow 공식 document를 참조하여 작성되었음을 알립니다.</em></p>
<h3 id="Index"><a href="#Index" class="headerlink" title="Index"></a>Index</h3><ol>
<li><del>Introduction of Convolution Operation</del></li>
<li><del>Definition of Convolutional Neural Network(CNN)</del></li>
<li><del>Back Propagation of CNN</del></li>
<li>Partice</li>
</ol>
<h4 id="6-Partice"><a href="#6-Partice" class="headerlink" title="6. Partice"></a>6. Partice</h4><hr>
<p>이 파트는 Convolutional Nerual Network를 직접 구현하는 파트이다. 비단 Convolutional Neural Network 뿐만 아니라 그를 이용한 다양한 현대적인 네트워크 구조들을 구현해 보는 시간을 가지도록 하겠다. 필자의 Deep learning 구현 관련 블로그 포스팅은 전부 Model Subclassing API로 구현될 예정이다. 왜냐? 필자 맘이다 <del>(꼬우면 보지 말든가)</del> 장닌이고, 필자가 생각하기에는 Model Subclassing API의 활용 장점은 확실히 있는 것 같다.</p>
<ol>
<li><p>모델을 가독성 있게 관리할 수 있다.</p>
<p>이는 전적으로 OOP에 대한 기본 개념 및 디자인 패턴을 잘 아는 사람에 한에서 그런거다.<br>Vision Transformer쯤 가면 알겠지만, 정말 짜야하는 연산들이 엄청 많다. 그런걸 하나하나 함수로 짜거나 Sequential API로 구성하면 지옥문이 열리게 된다. 아 물론 짜는건 무리가 없겠지만, 유지보수 관점에서는 정말 지옥일 것이다.<br>그런 의미에서 Model Subclassing API는 원하는 연산을 Class단위로 묶어서 설계하고 그들을 체계적으로 관리할 수 있는 지식이 조금이라도 있다면 (복잡한 디자인 패턴까지는 필요도 없다) 훨씬 가독성이 높은 코드를 짤 수 있다.</p>
</li>
<li><p>Low Level한 연산을 자유롭게 정의할 수 있다.</p>
<p> Model Subclassing API를 사용하면 Custom Layer, Scheduler 등등 여러 연산을 사용자의 입맛에 맞게 정의할 수 있다. 이러면 내가 세운 새로운 가설, 연구 아이디어를 보다 쉽게 구현할 수 있는 판로가 열리는 것이다. 물론 이는 전적으로 자신이 새로운 연산을 구상하고 구현할만한 경지에 도달했을때의 이야기이다.</p>
</li>
<li><p>특히 Pytorch로 소스코드 전환을 비교적 쉽게 할 수 있다.</p>
<p>이건 지극히 필자의 개인적인 생각이다. 필자는 pytorch와 tensorflow를 동시에 써가면서 일을 하고 있다. 모델 개발 및 연구는 pytorch로 배포는 tensorflow를 사용하고 있는데, 모델을 tensorflow로 완전히 포팅해야할 일이 가끔씩 있다. 이때 model subclassing API를 활용하는 편이 소스코드의 구조나 뽄새가 비슷해서 편했던 기억이 난다.</p>
</li>
</ol>
<p>하지만 단점도 명확하게 있다.</p>
<ol>
<li><p>못쓰면 이도 저도 안된다.</p>
<p> 보면 알다시피, OOP의 기초 지식과 low level로 연산을 정의해서 사용할 수 있는 사람이 아니라면 굳이 Subclassing API를 쓰겠다고 깝치다가 되려 오류만 범할 가능성이 높다.</p>
</li>
</ol>
<p>하지만 필자는 앞으로 잘하고 싶어서 힘든 길을 골라 보았다. 독자들도 이에 동의하리라고 믿는다. <del>(아니면 뒤로 가든가)</del></p>
<p>사족이 길었는데, 앞으로도 계속 Model Subclassing API만을 사용해서 포스팅을 할 예정이다.</p>
<p>우선, 지난 FCNN처럼 tensorflow 2로 어떻게 CNN layer를 정의할 수 있는지부터 알아보자.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CustomConv2D</span>(keras.layers.Layer):</span><br><span class="line">    <span class="comment">#              1. output image의 채널  2. 커널의 이미지 사이즈  3. Stride를 정해줬어야함. 4. Pooling을 정해줬어야함.(Optional) 5. Padding을 정해야함.</span></span><br><span class="line">    <span class="comment">#                                                       i  x 방향으로의 stride y 방향으로의 stride      i</span></span><br><span class="line">    <span class="comment"># &quot;SAME&quot; OH = H, &quot;VALID&quot; 가능한 패딩 필터 중에서 가장 작은 패딩(양수)으로 설정  </span></span><br><span class="line">    <span class="comment"># OH = (H + 2*P - KH)/S + 1 = 15.5</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, out_channel, kernel_size, Strides = (<span class="params"><span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span></span>), Padding = <span class="string">&quot;SAME&quot;</span>, trainable=<span class="literal">True</span>, name=<span class="literal">None</span>, dtype=<span class="literal">None</span>, dynamic=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(trainable=trainable, name=name, dtype=dtype, dynamic=dynamic, **kwargs)</span><br><span class="line">        <span class="comment"># &quot;3,4&quot; </span></span><br><span class="line">        self.out_channel = out_channel</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(kernel_size) == <span class="built_in">type</span>(<span class="number">1</span>):</span><br><span class="line">            self.kernel_size = (kernel_size, kernel_size)</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">type</span>(kernel_size) == <span class="built_in">type</span>(<span class="built_in">tuple</span>()):</span><br><span class="line">            self.kernel_size = kernel_size</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Not a Valid Kernel Type&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(Strides) == <span class="built_in">type</span>(<span class="number">1</span>):</span><br><span class="line">            self.Stride = (<span class="number">1</span>, Strides, Strides, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">type</span>(Strides) == <span class="built_in">type</span>(<span class="built_in">tuple</span>()):</span><br><span class="line">            self.Stride = Strides</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Not a Valid Stride Type&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(Padding) == <span class="built_in">type</span>(<span class="built_in">str</span>()):</span><br><span class="line">            self.Padding = Padding</span><br><span class="line">        <span class="keyword">else</span> :</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Not a Valid Padding Type&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build</span>(<span class="params">self, input_shape</span>):</span><br><span class="line">        WeightShape = (self.kernel_size[<span class="number">0</span>], self.kernel_size[<span class="number">1</span>], input_shape[-<span class="number">1</span>], self.out_channel)</span><br><span class="line">        self.Kernel = self.add_weight(</span><br><span class="line">            shape=WeightShape,</span><br><span class="line">            initializer=<span class="string">&quot;random_normal&quot;</span>,</span><br><span class="line">            trainable= <span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.Bias = self.add_weight(</span><br><span class="line">            shape=(self.out_channel, ),</span><br><span class="line">            initializer=<span class="string">&quot;random_normal&quot;</span>,</span><br><span class="line">            trainable=<span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, Input</span>):</span><br><span class="line">        Out = tf.nn.conv2d(Input, self.Kernel, strides=self.Stride, padding=self.Padding)</span><br><span class="line">        Out = tf.nn.bias_add(Out, self.Bias, data_format=<span class="string">&quot;NHWC&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> Out</span><br></pre></td></tr></table></figure>
<p>이전에도 설명했듯이 build에서 필요한 Weight를 정의한 뒤에 call에서 그것을 사용한다. 다행이게도 tensorflow에서는 최소한 convolution 연산을 정의해 주었다.<br>앞으로도 필요한 연산이 있다면 이렇게 정의해 주면 된다.</p>
<p>하지만 우리는 굳이 이렇게 convolution layer를 정의해줄 필요가 없다. 왜냐면 tensorflow keras에서 이미 정의되어 있는 좋은 함수가 있기 때문이다. 이에 대한 아주 간단한 사용 예제로써 Alexnet과 ResNet을 구현해 보도록 하겠다. 부록으로 GoogLeNet을 구현한 예제도 있는데, 이는 필자의 Github에 올려 두도록 할테니 시간이 되면 가서 봐 주었으면 한다.</p>
<p>우선 AlexNet부터 가보자. 모델의 구조를 사진으로 한번 봐보도록 하자.</p>
<p align="center"><img src="http://kimh060612.github.io/img/AlexNet.jpg" width="70%" height="70%"></p>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AlexNet</span>(keras.Model):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 원래 여기에는 커널 사이즈로 (11, 11)이 들어가고 padding은 valid이다. 하지만 메모리 때문에 돌아가지 않는 관계로 이미지 크기를 줄아느라 부득이하게 모델을 조금 변경했다.</span></span><br><span class="line">        self.Conv1 = keras.layers.Conv2D(<span class="number">96</span>, (<span class="number">3</span>, <span class="number">3</span>), strides=(<span class="number">4</span>, <span class="number">4</span>), padding=<span class="string">&quot;SAME&quot;</span>, activation=<span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        <span class="comment"># LRN 1</span></span><br><span class="line">        self.BatchNorm1 = keras.layers.BatchNormalization()</span><br><span class="line">        self.MaxPool1 = keras.layers.MaxPool2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>), strides=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">&quot;VALID&quot;</span>)</span><br><span class="line"></span><br><span class="line">        self.Conv2 = keras.layers.Conv2D(<span class="number">256</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), padding=<span class="string">&quot;SAME&quot;</span>, activation=<span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        <span class="comment"># LRN2</span></span><br><span class="line">        self.BatchNorm2 = keras.layers.BatchNormalization()</span><br><span class="line">        self.MaxPool2 = keras.layers.MaxPool2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>), strides=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">&quot;VALID&quot;</span>)</span><br><span class="line"></span><br><span class="line">        self.Conv3 = keras.layers.Conv2D(<span class="number">384</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&quot;SAME&quot;</span>, activation=<span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        self.Conv4 = keras.layers.Conv2D(<span class="number">384</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&quot;SAME&quot;</span>, activation=<span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        self.Conv5 = keras.layers.Conv2D(<span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&quot;SAME&quot;</span>, activation=<span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        self.MaxPool3 = keras.layers.MaxPool2D(pool_size=(<span class="number">3</span>, <span class="number">3</span>), strides=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">        self.Flat = keras.layers.Flatten()</span><br><span class="line"></span><br><span class="line">        self.Dense1 = keras.layers.Dense(<span class="number">4096</span>, activation=<span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        self.DropOut1 = keras.layers.Dropout(<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">        self.Dense2 = keras.layers.Dense(<span class="number">4096</span>, activation=<span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        self.DropOut2 = keras.layers.Dropout(<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">        self.OutDense = keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, Input</span>):</span><br><span class="line">        X = self.Conv1(Input)</span><br><span class="line">        X = self.BatchNorm1(X)</span><br><span class="line">        X = self.MaxPool2(X)</span><br><span class="line">        X = self.Conv2(X)</span><br><span class="line">        X = self.BatchNorm2(X)</span><br><span class="line">        X = self.MaxPool2(X)</span><br><span class="line">        X = self.Conv3(X)</span><br><span class="line">        X = self.Conv4(X)</span><br><span class="line">        X = self.Conv5(X)</span><br><span class="line">        X = self.MaxPool3(X)</span><br><span class="line">        X = self.Flat(X)</span><br><span class="line">        X = self.Dense1(X)</span><br><span class="line">        X = self.DropOut1(X)</span><br><span class="line">        X = self.Dense2(X)</span><br><span class="line">        X = self.DropOut2(X)</span><br><span class="line">        X = self.OutDense(X)</span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure>
<p>자, 필자는 굳이 더럽게 짜 보았다. 왜냐? <em>이렇게 짤거면 Model Subclassing을 쓰지 말라는 의미로 이렇게 짜 보았다.</em> 진짜 이따구로 짤거면 그냥 Sequential API나 Functional API를 사용하자. 근데 이 정도면 설명이 필요 없을 정도로 그냥 무지성 구현을 시전한 것이다. 그러니 간단하게 Keras의 Conv2D를 설명하고 넘어 가도록 하겠다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Conv2D(filters, kernel_size=(kernel_sz, kernel_sz), padding=<span class="string">&quot;SAME&quot;</span>, activation=<span class="string">&quot;relu&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>이전에 이론 글에서 설명했던 부분을 다시 되짚어 보고 위 함수를 다시 살펴보자.</p>
<blockquote>
<p><em>Definition 1</em></p>
</blockquote>
<ul>
<li>$w_{ijmk}^l$: $l$번째 층의 Weight의 $k$번째 Kernel Set에 $m$번째 Channel, $i$행, $j$열의 성분</li>
</ul>
<p>위의 $w_{ijmk}^l$를 우리는 위의 Conv2D 함수로 정의한 것이다. filters는 kernel set의 개수를 의미하며, kernel_size는 Weight kernel의 이미지 크기를 의미한다. padding은 “SAME”과 “VALID”가 있는데, “SAME”으로 하면 알아서 크기를 계산해서 입력 이미지와 출력 이미지의 크기를 같게 만든다. Valid를 선택하면 그냥 padding이 없다고 판단하면 된다. </p>
<p>AlextNet에서 대충 Conv2D를 어떻게 사용하는지 감이 왔다면, ResNet을 한번 구현해 보자.</p>
<p>ResNet에 대한 자세한 설명은 다른 포스트에서 정말 이게 맞나 싶을 정도로 분해해서 설명하도록 하겠다. 지금은 그저 다음과 같은 구조가 있구나 정도만 이해하고 넘어가면 된다.</p>
<p align="center"><img src="http://kimh060612.github.io/img/ResNet50.png" width="70%" height="70%"></p>

<p>이것이 ResNet50의 구조인데, 2가지 layer를 구현해 보아야 한다. 첫번째는 Conv Block이고 두번째는 Identity Block이다. 하나는 Skip Connection에 Convolution layer를 입힌 것이고 다른 하나는 그렇지 않은 것 뿐이다.</p>
<blockquote>
<p>Residual Conv Block</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResidualConvBlock</span>(tfk.layers.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, InputChannel, OutputChannel, strides = (<span class="params"><span class="number">1</span>, <span class="number">1</span></span>), trainable=<span class="literal">True</span>, name=<span class="literal">None</span>, dtype=<span class="literal">None</span>, dynamic=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(trainable=trainable, name=name, dtype=dtype, dynamic=dynamic, **kwargs)</span><br><span class="line"></span><br><span class="line">        self.Batch1 = tfk.layers.BatchNormalization(momentum=<span class="number">0.99</span>, epsilon= <span class="number">0.001</span>)</span><br><span class="line">        self.conv1 = tfk.layers.Conv2D(filters=InputChannel, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), strides=strides)</span><br><span class="line">        self.LeakyReLU1 = tfk.layers.LeakyReLU()</span><br><span class="line">        self.Batch2 = tfk.layers.BatchNormalization(momentum=<span class="number">0.99</span>, epsilon= <span class="number">0.001</span>)</span><br><span class="line">        self.conv2 = tfk.layers.Conv2D(filters=InputChannel, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), strides=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">&quot;SAME&quot;</span>)</span><br><span class="line">        self.LeakyReLU2 = tfk.layers.LeakyReLU()</span><br><span class="line">        self.Batch3 = tfk.layers.BatchNormalization(momentum=<span class="number">0.99</span>, epsilon= <span class="number">0.001</span>)</span><br><span class="line">        self.conv3 = tfk.layers.Conv2D(filters=OutputChannel, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), strides=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.LeakyReLU3 = tfk.layers.LeakyReLU()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Skip Connection</span></span><br><span class="line">        self.SkipConnection = tfk.layers.Conv2D(filters=OutputChannel, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), strides=strides)</span><br><span class="line">        self.SkipBatch = tfk.layers.BatchNormalization(momentum=<span class="number">0.99</span>, epsilon= <span class="number">0.001</span>)</span><br><span class="line">        self.LeakyReLUSkip = tfk.layers.LeakyReLU()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, Input</span>):</span><br><span class="line">        Skip = Input</span><br><span class="line">        Skip = self.SkipConnection(Skip)</span><br><span class="line">        Skip = self.SkipBatch(Skip)</span><br><span class="line">        Skip = self.LeakyReLUSkip(Skip)</span><br><span class="line">        Z = Input</span><br><span class="line">        Z = self.conv1(Z)</span><br><span class="line">        Z = self.Batch1(Z)</span><br><span class="line">        Z = self.LeakyReLU1(Z)</span><br><span class="line">        Z = self.conv2(Z)</span><br><span class="line">        Z = self.Batch2(Z)</span><br><span class="line">        Z = self.LeakyReLU2(Z)  </span><br><span class="line">        Z = self.conv3(Z)</span><br><span class="line">        Z = self.Batch3(Z)</span><br><span class="line">        Z = self.LeakyReLU3(Z)</span><br><span class="line">        <span class="keyword">return</span> Z + Skip </span><br></pre></td></tr></table></figure>
<blockquote>
<p>Residual Identity Block</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResidualIdentityBlock</span>(tfk.layers.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, InputChannel, OutputChannel, trainable=<span class="literal">True</span>, name=<span class="literal">None</span>, dtype=<span class="literal">None</span>, dynamic=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(trainable=trainable, name=name, dtype=dtype, dynamic=dynamic, **kwargs)</span><br><span class="line">        self.Batch1 = tfk.layers.BatchNormalization(momentum=<span class="number">0.99</span>, epsilon= <span class="number">0.001</span>)</span><br><span class="line">        self.conv1 = tfk.layers.Conv2D(filters=InputChannel, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), strides=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.LeakyReLU1 = tfk.layers.LeakyReLU()</span><br><span class="line">        self.Batch2 = tfk.layers.BatchNormalization(momentum=<span class="number">0.99</span>, epsilon= <span class="number">0.001</span>)</span><br><span class="line">        self.conv2 = tfk.layers.Conv2D(filters=InputChannel, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), strides=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">&quot;SAME&quot;</span>)</span><br><span class="line">        self.LeakyReLU2 = tfk.layers.LeakyReLU()</span><br><span class="line">        self.Batch3 = tfk.layers.BatchNormalization(momentum=<span class="number">0.99</span>, epsilon= <span class="number">0.001</span>)</span><br><span class="line">        self.conv3 = tfk.layers.Conv2D(filters=OutputChannel, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), strides=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.LeakyReLU3 = tfk.layers.LeakyReLU()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, Input</span>):</span><br><span class="line">        Skip = Input</span><br><span class="line">        Z = Input</span><br><span class="line">        Z = self.conv1(Z)</span><br><span class="line">        Z = self.Batch1(Z)</span><br><span class="line">        Z = self.LeakyReLU1(Z)</span><br><span class="line">        Z = self.conv2(Z)</span><br><span class="line">        Z = self.Batch2(Z)</span><br><span class="line">        Z = self.LeakyReLU2(Z)  </span><br><span class="line">        Z = self.conv3(Z)</span><br><span class="line">        Z = self.Batch3(Z)</span><br><span class="line">        Z = self.LeakyReLU3(Z)</span><br><span class="line">        <span class="comment"># Z : 256</span></span><br><span class="line">        <span class="keyword">return</span> Z + Skip</span><br></pre></td></tr></table></figure>
<p>우선 이 또한 정말이지 Model Subclassing을 그지같이 사용한 예시중 하나이다. 부디 독자들은 이따구로 구현할거면 그냥 Functional API를 사용하기 바란다.</p>
<p>이쯤되면 이런 질문이 나올 것이다. </p>
<blockquote>
<p>Q: 왜 저게 그지같이 구현한 예시인가요?<br>A: 여러 이유가 있지만, 가장 큰 이유는 굳이 모델(Weight)의 정의와 호출을 분리할 이유가 전혀 없는 구조이기 때문입니다.</p>
</blockquote>
<p>여기까지 잘 따라왔다면 이제 이 2개의 layer를 사용해서 ResNet50을 다음과 같이 구현할 수 있다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResNet50</span>(keras.Model):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># Input Shape 224*224*3</span></span><br><span class="line">        <span class="comment"># Conv 1 Block</span></span><br><span class="line">        self.ZeroPadding1 = keras.layers.ZeroPadding2D(padding=(<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">        self.Conv1 = keras.layers.Conv2D(filters = <span class="number">64</span>, kernel_size=(<span class="number">7</span>, <span class="number">7</span>), strides=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        self.Batch1 = keras.layers.BatchNormalization()</span><br><span class="line">        self.ReLU1 = keras.layers.LeakyReLU()</span><br><span class="line">        self.ZeroPadding2 = keras.layers.ZeroPadding2D(padding=(<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        self.MaxPool1 = keras.layers.MaxPool2D(pool_size=(<span class="number">3</span>, <span class="number">3</span>), strides=<span class="number">2</span>)</span><br><span class="line">        self.ResConvBlock1 = ResidualConvBlock(<span class="number">64</span>, <span class="number">256</span>, strides = (<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.ResIdentityBlock1 = ResidualIdentityBlock(<span class="number">64</span>, <span class="number">256</span>)</span><br><span class="line">        self.ResIdentityBlock2 = ResidualIdentityBlock(<span class="number">64</span>, <span class="number">256</span>)</span><br><span class="line"></span><br><span class="line">        self.ResConvBlock2 = ResidualConvBlock(<span class="number">128</span>, <span class="number">512</span>, strides = (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        self.ResIdentityBlock3 = ResidualIdentityBlock(<span class="number">128</span>, <span class="number">512</span>)</span><br><span class="line">        self.ResIdentityBlock4 = ResidualIdentityBlock(<span class="number">128</span>, <span class="number">512</span>)</span><br><span class="line">        self.ResIdentityBlock5 = ResidualIdentityBlock(<span class="number">128</span>, <span class="number">512</span>)</span><br><span class="line"></span><br><span class="line">        self.ResConvBlock3 = ResidualConvBlock(<span class="number">256</span>, <span class="number">1024</span>, strides = (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        self.ResIdentityBlock6 = ResidualIdentityBlock(<span class="number">256</span>, <span class="number">1024</span>)</span><br><span class="line">        self.ResIdentityBlock7 = ResidualIdentityBlock(<span class="number">256</span>, <span class="number">1024</span>)</span><br><span class="line">        self.ResIdentityBlock8 = ResidualIdentityBlock(<span class="number">256</span>, <span class="number">1024</span>)</span><br><span class="line">        self.ResIdentityBlock9 = ResidualIdentityBlock(<span class="number">256</span>, <span class="number">1024</span>)</span><br><span class="line">        self.ResIdentityBlock10 = ResidualIdentityBlock(<span class="number">256</span>, <span class="number">1024</span>)</span><br><span class="line"></span><br><span class="line">        self.ResConvBlock4 = ResidualConvBlock(<span class="number">512</span>, <span class="number">2048</span>, strides = (<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.ResIdentityBlock11 = ResidualIdentityBlock(<span class="number">512</span>, <span class="number">2048</span>)</span><br><span class="line">        self.ResIdentityBlock12 = ResidualIdentityBlock(<span class="number">512</span>, <span class="number">2048</span>)</span><br><span class="line">        </span><br><span class="line">        self.GAP = keras.layers.GlobalAveragePooling2D()</span><br><span class="line">        self.DenseOut = keras.layers.Dense(<span class="number">1000</span>, activation=<span class="string">&#x27;softmax&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, Input</span>):</span><br><span class="line">        X = self.ZeroPadding1(Input)</span><br><span class="line">        X = self.Conv1(X)</span><br><span class="line">        X = self.Batch1(X)</span><br><span class="line">        X = self.ReLU1(X)</span><br><span class="line">        X = self.ZeroPadding2(X)</span><br><span class="line"></span><br><span class="line">        X = self.MaxPool1(X)</span><br><span class="line">        X = self.ResConvBlock1(X)</span><br><span class="line">        X = self.ResIdentityBlock1(X)</span><br><span class="line">        X = self.ResIdentityBlock2(X)</span><br><span class="line"></span><br><span class="line">        X = self.ResConvBlock2(X)</span><br><span class="line">        X = self.ResIdentityBlock3(X)</span><br><span class="line">        X = self.ResIdentityBlock4(X)</span><br><span class="line">        X = self.ResIdentityBlock5(X)</span><br><span class="line"></span><br><span class="line">        X = self.ResConvBlock3(X)</span><br><span class="line">        X = self.ResIdentityBlock6(X)</span><br><span class="line">        X = self.ResIdentityBlock7(X)</span><br><span class="line">        X = self.ResIdentityBlock8(X)</span><br><span class="line">        X = self.ResIdentityBlock9(X)</span><br><span class="line">        X = self.ResIdentityBlock10(X)</span><br><span class="line"></span><br><span class="line">        X = self.ResConvBlock4(X)</span><br><span class="line">        X = self.ResIdentityBlock11(X)</span><br><span class="line">        X = self.ResIdentityBlock12(X)</span><br><span class="line"></span><br><span class="line">        X = self.GAP(X)</span><br><span class="line">        Out = self.DenseOut(X)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> Out</span><br></pre></td></tr></table></figure>
<p>여기서 하나 GAP로 정의된 Global Average Pooling layer가 있다. 이것에 대해서 간단하게만 알아보자.</p>
<p>이 layer는 단순하게 말하자면 feaeture map을 1차원을 만들어 주는 layer이다. 대개, Image는 3차원인데, 차원별로 존재하는 image를 하나의 Scalar 값으로 만든다는 뜻이다. (Global Pooling) 그때, Scalar 값으로 만드는 과정에서 이미지의 각 픽셀 값을 평균을 내는 방법을 취한 것 뿐이다. (Average)</p>
<p>이를 간단히 그림으로 표현하자면 다음과 같다.</p>
<p align="center"><img src="http://kimh060612.github.io/img/GAP.jpg" width="100%" height="100%"></p>

<p>그림에서 보여지는 것과 같이, 각 채널에 있는 이미지들의 픽셀값을 평균을 내서 그것을 모으면 채널의 개수 만큼의 크기를 가지는 1-dimensional vector가 완성된다.</p>
<p>여기까지 Convolutional Neural Network의 구현 실습을 마치도록 하겠다. 부디 도움이 되었….을까?는 모르겠지만 재밌게 보았으면 좋겠다.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kimh060612.github.io/2022/03/05/CNN-A/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Michael Kim">
      <meta itemprop="description" content="Backend Engineering, Deep learning, Algorithm & DS, CS 전공 지식 공부 저장소">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Michael's Study Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/05/CNN-A/" class="post-title-link" itemprop="url">Convoltional Neural Network 강의 내용 part.A</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-03-05 19:26:40" itemprop="dateCreated datePublished" datetime="2022-03-05T19:26:40+09:00">2022-03-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-05-30 00:00:32" itemprop="dateModified" datetime="2022-05-30T00:00:32+09:00">2022-05-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2022/03/05/CNN-A/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2022/03/05/CNN-A/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><em>본 포스트는 Hands-on Machine learning 2nd Edition, CS231n, Tensorflow 공식 document를 참조하여 작성되었음을 알립니다.</em></p>
<h3 id="Index"><a href="#Index" class="headerlink" title="Index"></a>Index</h3><ol>
<li>Introduction of Convolution Operation  </li>
<li>Definition of Convolutional Neural Network(CNN)</li>
<li>Back Propagation of CNN</li>
<li>Partice</li>
</ol>
<p>여기서는 1~3는 Part. A이고 4은 Part. B에서 다루도록 하겠다.<br><br></p>
<h3 id="Introduction-of-Convolution-Operation"><a href="#Introduction-of-Convolution-Operation" class="headerlink" title="Introduction of Convolution Operation"></a>Introduction of Convolution Operation</h3><hr>
<p>Convolutional Neural Network는 Convolution 연산을 Neural Network에 적용한 것이다. 따라서 이를 알기 위해서는 Convolution 연산을 먼저 알아야할 필요가 있다. 관련 학과 대학생이라면 아마도 신호와 시스템을 배우면서 이를 처음 접했을 것이다. Continuous domain, Discrete domian까지 이 연산을 정의될 수 있고 각각에 따라 계산 방법 또한 배웠을 것이다. 예를 들어서 2차원의 Image와 2차원의 Filter의 Convolution 연산을 수식으로 표현해 보도록 하겠다.</p>
<ul>
<li>Image 행렬 정의</li>
</ul>
<script type="math/tex; mode=display">
I(i, j)</script><p>Image의 $i$열, $j$행의 성분 </p>
<ul>
<li>Filter 행렬 정의</li>
</ul>
<script type="math/tex; mode=display">
K(i, j)</script><p>Filter의 $i$열, $j$행의 성분. 높이를 $k_1$, 너비를 $k_2$라고 가정.</p>
<ul>
<li>$I$와 $K$의 Convolution 연산</li>
</ul>
<script type="math/tex; mode=display">
(I*K)_{ij} = \sum_{m = 0}^{k_1 - 1} \sum_{n = 0}^{k_2 - 1}I(i - m, j - n)K(m, n)
\tag{equation (1)}</script><p>위 정의를 조금 틀면 다음과 같이도 표현이 가능하다.</p>
<script type="math/tex; mode=display">
(I*K)_{ij} = \sum_{m = 0}^{k_1 - 1} \sum_{n = 0}^{k_2 - 1}I(i + m, j + n)K(-m, -n)
\tag{equation (2)}</script><p>위와 같은 연산의 형태를 Correlation이라고 한다. 즉, Convolution 연산의 Filter를 $\pi$만큼 회전시킨다면 그것이 Correlation 연산인 것이다. 이는 아주 중요한 관계이므로 꼭 기억해 두도록 하자.</p>
<h3 id="Definition-of-Convolutional-Neural-Network-CNN"><a href="#Definition-of-Convolutional-Neural-Network-CNN" class="headerlink" title="Definition of Convolutional Neural Network(CNN)"></a>Definition of Convolutional Neural Network(CNN)</h3><hr>
<p>CNN의 정의는 위의 Convolution 연산을 사용하여 Weight와 Input을 계산하는 것이다. 매우 간단한 예시로 LeNet이라는 것을 보자. 너무 자주 나오는 예시라서 하품이 나올 것 같지만, 본래 기본이라는 것은 “쉬운”것이 아니라 “중요한”것이다.</p>
<p align="center"><img src="https://kimh060612.github.io/img/LeNet.png" width="100%"></p>

<p>그림에서의 각 파트를 분해해서 살펴보면 다음과 같다.</p>
<blockquote>
<p>Image Input -&gt; (Convolution) -&gt; Feature Map -&gt; (Pooling) -&gt; Feature Map -&gt; (Convolution) -&gt; Feature Map -&gt; (Pooling) -&gt; Feature Map -&gt; (Flatten) -&gt; Feature Vector -&gt; (FCNN) -&gt; Feature Vector -&gt; (FCNN) -&gt; Feature Vector -&gt; (Gaussian Connection) -&gt; Output Vector</p>
</blockquote>
<p>여기서 괄호 안에 들어 있는 것이 연산의 이름이다. FCNN은 다른 포스트에서 봤다고 가정하고, 여기서 주목해야 할 것은 Pooling Layer이다. </p>
<p>Pooling은 다양한 종류가 있는데 간단하게 한가지만 소개하자면 Max Pooling이 있다. 자세한 것은 Tensorflow 2 <a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D">공식 문서</a>를 참조하는 것이 더 좋을 것 같다.<br>여기서는 Pooling까지 자세히 다룰 이유는 없는 것 같다.</p>
<p>이제 본격적으로 Convolution 연산에 대해서 알아보도록 하겠다. 그 전에 FCNN 포스트에서도 그랬듯이, 수식 표현을 하기 위한 정의부터 하고 시작하자.</p>
<blockquote>
<p><em>Definition 1</em></p>
</blockquote>
<ul>
<li>$u_{ijm}^l$: $l$번째 층의 Feature Map의 $m$번째 Channel $i$행, $j$열의 성분</li>
<li>$x_{ijm}$: Input의 $m$번째 Channel의 $i$행, $j$열의 성분</li>
<li>$w_{ijmk}^l$: $l$번째 층의 Weight의 $k$번째 Kernel Set에 $m$번째 Channel, $i$행, $j$열의 성분</li>
<li>$b_m^l$: $l$번째 층의 $m$번째 Channel의 Bias</li>
</ul>
<p>이를 통해서 Convolution Layer를 수학적으로 표현해 보자면 다음과 같다.</p>
<script type="math/tex; mode=display">
u_{ijm}^l = \sum^{H - 1}_{p = 0} \sum^{W - 1}_{q = 0} \sum^{K - 1}_{k = 0} z^{l - 1}_{i+sp,j+sq,k}w^l_{p,q,k,m} + b^l_m
\tag{equation (3)}</script><p>여기서 $z$행렬은 $u$행렬에 activation function을 씌워 놓은 것이라고 생각하면 편하다.</p>
<p>이를 그림으로 표현해보면 다음과 같다. </p>
<p align="center"><img src="https://kimh060612.github.io/img/CNN.png" width="100%"></p>

<p>위 수식에서는 아직 정의되지 않은 부분, 설명되지 않은 부분이 많다. 첫번째로 $H$, $W$, $K$의 의미, 그리고 index 부분의 $s$의 의미이다. 또한, 위의 연산은 Correlation인데 왜 Convolution 연산이라고 하는 것일까? </p>
<p>일단 첫번째는 $H$, $W$, $K$인데, 이는 각각 Weight의 높이, 너비, 채널수이다. 그리고 index 부분의 $s$는 Stride이다. Convolution 연산의 Weight를 옮겨가면서 곱셉을 할때 얼마나 옮길지를 결정한다. 이 값을 키울수록 결과 이미지의 크기가 작아진다. 자세한 것을 하나하나 까볼려면 오래 걸리니, 이 부분은 혼자서 잘 생각해 보는게 좋을 것 같다. 어디까지나 이 문서는 입문서가 아니라는 점을 알아주었으면 좋겠다. 기존에 Tensorflow/Pytorch만을 사용하던 사람들에게 이론을 제공하고자 함이다.</p>
<h3 id="Back-Propagation-of-CNN"><a href="#Back-Propagation-of-CNN" class="headerlink" title="Back Propagation of CNN"></a>Back Propagation of CNN</h3><hr>
<p>자. 본격적으로 어려운 부분이다. 앞으로 Deep learning 강의를 써내려가면서 이보다 어려운 부분은 없다. 그리고 필자가 생각하기에도 쓸모가 없다. 단지 지적 유희를 위해서 읽어주기를 바라며 틀린 부분이 있다면 지적해 주기를 바란다. </p>
<p>그 전에, 왜 필자는 굳이 이 파트를 써내려 가는가를 적어보도록 하겠다. (잡담이니 굳이 안 읽어도 상관 없다.) 최근의 Deep learning 개발은 Auto Grad 계열의 알고리즘들을 활용하여 앞먹임 연산만을 정의하면 알아서 역전파 수식이 계산되어 BackPropagation을 편리하게 할 수 있다. 하지만 라이브러리에 모든 것을 맡기고 개발만 하는 것이 과연 좋은 개발자/연구원 이라고 할 수 있을까? 필요하다면 더 깊은 인사이트를 얻어서 문제를 해결해야할 필요가 있다. 이 글은 그런 사람들을 위함이기도 하고 나처럼 학문 변태들을 위한 것이기도 하다. 그러니 이 파트가 필요 없다고 판단되면 읽지 않는 것을 추천하고, 만약 틀린 것이 있다면 부디 연락해서 알려주었으면 좋겠다. 환영하는 마음으로 받아들이고 수정하도록 하겠다.</p>
<p>사족이 길었는데, 그래서 Back Propagation이 어떻게 정의되는 것일까? 큰 틀은 FCNN과 다를 바가 없다. Weight를 업데이트함에 있어서 Chain Rule을 활용하는 것이다. 그렇다면 어떻게 그것을 진행할 것인가?<br>우선 첫번째로 미분부터 써내려 가보자. </p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial w^l_{ijmk}}
\tag{def 2}</script><p>이것을 구해서 Weight를 업데이트하는 것이 Back Propagation의 핵심이다. 그렇다면 FCNN과 똑같이 일반화된 Delta Rule을 활용해 보는 것으로 시작하자. 이를 위해서 chain rule을 적용해 보면 다음과 같이 표현할 수 있다. </p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial w^l_{ijmk}} = \sum_{p=0}^{H'}\sum_{q=0}^{W'} \frac{\partial E}{\partial u^l_{pqm}}\frac{\partial u^l_{pqm}}{\partial w^l_{ijmk}}
\tag{equation (4)}</script><p>이때, $H’$, $W’$는 Convolution output의 결과 Feature map의 높이와 너비이다.<br>FCNN때와 똑같이 한다면 다음과 같이 Delta를 정의하고 식을 수정할 수 있다. </p>
<script type="math/tex; mode=display">
\delta_{pqm}^l = \frac{\partial E}{\partial u^l_{pqm}}
\tag{def 3}</script><p>그리고 다음과 같이 식을 유도하는 것이 가능하다. </p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial w^l_{ijmk}} = \sum_{p=0}^{H'}\sum_{q=0}^{W'} \delta_{pqm}^l  \frac{\partial u^l_{pqm}}{\partial w^l_{ijmk}} =  \sum_{p=0}^{H'}\sum_{q=0}^{W'} \delta_{pqm}^l z^{l - 1}_{i+sp, j+sq, k}
\tag{equation (5)}</script><p>결국 다음과 같이 표현 가능하다.</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial w^l_{ijmk}} = (\delta_{m}^l * z^{l-1}_k)_{ij}
\tag{equation (6)}</script><p>이제 delta를 정의했으니, 앞층의 delta와 뒷층의 delta간의 관계를 밝혀내서 연산을 효율화 하면 된다. 이 과정을 손으로 유도하는 것은 필자가 생각해도 실용적 측면에서는 정말 쓸데가 없다. 왜냐하면 현대의 신경망 구조는 너무 복잡해져서 이걸 유도했다 쳐도 다른 구조들이 정말 많이 때문에 써먹을 수가 없기 때문이다. 하지만 아주 제한적인 경우에 대해서 이걸 유도해 보도록 하겠다. </p>
<ol>
<li>Convolution - Convolution layer 에서의 Delta 점화식</li>
</ol>
<p>우선 다시 한번 delta에서 chain rule을 적용해 보도록 하겠다 . 이 과정은 FCNN에서도 했을 것이다. 따라서 최대한 간결하게 진행해 보도록 하겠다.  </p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial u^l_{pqm}} = \sum_{x=0}^{H''}\sum_{y=0}^{W''} \sum_{c=0}^{C''}  \frac{\partial E}{\partial u^{l + 1}_{p - sx,q - sy, c}} \frac{\partial u^{l + 1}_{p - sx,q - sy, c}}{\partial u^l_{pqm}}
\tag{equation (7)}</script><p>이때 $H’’$, $W’’$, $C’’$는 다음 층에서의 Feature Map의 크기이다.</p>
<p>이를 전개해 보면 다음과 같다.</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial u^l_{pqm}} = \sum_{x=0}^{H''}\sum_{y=0}^{W''} \sum_{c=0}^{C''} \delta_{p-sx,q-sy,c}^{l+1} w^{l+1}_{xycm} f'(u^{l}_{pqm})
\tag{equation (8)}</script><p>이는 다음과 같이 Convolution 연산으로 표현될 수 있다.</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial u^l_{pqm}} = (\delta^{l+1} * w_m^{l+1}) \odot f'(u^l_m)
\tag{equation (9)}</script><p>위의 $\odot$은 성분 끼리의 곱(element wise multiplication)을 의미한다. </p>
<ol>
<li>Convolution - Pooling - Convolution 에서의 Delta 점화식</li>
</ol>
<p>위에서 delta를 유도함에 있어서 한 층이 더 추가될 뿐이다. 다음과 같이 말이다. 여기서는 Max Pooling, Average Pooling을 예로 들어보겠다. </p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial u^l_{pqm}} = \sum_{x=0}^{H''}\sum_{y=0}^{W''} \sum_{c=0}^{C''}  \frac{\partial E}{\partial u^{l + 2}_{p' - sx,q' - sy, c}} \frac{\partial u^{l + 2}_{p' - sx,q' - sy, c}}{\partial u^{l + 1}_{p'q'm}} \frac{u^{l + 1}_{p'q'm}}{u^l_{pqm}}
\tag{equation (10)}</script><script type="math/tex; mode=display">
\frac{\partial E}{\partial u^l_{pqm}} = \sum_{x=0}^{H''}\sum_{y=0}^{W''} \sum_{c=0}^{C''}  \delta_{p-sx,q-sy,c}^{l+1} w^{l+1}_{xycm} \frac{\partial u^{l + 2}_{p' - sx,q' - sy, c}}{\partial u^{l + 1}_{p'q'm}} f'(u^{l}_{pqm})
\tag{equation (11)}</script><p>위 식에서 중간에 있는 $l+1$층이 Pooling 층이다. 이 미분은 다음과 같이 정의된다. </p>
<ul>
<li>Max Pooling의 경우</li>
</ul>
<script type="math/tex; mode=display">
\frac{\partial u^{l + 2}_{p' - sx,q' - sy, c}}{\partial u^{l + 1}_{p'q'm}} = 
\begin{cases}
    1 \space \space \space \space \space \text{if p, q 성분이 최댓값이었을 경우} \\
    0 \space \space \space \space \space \text{otherwise}
\end{cases} 
\tag{equation (12)}</script><ul>
<li>Average Pooling의 경우</li>
</ul>
<script type="math/tex; mode=display">
\frac{\partial u^{l + 2}_{p' - sx,q' - sy, c}}{\partial u^{l + 1}_{p'q'm}} = \frac{1}{H''' * W'''} 
\tag{equation (12)}</script><p>여기서 $H’’’$,$W’’’$는 Pooling Layer의 크기이다.</p>
<p>결국 Pooling layer까지 포함하면 다음과 같이 convolution 연산으로 정의할 수 있다.</p>
<script type="math/tex; mode=display">
\delta_{pqm}^l = \text{Upsampling}[(\delta^{l + 2} * w^{l + 2}_m)] \odot f'(u^l_m)
\tag{equation (13)}</script><p>어디까지나 이렇게 연산을 할 수 있는 이유는 Pooling layer는 업데이트를 할 필요가 없기 때문이다. 만약 업데이트를 할 피라미터가 있다면 “제대로” 다시 delta rule의 방정식을 수정해 줘야 한다.</p>
<ol>
<li>이게 정말 쓸데 없는 이유</li>
</ol>
<p>현대의 신경망은 에시당초 Convolution layer에서 탈각하는 분위기 인데다가 Convolution - Feed Forward 관계나 ResNet같은 현대의 신경망 구조에서는 이런 복잡한 수식으로 구현하는 것은 사실상 불가능에 가깝다. 그러니 우리는 Autograd를 믿고 이런건 그냥 지적 유희로만 알아 두도록 하자.</p>
<p>그리고 마지막으로 Bias의 Update 방법을 알아보면 다음과 같다.</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial b_m^l} = \sum^{H_o - 1}_{p = 0} \sum^{W_o - 1}_{q = 0} \frac{\partial E}{\partial u_{pqm}^l} \frac{\partial u_{pqm}^l}{\partial b_m^l} 
\tag{equation (14)}</script><p>이때, 곱셈 term의 뒷 항은 전부 1이므로 다음과 같은 식이 성립한다.</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial b_m^l} = \sum^{H' - 1}_{p = 0} \sum^{W' - 1}_{q = 0} \delta_{pqm}^l
\tag{equation (15)}</script><p>이걸로 CNN의 이론 부분은 끝났다. 다음에는 실습 부분으로 찾아오도록 하겠다.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kimh060612.github.io/2022/03/05/FCNN-B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Michael Kim">
      <meta itemprop="description" content="Backend Engineering, Deep learning, Algorithm & DS, CS 전공 지식 공부 저장소">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Michael's Study Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/05/FCNN-B/" class="post-title-link" itemprop="url">Fully Connected Neural Network 강의 내용 part.B</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-03-05 17:33:29" itemprop="dateCreated datePublished" datetime="2022-03-05T17:33:29+09:00">2022-03-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-05-30 00:00:32" itemprop="dateModified" datetime="2022-05-30T00:00:32+09:00">2022-05-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2022/03/05/FCNN-B/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2022/03/05/FCNN-B/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><em>본 포스트는 Hands-on Machine learning 2nd Edition, CS231n, Tensorflow 공식 document를 참조하여 작성되었음을 알립니다.</em></p>
<h6 id="Index"><a href="#Index" class="headerlink" title="Index"></a>Index</h6><ol>
<li><del>Basic of Neural Network</del>  </li>
<li><del>Definition of Fully Connected Neural Network(FCNN)</del></li>
<li><del>Feed Forward of FCNN</del></li>
<li><del>Gradient Descent</del></li>
<li><del>Back Propagation of FCNN</del></li>
<li>Partice(+ 부록 Hyper parameter tuning)</li>
</ol>
<p><br></p>
<h4 id="6-Partice-부록-Hyper-parameter-tuning"><a href="#6-Partice-부록-Hyper-parameter-tuning" class="headerlink" title="6. Partice(+ 부록 Hyper parameter tuning)"></a>6. Partice(+ 부록 Hyper parameter tuning)</h4><hr>
<p>자 실습 시간이다. 왜 실습을 Part. B로 뺐느냐? FCNN이 뭐 할게 있다고?<br>뭐 할게 있겠다. Tensorflow 2가 대충 어떻게 이루어 졌는지 설명하기 위해 분량 조절을 위해서 뺀것이다.<br>무엇보다 Part. A 쓰는데 수식을 너무 많이 써서 힘들어서 분리했다. <del>Tlqkf</del><br>자, 우선 실습에 들어가기에 앞서, TF 2를 애정하는 나로써는 앞으로 이 스터디 포스트에 작성될 대부분의 소스코드를 꿰뚫는 구현 체계를 먼저 설명하고 넘어가겠다.<br>다음 사진을 보자.</p>
<p align="center"><img src="http://kimh060612.github.io/img/API.png" width="70%" height="70%"></p>

<p><em>출처: pyimagesearch blog: <a target="_blank" rel="noopener" href="https://www.pyimagesearch.com/2019/10/28/3-ways-to-create-a-keras-model-with-tensorflow-2-0-sequential-functional-and-model-subclassing/">링크</a></em></p>
<p>위 그림에서 필자는 대부분의 코드를 <strong>Model Subclassing</strong> 방식으로 구현할 것이다. 구현 하면서 설명할 터이니 잘 따라와 주기를 바란다.<br>여기서부터는 대학교 강의 수준의 <strong>객체지향프로그래밍</strong> 지식을 갖추지 않으면 읽기 힘들 수 있다. “상속”, “오버라이딩”의 개념이라도 살펴보고 오자.</p>
<h6 id="6-1-Model-Subclassing"><a href="#6-1-Model-Subclassing" class="headerlink" title="6-1. Model Subclassing"></a>6-1. Model Subclassing</h6><hr>
<p>우선 준비한 소스부터 보고 시작하자.<br><em>file: model/FCNN1.py</em><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FCNN</span>(keras.Model):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, _units = [<span class="number">56</span>, <span class="number">56</span>], _activation = [<span class="string">&#x27;relu&#x27;</span>, <span class="string">&#x27;relu&#x27;</span>], **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)</span><br><span class="line">        self.Hidden1 = keras.layers.Dense(units=_units[<span class="number">0</span>], activation=_activation[<span class="number">0</span>], kernel_initializer=<span class="string">&quot;normal&quot;</span>)</span><br><span class="line">        self.Hidden2 = keras.layers.Dense(units=_units[<span class="number">1</span>], activation=_activation[<span class="number">1</span>], kernel_initializer=<span class="string">&quot;normal&quot;</span>)</span><br><span class="line">        self._output = keras.layers.Dense(units=<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, Input</span>):</span><br><span class="line">        hidden1 = self.Hidden1(Input)</span><br><span class="line">        hidden2 = self.Hidden2(hidden1)</span><br><span class="line">        Output = self._output(hidden2)</span><br><span class="line">        <span class="keyword">return</span> Output</span><br></pre></td></tr></table></figure><br>자, 별거 없다. keras를 써봤다면 뭔지 바로 감이 올 것이다.<br>이 부분에 대해서는 함수에 대한 설명보다는 class에 대한 설명을 해야할 것 같다. 바로 FCNN class가 상속을 받은 부모 클래스인 keras.Model 클래스에 관해서이다.</p>
<p>keras.Model class는 케라스에서 Deep learning을 진행하는 모델을 정의해주는 class이다. 우리가 이미 존재하는 layer를 가져다가 특정 순서로 연산을 진행하는 graph를 만들어 내기 위한 class이다. 하지만 그렇게 어렵게 생각하지 말자. 사용하는 것을 보면 바로 답이 나온다.</p>
<p>여기서는 생성자와 call이라는 함수를 오버라이딩을 통해서 사용자가 재 정의를 해서 사용한다. call은 우리가 구현하고자 하는 model이 feed forward를 진행할때 호출되는 함수이다. 생성자는 사용할 폭이 넓다. 여기서는 model을 구성하는 layer를 정의하는데 사용하였는데, 꼭 그 역할만 할 필요는 없는 것이다.</p>
<p>가타부타 말이 많았는데, 실제 어떻게 동작을 시키는가?</p>
<p><em>file: train_MNIST.py</em><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> model.FCNN1 <span class="keyword">import</span> FCNN</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">mnist = keras.datasets.mnist</span><br><span class="line"></span><br><span class="line">(train_img, train_labels), (test_img, test_labels) = mnist.load_data()</span><br><span class="line"></span><br><span class="line">train_img, test_img = train_img.reshape([-<span class="number">1</span>, <span class="number">784</span>]), test_img.reshape([-<span class="number">1</span>, <span class="number">784</span>])</span><br><span class="line"></span><br><span class="line">train_img = train_img.astype(np.float32) / <span class="number">255.</span></span><br><span class="line">test_img = test_img.astype(np.float32) / <span class="number">255.</span></span><br><span class="line"></span><br><span class="line">model = FCNN()</span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adam&#x27;</span>, loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>, metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br><span class="line">model.fit(train_img, train_labels, batch_size = <span class="number">32</span>, epochs = <span class="number">15</span>, verbose = <span class="number">1</span>, validation_split = <span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line">model.evaluate(test_img, test_labels, verbose=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><br>간단하다. keras.Model class를 상속 받았으니, 그곳에 있는 기본 함수들을 모두 사용할 수있다. fit method로 학습을 진행하고 evalute로 test데이터로 모델을 평가한다.<br>이는 기존에 keras의 사용법과 별반 다른게 없다.</p>
<p>여기까지는 쉽다. 하지만 이러고 끝낼거면 시작도 하지 않았다.</p>
<p>다음을 진짜 자세하게 설명할 것이다.</p>
<p><em>file: model/layer.py</em><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line"><span class="comment"># Weight Sum을 진행하는 Layer를 정의하는 예제 1</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">WeightSum</span>(keras.layers.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, units=<span class="number">32</span>, input_dim=<span class="number">32</span>, trainable=<span class="literal">True</span>, name=<span class="literal">None</span>, dtype=<span class="literal">None</span>, dynamic=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(trainable=trainable, name=name, dtype=dtype, dynamic=dynamic, **kwargs)</span><br><span class="line">        <span class="comment"># tf.Variable을 활용하는 예시</span></span><br><span class="line">        <span class="comment"># 가중치 행렬을 초기화 하기 위한 객체</span></span><br><span class="line">        w_init = tf.random_normal_initializer()</span><br><span class="line">        <span class="comment"># 가중치 행렬을 tf.Variable로 정의함. 당연히 학습해야하니 training parameter를 true로 둠.</span></span><br><span class="line">        self.Weight = tf.Variable(</span><br><span class="line">            initial_value = w_init(shape=(input_dim, units), dtype=<span class="string">&quot;float32&quot;</span>),</span><br><span class="line">            trainable = <span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line">        b_init = tf.zeros_initializer()</span><br><span class="line">        <span class="comment"># 위랑 같음. 별반 다를거 X</span></span><br><span class="line">        self.Bias = tf.Variable(</span><br><span class="line">            initial_value = b_init(shape=(units, ), dtype=<span class="string">&quot;float32&quot;</span>),</span><br><span class="line">            trainable = <span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># add_weight를 활용하는 예시 - Short Cut</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        # 이는 keras.layers.Layer의 method 중에서 add_weight를 사용하는 방법임. </span></span><br><span class="line"><span class="string">        # 주로 training을 시키기 위한 행렬을 이렇게 선언해서 나중에 편하게 불러오기 위한 목적이 큼. </span></span><br><span class="line"><span class="string">        self.Weight = self.add_weight(</span></span><br><span class="line"><span class="string">            shape=(input_dim, units), initializer=&quot;random_normal&quot;, trainable=True</span></span><br><span class="line"><span class="string">        )</span></span><br><span class="line"><span class="string">        self.Bias = self.add_weight(shape=(units,), initializer=&quot;zeros&quot;, trainable=True)</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, Input</span>):</span><br><span class="line">        <span class="comment"># 행렬 곱을 위한 tf 함수임. 별거 없음</span></span><br><span class="line">        <span class="comment"># 그냥 U = WZ + B 구현한거. </span></span><br><span class="line">        <span class="keyword">return</span> tf.matmul(Input, self.Weight) + self.Bias</span><br><span class="line"></span><br><span class="line"><span class="comment"># Weight Sum을 진행하는 Layer를 정의하는 예제 2</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">WeightSumBuild</span>(keras.layers.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, _units = <span class="number">32</span>, trainable=<span class="literal">True</span>, name=<span class="literal">None</span>, dtype=<span class="literal">None</span>, dynamic=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(trainable=trainable, name=name, dtype=dtype, dynamic=dynamic, **kwargs)</span><br><span class="line">        self.units = _units</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build</span>(<span class="params">self, input_dim</span>):</span><br><span class="line">        <span class="comment"># 이 함수는 밑에서 자세히 설명함.</span></span><br><span class="line">        self.Weight = self.add_weight(</span><br><span class="line">            shape=(input_dim[-<span class="number">1</span>], self.units),</span><br><span class="line">            initializer = <span class="string">&quot;random_normal&quot;</span>,</span><br><span class="line">            trainable= <span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.Bias = self.add_weight(</span><br><span class="line">            shape = (self.units,),</span><br><span class="line">            initializer = <span class="string">&quot;random_normal&quot;</span>,</span><br><span class="line">            trainable = <span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, Input</span>):</span><br><span class="line">        <span class="comment"># 상기 동일.</span></span><br><span class="line">        <span class="keyword">return</span> tf.matmul(Input, self.Weight) + self.Bias</span><br></pre></td></tr></table></figure><br>위의 2개의 class는 하는 짓거리가 완벽하게 똑같다. 하지만 하는 짓거리는 같은데 아주 치명적인 부분이 조금 다르다. 바로 build 함수의 overwritting이다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#                입력 벡터/텐서의 크기를 받음</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build</span>(<span class="params">self, input_dim</span>):</span><br><span class="line">    <span class="comment"># 그에 따라서 Weight와 Bias의 차원을 결정함.</span></span><br><span class="line">    <span class="comment"># 물론, Bias는 차이가 없을 지언정, Weight는 크게 차이가 나게 된다.</span></span><br><span class="line">    self.Weight = self.add_weight(</span><br><span class="line">        shape=(input_dim[-<span class="number">1</span>], self.units),</span><br><span class="line">        initializer = <span class="string">&quot;random_normal&quot;</span>,</span><br><span class="line">        trainable= <span class="literal">True</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    self.Bias = self.add_weight(</span><br><span class="line">        shape = (self.units,),</span><br><span class="line">        initializer = <span class="string">&quot;random_normal&quot;</span>,</span><br><span class="line">        trainable = <span class="literal">True</span></span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>이 함수는 call이 호출되기 전에 무조건 실행되는 함수라고 생각하면 된다. 즉, 호출하기 전에 Weight를 정의하는 것이다. 즉, 입력 벡터의 크기에 따라 모델의 형태가 알아서 바꾸게 해줄 수 있는 것이다. 이는 Image를 처리할때 이점이 될 수 있다.<br>Image를 학습시킬때, 이러한 처리가 없으면 이미지를 전부 동일한 크기로 만들어 주어야 한다. 하지만 build 함수를 정의해서 그때 그때 입력 벡터/텐서에 따라 커널을 수정해 주면 굳이 그럴 필요가 없다. 전처리 비용이 줄어드는 것이다. </p>
<p>그리고, 이제 이를 학습시키기 위한 코드를 보도록 하자.</p>
<p><em>file: train_MNIST_2.py</em><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> model.FCNN2 <span class="keyword">import</span> FCNN</span><br><span class="line"></span><br><span class="line">EPOCHS = <span class="number">15</span></span><br><span class="line">LR = <span class="number">0.01</span></span><br><span class="line">BatchSize = <span class="number">32</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 이미지 불러오기</span></span><br><span class="line">mnist = keras.datasets.mnist</span><br><span class="line">(train_img, train_labels), (test_img, test_labels) = mnist.load_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 필요한 전처리</span></span><br><span class="line">train_img, test_img = train_img.reshape([-<span class="number">1</span>, <span class="number">784</span>]), test_img.reshape([-<span class="number">1</span>, <span class="number">784</span>])</span><br><span class="line">train_img = train_img.astype(np.float32) / <span class="number">255.</span></span><br><span class="line">test_img = test_img.astype(np.float32) / <span class="number">255.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Train-Validation Split</span></span><br><span class="line">validation_img = train_img[-<span class="number">18000</span>:]</span><br><span class="line">validation_label = train_labels[-<span class="number">18000</span>:]</span><br><span class="line">train_img = train_img[:-<span class="number">18000</span>]</span><br><span class="line">train_labels = train_labels[:-<span class="number">18000</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train Data의 규합. &amp; Batch 별로 쪼갬</span></span><br><span class="line">train_dataset = tf.data.Dataset.from_tensor_slices((train_img, train_labels))</span><br><span class="line">train_dataset = train_dataset.shuffle(buffer_size=<span class="number">1024</span>).batch(BatchSize)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Validation Data의 규합 &amp; Batch 별로 쪼갬</span></span><br><span class="line">validation_dataset = tf.data.Dataset.from_tensor_slices((validation_img, validation_label))</span><br><span class="line">validation_dataset = validation_dataset.batch(BatchSize)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Optimizer &amp; Loss Function 정의</span></span><br><span class="line">optimizer = keras.optimizers.Adam(learning_rate=LR)</span><br><span class="line">loss_function = keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 학습이 잘 되고 있나 확인하기 위한 지표를 확인하기 위함.</span></span><br><span class="line">train_accuracy = keras.metrics.SparseCategoricalAccuracy()</span><br><span class="line">val_acc_metric = keras.metrics.SparseCategoricalAccuracy()</span><br><span class="line"></span><br><span class="line">model = FCNN()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Custom Training을 위한 반복문</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCHS):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Epoch %d start&quot;</span>%epoch)</span><br><span class="line">    <span class="keyword">for</span> step, (x_batch, y_batch) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_dataset):</span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">            <span class="comment"># Model의 Feed Forward</span></span><br><span class="line">            logits = model(x_batch, training=<span class="literal">True</span>)</span><br><span class="line">            <span class="comment"># Feed Forward 결과를 바탕으로 Loss를 구함</span></span><br><span class="line">            loss_val = loss_function(y_batch, logits)</span><br><span class="line">        <span class="comment"># 위의 과정을 바탕으로 gradient를 구함</span></span><br><span class="line">        grad = tape.gradient(loss_val, model.trainable_weights)</span><br><span class="line">        <span class="comment"># Optimizer를 통해서 Training Variables를 업데이트</span></span><br><span class="line">        optimizer.apply_gradients(<span class="built_in">zip</span>(grad, model.trainable_weights))</span><br><span class="line">        <span class="comment"># Batch 별로 Training dataset에 대한 정확도를 구함.</span></span><br><span class="line">        train_accuracy.update_state(y_batch, logits)</span><br><span class="line">        <span class="keyword">if</span> step % <span class="number">500</span> == <span class="number">0</span> :</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Training loss at step %d: %.4f&quot;</span>%(step, loss_val))</span><br><span class="line">    <span class="comment"># 정확도를 규합해서 출력</span></span><br><span class="line">    train_acc = train_accuracy.result()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Training acc over epoch: %.4f&quot;</span> % (<span class="built_in">float</span>(train_acc),))</span><br><span class="line">    train_accuracy.reset_states()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Validation을 진행함.</span></span><br><span class="line">    <span class="keyword">for</span> x_batch_val, y_batch_val <span class="keyword">in</span> validation_dataset:</span><br><span class="line">        <span class="comment"># Validation을 위한 Feed Forward</span></span><br><span class="line">        val_logits = model(x_batch_val, training = <span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># Batch 별로 Validation dataset에 대한 정확도를 구함</span></span><br><span class="line">        val_acc_metric.update_state(y_batch_val, val_logits)</span><br><span class="line">    <span class="comment"># 구한 정확도를 규합하여 출력함.</span></span><br><span class="line">    val_acc = val_acc_metric.result()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Validation acc: %.4f&quot;</span> % (<span class="built_in">float</span>(val_acc),))</span><br><span class="line">    val_acc_metric.reset_states()</span><br></pre></td></tr></table></figure><br>이는 TF2에서 추가된 tf.GradientTape를 통해서 사용자 정의 학습 루프를 만든 것이다. 각 줄마다 주석을 달아 놓았으니 Part. A의 내용을 숙지했다면 그렇게 어렵지 않게 알아들을 수 있을 것이다. </p>
<p>이제, 한가지 의문이 든다. 그렇다면 위의 Hyper parameter들을 변화시켜가면서 model을 최적화 하려면 노가다 밖에 답이없는건가? 답은 아니다. 이번에는 맛보기만 보여줄 것이다. 이는 scikit learn의 RandomizedSearchCV를 통해서 확인할 수 있다.</p>
<p><em>file: RandomSearch.py</em><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> reciprocal</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> RandomizedSearchCV</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">mnist = keras.datasets.mnist</span><br><span class="line">(train_img, train_labels), (test_img, test_labels) = mnist.load_data()</span><br><span class="line">train_img, test_img = train_img.reshape([-<span class="number">1</span>, <span class="number">784</span>]), test_img.reshape([-<span class="number">1</span>, <span class="number">784</span>])</span><br><span class="line">train_img = train_img.astype(np.float32) / <span class="number">255.</span></span><br><span class="line">test_img = test_img.astype(np.float32) / <span class="number">255.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 여기 parameter들의 Key는 무적권 입력 함수의 parameter와 같아야 한다. 아마도 **kwargs로 한번에 보내버리는 것일거다.</span></span><br><span class="line">param_distribution = &#123;</span><br><span class="line">    <span class="string">&quot;n_hidden&quot;</span>: [<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],</span><br><span class="line">    <span class="string">&quot;n_neurons&quot;</span>: np.arange(<span class="number">1</span>,<span class="number">100</span>),</span><br><span class="line">    <span class="string">&quot;lr&quot;</span>: reciprocal(<span class="number">3e-4</span>, <span class="number">3e-2</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 이것을 사용하기 위해서는 모델을 만들어줄 함수가 하나 필요하다.</span></span><br><span class="line"><span class="comment"># 여기서는 그냥 Sequential API를 사용하였다. 생각하기 귀찮았다. ㅋ;; </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Build_model</span>(<span class="params">n_hidden = <span class="number">1</span>, n_neurons=<span class="number">30</span>, lr = <span class="number">3e-3</span>, input_shape=[<span class="number">784</span>]</span>):</span><br><span class="line">    model = keras.models.Sequential()</span><br><span class="line">    model.add(keras.layers.InputLayer(input_shape=input_shape))</span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="built_in">range</span>(n_hidden):</span><br><span class="line">        model.add(keras.layers.Dense(n_neurons, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">    model.add(keras.layers.Dense(units=<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line">    optimizer = keras.optimizers.SGD(learning_rate=lr)</span><br><span class="line">    model.<span class="built_in">compile</span>(optimizer=optimizer, loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>, metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="comment"># 이는 keras 모델을 scikit learn에서 관리하기 위해 호출하는 함수이다.</span></span><br><span class="line">keras_classify = keras.wrappers.scikit_learn.KerasClassifier(Build_model)</span><br><span class="line"><span class="comment"># 여기서부터 본론이다. parameter_distribution으로 주어진 집합 한에서 가장 좋은 성능의 모델을 탐색한다.</span></span><br><span class="line"><span class="comment"># 여기서는 Cross-Validation을 사용한다. cv 항은 몇개로 Validation-training set을 분리할지 정하는 것이다. </span></span><br><span class="line">rnd_search_model = RandomizedSearchCV(keras_classify, param_distributions=param_distribution, n_iter = <span class="number">10</span>, cv = <span class="number">3</span>)</span><br><span class="line"><span class="comment"># 이제 주어진 데이터를 가지고 학습을 돌면서 최적의 모델을 탐색한다. </span></span><br><span class="line">rnd_search_model.fit(train_img, train_labels, epochs=<span class="number">10</span>, validation_data=(test_img,test_labels), callbacks=[keras.callbacks.EarlyStopping(patience=<span class="number">10</span>)])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(rnd_search_model.best_params_)</span><br></pre></td></tr></table></figure></p>
<p>오늘은 이것으로 끝내도록 하자.<br>다음 포스트는 Convolutional Neural Network를 오늘처럼 다뤄볼 예정이다.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Michael Kim</p>
  <div class="site-description" itemprop="description">Backend Engineering, Deep learning, Algorithm & DS, CS 전공 지식 공부 저장소</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">21</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">15</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/kimh060612" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;kimh060612" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:kimh060612@khu.ac.kr" title="E-Mail → mailto:kimh060612@khu.ac.kr" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Michael Kim</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://michael-blog-1.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>

</body>
</html>
