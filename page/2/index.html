<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
<meta name="google-site-verification" content="L0ZN3xykszl3pBQeJ5QFwk98KKJ8kHDzGSBLhEtwmNU" />
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"kimh060612.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Backend Engineering, Deep learning, Algorithm &amp; DS, CS ì „ê³µ ì§€ì‹ ê³µë¶€ ì €ì¥ì†Œ">
<meta property="og:type" content="website">
<meta property="og:title" content="Michael&#39;s Study Blog">
<meta property="og:url" content="https://kimh060612.github.io/page/2/index.html">
<meta property="og:site_name" content="Michael&#39;s Study Blog">
<meta property="og:description" content="Backend Engineering, Deep learning, Algorithm &amp; DS, CS ì „ê³µ ì§€ì‹ ê³µë¶€ ì €ì¥ì†Œ">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Michael Kim">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://kimh060612.github.io/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Michael's Study Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><link rel="alternate" href="/rss2.xml" title="Michael's Study Blog" type="application/rss+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Michael's Study Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Backend Engineering & Deep learning!</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>Tags<span class="badge">12</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>Categories<span class="badge">5</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>Archives<span class="badge">19</span></a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kimh060612.github.io/2022/03/05/Transformer-A/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Michael Kim">
      <meta itemprop="description" content="Backend Engineering, Deep learning, Algorithm & DS, CS ì „ê³µ ì§€ì‹ ê³µë¶€ ì €ì¥ì†Œ">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Michael's Study Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/05/Transformer-A/" class="post-title-link" itemprop="url">Transformer ê°•ì˜ ë‚´ìš© part.A</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-03-05 19:28:17" itemprop="dateCreated datePublished" datetime="2022-03-05T19:28:17+09:00">2022-03-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-05-30 00:00:32" itemprop="dateModified" datetime="2022-05-30T00:00:32+09:00">2022-05-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2022/03/05/Transformer-A/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2022/03/05/Transformer-A/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><em>ë³¸ í¬ìŠ¤íŠ¸ëŠ” í•´ë‹¹ ë§í¬ì˜ <a target="_blank" rel="noopener" href="https://wikidocs.net/22893">ë¸”ë¡œê·¸</a>ì„ ì°¸ì¡°í•˜ì—¬ ì‘ì„±ë˜ì—ˆìŒì„ ì•Œë¦½ë‹ˆë‹¤.</em></p>
<h3 id="Index"><a href="#Index" class="headerlink" title="Index"></a>Index</h3><ol>
<li>Attention!<ul>
<li>Seq2Seq</li>
<li>Attention in Seq2Seq</li>
</ul>
</li>
<li>Transformer Encoder<ul>
<li>Attention in Transformer</li>
<li>Multi-Head Attention</li>
<li>Masking</li>
<li>Feed Forward</li>
</ul>
</li>
<li>Transformer Decoder</li>
<li>Positional Encoding</li>
<li>Partice<ul>
<li>Seq2Seq Attention</li>
<li>Transformer</li>
</ul>
</li>
</ol>
<p>ì—¬ê¸°ì„œëŠ” 1ì€ Part. Aì´ê³  2~4ëŠ” Part. Bì—ì„œ, 5ëŠ” part. Cì—ì„œ ë‹¤ë£¨ë„ë¡ í•˜ê² ë‹¤.<br><br></p>
<h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention!"></a>Attention!</h3><hr>
<h4 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h4><p>ì~ ì£¼ëª©! ã…‹ã…‹ã…‹;;<br>ìš°ì„  ëª‡ëª‡ ì‚¬ëŒë“¤ì€ ì˜ì•„í•´ í•  ê²ƒì´ë‹¤. ìˆœì„œìƒìœ¼ë¡œëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê°€ëŠ” ê²ƒì´ êµ­ë£°ì´ê¸° ë•Œë¬¸ì´ë‹¤.</p>
<blockquote>
<p>RNN -&gt; LSTM -&gt; (GRU) -&gt; Seq2Seq -&gt; Attention</p>
</blockquote>
<p>í â€¦ ì¼ë‹¨ í•„ìê°€ ìƒê°í•˜ê¸°ì—ëŠ” LSTMê³¼ GRUëŠ” RNNì—ì„œ êµ¬ì¡°ë¥¼ ë°”ê¾¼ ê²ƒì´ë‹¤. ê·¸ë¦¬ê³  ì´ë“¤ë¼ë¦¬ëŠ” ì–¸ì œë‚˜ ì„œë¡œ ë°”ë€” ìˆ˜ ìˆëŠ”, ë§ˆì¹˜ ê¸°ê³„ ê³µì¥ì˜ ë‚˜ì‚¬ì™€ ê°™ì€ ì¡´ì¬ì´ê¸°ì—, ê¸°ë³¸ì´ ë˜ëŠ” RNNë§Œì„ ë‹¤ë£¨ê³  ë„˜ì–´ê°„ ê²ƒì´ë‹¤. ë¬¼ë¡ ! Gradient ì…ì¥ì—ì„œ ë§í•œë‹¤ë©´ í•  ë§ì€ ë§ë‹¤. ì´ë“¤ì´ ë‚˜ì˜¨ ì´ë¡ ì ì¸ í† ëŒ€ëŠ” ëª…í™•í•˜ì§€ë§Œ, êµ³ì´? ì´ê±¸? í•„ìì˜ ë¸”ë¡œê·¸ì—ì„œ? ë‹¤ë£¨ê¸°ì—ëŠ” ë„ˆë¬´ í”í•´ ë¹ ì§„ ë‚´ìš©ì´ë¼ ë°”ë¡œ Attention ë¶€í„° ì£ ì§€ê³  ê°€ë„ë¡ í•˜ê² ë‹¤. (ë¬¼ë¡  Attention ë˜í•œ ìš”ì¦˜ì€ í”í•´ ë¹ ì§„ ë‚´ìš© ë§ë‹¤ ã…‹;;)</p>
<p>ê·¸ë˜ì„œ ì™œ Attentionì— part í•˜ë‚˜ë¥¼ ë‹¤ ì¨ë¨¹ë‚˜? ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œ ë‚´ìš©ì´ê¸¸ë˜?</p>
<p>ì¤‘ìš”í•œ ë‚´ìš©ì¸ ê²ƒë„ ë§ì§€ë§Œ, í•„ìê°€ ë” ìì„¸íˆ ê³µë¶€í•˜ë ¤ê³  ì´ëŸ° ê²ƒì´ë‹¤. ê·¸ë¦¬ê³  í•„ìëŠ” ì´ ì´í›„ í¬ìŠ¤íŒ…ì—ì„œëŠ” ì¼ë°˜í™”ëœ ë¸íƒ€ ê·œì¹™ì„ ë‚´ì„¸ìš°ì§€ ì•Šì„ ê²ƒì´ë‹¤. ì™œëƒí•˜ë©´ ì´ì œë¶€í„°ëŠ” ì§„ì§œ ì˜ë¯¸ê°€ ì—†ë‹¤ê³  ìƒê°í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ì´ê±¸ êµ³ì´ ì¼ë°˜í™”ëœ ë¸íƒ€ ê·œì¹™ìœ¼ë¡œ í’€ì–´ì„œ êµ¬í˜„í•  ë°”ì—ëŠ” ê·¸ëƒ¥ AutoGradë¥¼ ì§ì ‘ êµ¬í˜„í•˜ëŠ”ê²Œ ë¹ ë¥´ë‹¤.</p>
<p>ìš°ì„  ê·¸ë ‡ë‹¤ë©´ Attentionì´ ì™œ ë‚˜ì™”ë‚˜ë¶€í„° ìƒê°í•´ ë³´ë„ë¡ í•˜ì.<br>ê·¸ëŸ¬ê¸° ìœ„í•´ì„œëŠ” ìš°ì„  Seq2Seqë¶€í„° ì•Œì•„ì•¼ í•˜ëŠ”ë°, ê°„ë‹¨í•˜ê²Œë§Œ ì•Œì•„ë³´ë„ë¡ í•˜ì. ë‹¤ìŒ ê·¸ë¦¼ìœ¼ë¡œ Seq2Seqì˜ êµ¬ì¡°ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆë‹¤.</p>
<p align="center"><img src="https://kimh060612.github.io/img/Seq2Seq.png" width="100%"></p>

<p>ì´ Seq2SeqëŠ” ê·¸ë¦¼ê³¼ ê°™ì´ ë™ì‘í•œë‹¤.<br>Encoderì—ì„œëŠ” ì…ë ¥ ë²¡í„°ì˜ ë¶„ì„ì„, Decoderì—ì„œëŠ” ì¶œë ¥ì„ ê²°ì •í•˜ëŠ” ì—­í• ì„ í•œë‹¤. ì—¬ê¸°ì„œ EncoderëŠ” ì…ë ¥ì˜ Contextë¥¼ ë¶„ì„í•˜ëŠ” ì—­í• ì„ í•œë‹¤ê³  í•œë‹¤. </p>
<p>Seq2Seqì˜ ë‹¨ì  </p>
<ol>
<li>ì…ë ¥ì„ Encoderì—ì„œ ê³ ì •ëœ í¬ê¸°ë¡œ ì••ì¶•í•˜ì—¬ context vectorë¡œ ë§Œë“ ë‹¤. ê·¸ëŸ° êµ¬ì¡°ëŠ” ì •ë³´ ì†ì‹¤ì„ ê°€ì ¸ì˜¤ê¸° ë§ˆë ¨ì´ë‹¤.</li>
<li>ì´ëŸ¬í•œ Context VectorëŠ” Encoderì˜ ë§ˆì§€ë§‰ ì¶œë ¥ì¸ë° ì´ê²ƒë§Œì„ ì‚¬ìš©í•˜ê²Œ ëœë‹¤ë©´ ì´ˆë°˜ì˜ ì •ë³´ëŠ” ìœ ì‹¤ë˜ê¸° ë§ˆë ¨ì´ë‹¤.  </li>
<li>RNNì˜ ê³ ì§ˆì ì¸ ë¬¸ì œì¸ Gradient Vanishing ë¬¸ì œê°€ ë°œìƒí•œë‹¤.</li>
</ol>
<p>ì´ë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•´ì„œ ë‚˜ì˜¨ ê²ƒì´ Attention êµ¬ì¡°ì´ë‹¤.</p>
<h4 id="Attention-in-Seq2Seq"><a href="#Attention-in-Seq2Seq" class="headerlink" title="Attention in Seq2Seq"></a>Attention in Seq2Seq</h4><p>ë¨¼ì €, Attentionì˜ í° êµ¬ì¡° ë¶€í„° ì•Œê³  ë„˜ì–´ê°€ì. Attentionì€ ë‹¤ìŒê³¼ ê°™ì€ í•¨ìˆ˜ë¡œ í‘œí˜„ë  ìˆ˜ ìˆë‹¤.</p>
<p>Attention($Q$, $K$, $V$) = Attention Value</p>
<p>ê·¸ë ‡ë‹¤ë©´ ìš°ë¦¬ëŠ” Attention í•¨ìˆ˜ ìì²´ì— ëŒ€í•´ì„œ ì•Œê¸° ì „ì— ë¨¼ì € $Q$, $K$, $V$ë¥¼ ë¨¼ì € ì •ì˜í•  í•„ìš”ê°€ ìˆë‹¤.<br>Seq2Seq ëª¨ë¸ì—ì„œ $Q$, $K$, $V$ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ë  ìˆ˜ ìˆë‹¤.</p>
<p>$Q$ : íŠ¹ì • ì‹œì ì˜ ë””ì½”ë” ì…€ì—ì„œì˜ ì€ë‹‰ ìƒíƒœ<br>$K$ : ëª¨ë“  ì‹œì ì˜ ì¸ì½”ë” ì…€ì˜ ì€ë‹‰ ìƒíƒœë“¤<br>$V$ : ëª¨ë“  ì‹œì ì˜ ì¸ì½”ë” ì…€ì˜ ì€ë‹‰ ìƒíƒœë“¤  </p>
<p>Seq2Seq ëª¨ë¸ì—ì„œ ìˆœì „íŒŒë¥¼ í•œë‹¤ë©´ ìœ„ì˜ í–‰ë ¬ë“¤ì„ ì „ë¶€ êµ¬í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. ê·¸ë ‡ë‹¤ë©´ ìš°ë¦¬ëŠ” Attention ì—°ì‚°ì„ ì •ì˜í•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤. Attention ì—°ì‚°ì€ ë˜ê²Œ ë‹¤ì–‘í•œ ì¢…ë¥˜ê°€ ìˆë‹¤. ê°„ë‹¨í•˜ê²Œ ì •ë¦¬í•´ë³´ìë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>
<ol>
<li>Dot Attention</li>
<li>Scaled Dot Attention</li>
<li>Bahdanau Attention</li>
</ol>
<p>ìš°ë¦¬ëŠ” ì´ ì¤‘ì—ì„œ Scaled Dot Attentionì„ ë‹¤ë¤„ë³¼ ê²ƒì´ë‹¤. ì™œëƒ? Transformerì— ì“°ì´ë‹ˆê¹.</p>
<p>Scaled Dot Attentionì˜ ìˆ˜ì‹ì„ ì¨ë³´ìë©´ ë‹¤ìŒê³¼ ê°™ì´ ê°„ë‹¨í•˜ê²Œ ì“¸ ìˆ˜ ìˆë‹¤.</p>
<script type="math/tex; mode=display">
\text{Attention}(Q, K, V) = \text{Softmax}(\frac{QK^T}{\sqrt{n}})V
\tag{definition 1}</script><p>ì—¬ê¸°ì„œ $n$ì€ RNN(LSTMì´ë“  ë­ë“ )ì˜ ì¶œë ¥ ë²¡í„°ì˜ í¬ê¸°ì´ë‹¤. ì¦‰, $Q$ì˜ í¬ê¸°ì™€ ê°™ë‹¤.<br>ê²°êµ­ Attention êµ¬ì¡°ëŠ” ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ì„ ë„ê³  Seq2Seqì™€ ê²°í•©í•œë‹¤.</p>
<p align="center"><img src="https://kimh060612.github.io/img/Attention.png" width="100%"></p>

<p>ì¶œì²˜: Luongâ€™s <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1508.04025v5">paper</a></p>
<p>ìœ„ ê·¸ë¦¼ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, ê° $Q$ ë²¡í„°ë“¤ì€ RNN(ë˜ëŠ” LSTM ë˜ëŠ” GRU)ì˜ Decoder ë¶€ë¶„ì˜ ì€ë‹‰ ìƒíƒœì´ë‹¤. ê° ì‹œê°„ ìŠ¤í…ì—ì„œì˜ ì¶œë ¥ì„ ë°”íƒ•ìœ¼ë¡œ ìœ„ì˜ Attention í•¨ìˆ˜ë¥¼ Encoderì˜ ì¶œë ¥ ê°’ê³¼ ê°™ì´ ê³„ì‚°í•  ìˆ˜ ìˆê³ , ê·¸ë¥¼ ì´ìš©í•´ì„œ Attention Vectorë¥¼ ë§Œë“¤ê³  ë‹¤ì‹œ $Q$ì™€ ê²°í•©í•œ ê²ƒì„ FCNNì— ì…ë ¥í•˜ì—¬ ìµœì¢… ì¶œë ¥ vectorë¥¼ ì–»ì–´ë‚¸ ë’¤ì— Softmaxë¥¼ ì”Œì›Œì„œ ì˜ˆì¸¡ ë‹¨ì–´ë¥¼ ë¶„ë¥˜í•œë‹¤.</p>
<p>ë§ë¡œ ê¸¸ê²Œ ì„¤ëª…í•˜ì˜€ëŠ”ë°, ì´ë¥¼ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•´ ë³´ì.</p>
<script type="math/tex; mode=display">
Q^t \in \mathbb{R}^n \\ 
K \in \mathbb{R}^{n * T} \\
V \in \mathbb{R}^{T * n}
\tag{definition 2}</script><p>ìš°ì„  ê° í–‰ë ¬ë“¤ì˜ í¬ê¸°ëŠ” ìœ„ì™€ ê°™ì´ í‘œí˜„ì´ ê°€ëŠ¥í•˜ë‹¤. $Q^t$ëŠ” Decoderì—ì„œ time step $t$ì—ì„œì˜ ì¶œë ¥ì´ë‹¤. ì—¬ê¸°ê¹Œì§€ ì´í•´ê°€ ë˜ì—ˆìœ¼ë©´ ë‹¤ì‹œ Scaled Dot Attentionì˜ ì •ì˜ì—ì„œ ì„±ë¶„ë³„ë¡œ í•˜ë‚˜ì”© ëœ¯ì–´ì„œ ì‚´í´ë³´ì.</p>
<ol>
<li>$Q^tK^{\textbf{T}}$: $K$ì˜ ê²½ìš°, inputì˜ ê° time stepì˜ ì¶œë ¥ì„ í•œë° ëª¨ì•„ë†“ì€ Matrixì´ë‹¤. ì´ë¥¼ $Q^t$ ë²¡í„°ì™€ ê³±í•œë‹¤. ì´ë¥¼ ì•ìœ¼ë¡œ í¸ì˜ìƒ $e^t$ë¼ê³  ì •ì˜í•˜ë„ë¡ í•˜ê² ë‹¤.</li>
<li>$\text{Softmax}$: ìœ„ì˜ $e^t$ì— $\sqrt{n}$ë¡œ ë‚˜ëˆˆ ê°’ì„ Softmaxì— ì§‘ì–´ ë„£ìœ¼ë¯€ë¡œì¨ Attention Distributionì„ ë½‘ì•„ë‚¸ë‹¤. ì´ë¥¼ í¸ì˜ìƒ $\alpha^t$ë¼ê³  ë¶€ë¥´ë„ë¡ í•˜ê² ë‹¤.</li>
<li>$V$: ìœ„ì˜ $\alpha^t$ëŠ” $\mathbb{R}^{T}$ì¸ ë²¡í„°ì´ë‹¤. ì´ ë²¡í„°ì˜ ê° ì„±ë¶„ìœ¼ë¡œ Input Encoderì˜ ê° ì¶œë ¥ì„ ê°€ì¤‘í•©í•œë‹¤. ì´ ê²°ê³¼ë¥¼ Context vectorë¼ê³ ë„ ë¶€ë¥¸ë‹¤.</li>
</ol>
<p>ì´ë ‡ê²Œ Context Vectorë¥¼ ì–»ì€ í›„ì— ìš°ë¦¬ëŠ” $Q^t$ì™€ Context vectorë¥¼ Concatenationí•œ ë’¤ì— FCNN Layerì— ì…ë ¥ìœ¼ë¡œ ë„£ëŠ”ë‹¤.</p>
<script type="math/tex; mode=display">
W_c \in \mathbb{R}^{n * 2n}: \text{FCNNì˜ ê°€ì¤‘ì¹˜ ë²¡í„°}
\tag{definition 3}</script><p>Context vectorì™€ $Q^t$ë¥¼ concatenationí•œ vectorì˜ í¬ê¸°ê°€ $2n$ì´ë‹ˆ, FCNNì˜ ê°€ì¤‘ì¹˜ ë²¡í„°ê°€ ìœ„ì™€ ê°™ì´ ì •ì˜ë˜ì–´ì•¼ í•œë‹¤.<br>ê·¸ë¦¬ê³  ê·¸ ê²°ê³¼ ì¶œë ¥ì„ Softmax í•¨ìˆ˜ì— ë„£ìœ¼ë©´ timestep $t$ì˜ ì¶œë ¥ì´ ëœë‹¤.</p>
<p>ì´ë ‡ê²Œ Attentionì„ ì•Œì•„ ë³´ì•˜ëŠ”ë° ë‹¤ìŒ íŒŒíŠ¸ì—ì„œëŠ” Attentionì´ ì–´ë–»ê²Œ Transformerì— ì‚¬ìš©ë˜ëŠ”ì§€ë¥¼ ì•Œì•„ë³´ë„ë¡ í•˜ê² ë‹¤.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kimh060612.github.io/2022/03/05/Optimization-B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Michael Kim">
      <meta itemprop="description" content="Backend Engineering, Deep learning, Algorithm & DS, CS ì „ê³µ ì§€ì‹ ê³µë¶€ ì €ì¥ì†Œ">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Michael's Study Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/05/Optimization-B/" class="post-title-link" itemprop="url">Optimization ê°•ì˜ ë‚´ìš© part.B</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-03-05 19:27:39" itemprop="dateCreated datePublished" datetime="2022-03-05T19:27:39+09:00">2022-03-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-05-30 00:00:32" itemprop="dateModified" datetime="2022-05-30T00:00:32+09:00">2022-05-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2022/03/05/Optimization-B/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2022/03/05/Optimization-B/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><em>ë³¸ í¬ìŠ¤íŠ¸ëŠ” Hands-on Machine learning 2nd Edition, CS231n, Tensorflow ê³µì‹ document, Pattern Recognition and Machine Learning, Deep learning(Ian Goodfellow ì €)ì„ ì°¸ì¡°í•˜ì—¬ ì‘ì„±ë˜ì—ˆìŒì„ ì•Œë¦½ë‹ˆë‹¤.</em></p>
<h3 id="Index"><a href="#Index" class="headerlink" title="Index"></a>Index</h3><ol>
<li><del>Essential Mathematics</del>  <ul>
<li><del>Basic of Bayesian Statistics</del></li>
<li><del>Information Theory</del></li>
<li><del>Gradient</del></li>
</ul>
</li>
<li><del>Loss Function Examples</del></li>
<li><del>What is Optimizer?</del></li>
<li><del>Optimizer examples</del></li>
<li>Partice</li>
</ol>
<p><br></p>
<h3 id="Partice"><a href="#Partice" class="headerlink" title="Partice"></a>Partice</h3><hr>
<p>ì, ì´ì œ êµ¬í˜„ì˜ ë‚œì´ë„ ì¸¡ë©´ì—ì„œëŠ” ê°€ì¥ ì–´ë µë‹¤ê³  ë§í•  ìˆ˜ ìˆëŠ” íŒŒíŠ¸ê°€ ì™”ë‹¤. ì´ë²ˆì—ë„ Model Subclassing APIë¥¼ í™œìš©í•˜ì—¬ Custom Optimizerë¥¼ ë§Œë“¤ì–´ ë³´ë„ë¡ í•˜ê² ë‹¤. Customì´ë¼ê³  í•´ì„œ ë­”ê°€ ìƒˆë¡œìš´ê±´ ì•„ë‹ˆê³ , ê°„ë‹¨í•˜ê²Œ SGDë¥¼ êµ¬í˜„í•´ ë³¼ ê²ƒì´ë‹¤. ê·¼ë° ì†”ì§íˆ ì´ê±´ ë³„ë¡œ ì“¸ë°ê°€ ì—†ë‹¤. ì´ê²ƒê¹Œì§€ ê±´ë“œë ¤ì•¼í•˜ëŠ” ì‚¬ëŒì€ ì•„ë§ˆë„ ì§ì ‘ ì§œëŠ”ê²Œ ë¹ ë¥´ì§€ ì•Šì„ê¹Œ ì‹¶ë‹¤.<br>ì—¬ê¸°ì„œ ê°€ì¥ ì¤‘ìš”í•œ ê²ƒì€ Custom Training loopì™€ Custom lossì´ë‹¤. SGDëŠ” ê°„ë‹¨í•˜ê²Œ ì†ŒìŠ¤ì½”ë“œë§Œ ë³´ê³  Training loopì™€ Custom lossë¥¼ ìì„¸íˆ ì„¤ëª…í•˜ë„ë¡ í•˜ê² ë‹¤.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CustomSGDOptimizer</span>(keras.optimizers.Optimizer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, learning_rate = <span class="number">0.001</span>, name = <span class="string">&quot;CustomSGDOptimizer&quot;</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(name, **kwargs)</span><br><span class="line">        self._set_hyper(<span class="string">&quot;learning_rate&quot;</span>, kwargs.get(<span class="string">&quot;lr&quot;</span>, learning_rate))</span><br><span class="line">        self._is_first = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_create_slots</span>(<span class="params">self, var_list</span>):</span><br><span class="line">        <span class="keyword">for</span> var <span class="keyword">in</span> var_list:</span><br><span class="line">            self.add_slot(var, <span class="string">&quot;pv&quot;</span>) <span class="comment"># previous variable</span></span><br><span class="line">        <span class="keyword">for</span> vat <span class="keyword">in</span> var_list:</span><br><span class="line">            self.add_slot(var, <span class="string">&quot;pg&quot;</span>) <span class="comment"># previous gradient</span></span><br><span class="line">    </span><br><span class="line"><span class="meta">    @tf.function</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_resource_apply_dense</span>(<span class="params">self, grad, var</span>):</span><br><span class="line">        var_dtype = var.dtype.base_dtype</span><br><span class="line">        lr_t = self._decayed_lr(var_dtype)</span><br><span class="line"></span><br><span class="line">        new_var_m = var - lr_t * grad</span><br><span class="line"></span><br><span class="line">        pv_var = self.get_slot(var, <span class="string">&quot;pv&quot;</span>)</span><br><span class="line">        pg_var = self.get_slot(var, <span class="string">&quot;pg&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self._is_first :</span><br><span class="line">            self._is_first = <span class="literal">False</span></span><br><span class="line">            new_var = new_var_m</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            cond = grad * pg_var &gt;= <span class="number">0</span></span><br><span class="line">            avg_weight = (pv_var + var) / <span class="number">2.0</span></span><br><span class="line">            new_var = tf.where(cond, new_var_m, avg_weight)</span><br><span class="line">        </span><br><span class="line">        pv_var.assign(var)</span><br><span class="line">        pg_var.assign(grad)</span><br><span class="line"></span><br><span class="line">        var.assign(new_var)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_resource_apply_sparse</span>(<span class="params">self, grad, var</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_config</span>(<span class="params">self</span>):</span><br><span class="line">        base_config = <span class="built_in">super</span>().get_config()</span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            **base_config,</span><br><span class="line">            <span class="string">&quot;learning_rate&quot;</span> : self._serialize_hyperparameter(<span class="string">&quot;lr&quot;</span>)</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>ê·¸ë§Œ ì•Œì•„ë³´ì.</p>
</blockquote>
<p>ì¥ë‚œ ì•ˆì¹˜ê³  ì´ê±´ ë‚˜ì¤‘ì— í•˜ë‚˜ì˜ í¬ìŠ¤íŠ¸ë¥¼ ë‹¤ ì¨ì„œ ì„¤ëª…í•  ê²ƒì´ë‹¤. ê·¸ ì •ë„ë¡œ ë‹¤ë¥¸ ì¤‘ìš”í•œ topicê³¼ ê°™ì´ ë‹¤ë£¨ê¸°ì—ëŠ” ë¬´ê²ë‹¤.</p>
<p>ì¼ë‹¨, Custom Training loopë¥¼ ì–´ë–»ê²Œ ë§Œë“œëŠ”ì§€ë¥¼ ì•Œì•„ë³´ë„ë¡ í•˜ê² ë‹¤.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">train_dataset = tf.data.Dataset.from_tensor_slices((train_img, train_labels))</span><br><span class="line">                            <span class="comment"># ì„ì–´.                        # ë°°ì¹˜ ì‚¬ì´ì¦ˆ ë§Œí¼ ë‚˜ëˆ .</span></span><br><span class="line">train_dataset = train_dataset.shuffle(buffer_size=<span class="number">1024</span>).batch(BatchSize)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Optimizer &amp; Loss Function ì •ì˜</span></span><br><span class="line">optimizer = keras.optimizers.Adam(learning_rate=LR)</span><br><span class="line">loss_function = keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">train_accuracy = keras.metrics.SparseCategoricalAccuracy()</span><br><span class="line">val_acc_metric = keras.metrics.SparseCategoricalAccuracy()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCHS):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Epoch %d start&quot;</span>%epoch)</span><br><span class="line">    <span class="comment"># step, 1ê°œì˜ batch ==&gt; ì˜ì‚¬ ì½”ë“œì—ì„œ batch ë½‘ëŠ” ì—­í• </span></span><br><span class="line">    <span class="keyword">for</span> step, (x_batch, y_batch) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_dataset):</span><br><span class="line">        <span class="comment"># **********************************************************************************************</span></span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">            logits = model(x_batch, training=<span class="literal">True</span>)</span><br><span class="line">            loss_val = loss_function(y_batch, logits)</span><br><span class="line">            <span class="comment"># ì—¬ê¸°ì„œ ì‹ ê²½ë§ì˜ Feed Forward &amp; Expectation of Loss ê³„ì‚°ì„ ì§„í–‰í•¨.</span></span><br><span class="line">        <span class="comment"># tape.gradientë¥¼ í˜¸ì¶œí•˜ë©´ ==&gt; gradient ê³„ì‚°ì´ ì§„í–‰ë¨.</span></span><br><span class="line">        grad = tape.gradient(loss_val, model.trainable_weights)</span><br><span class="line">        <span class="comment"># ì •ì˜ëœ Optimizerë¥¼ ì´ìš©í•´ì„œ Updateë¥¼ ì§„í–‰í•¨.</span></span><br><span class="line">        optimizer.apply_gradients(<span class="built_in">zip</span>(grad, model.trainable_weights))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># trainì—ì„œì˜ ì •í™•ë„ë¥¼ ê³„ì‚°í•¨.</span></span><br><span class="line">        train_accuracy.update_state(y_batch, logits)</span><br><span class="line">        <span class="keyword">if</span> step % <span class="number">500</span> == <span class="number">0</span> :</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Training loss at step %d: %.4f&quot;</span>%(step, loss_val))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># ì •í™•ë„ ë½‘ì•„ ë³´ê² ë‹¤.</span></span><br><span class="line">    train_acc = train_accuracy.result()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Training acc over epoch: %.4f&quot;</span> % (<span class="built_in">float</span>(train_acc),))</span><br><span class="line">    train_accuracy.reset_states()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> x_batch_val, y_batch_val <span class="keyword">in</span> validation_dataset:</span><br><span class="line">        val_logits = model(x_batch_val, training = <span class="literal">False</span>)</span><br><span class="line">        val_acc_metric.update_state(y_batch_val, val_logits)</span><br><span class="line">    val_acc = val_acc_metric.result()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Validation acc: %.4f&quot;</span> % (<span class="built_in">float</span>(val_acc),))</span><br><span class="line">    val_acc_metric.reset_states()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>ê¸°ë³¸ì ìœ¼ë¡œ Custom Training LoopëŠ” ìœ„ì™€ ê°™ì´ êµ¬í˜„ëœë‹¤. ì´ê±¸ í•˜ë‚˜í•˜ë‚˜ ëœ¯ì–´ì„œ ì„¤ëª…í•´ ë³´ë„ë¡ í•˜ê² ë‹¤.</p>
<ol>
<li><p>tf.GradientTape()</p>
<p> tensorflow 2ì˜ í•µì‹¬ì¸ AutoGradë¥¼ êµ¬ë™ì‹œì¼œì£¼ëŠ” ì¹œêµ¬ì´ë‹¤. ì´ tape scope ì•ˆì—ì„œ ì‹¤í–‰ëœ tensorflow operationë“¤ì€ back propagationì„ ìœ„í•œ AutoGrad ë¯¸ë¶„ ê·¸ë˜í”„ì˜ êµ¬ì¶•ì´ ì‹œì‘ëœë‹¤.  </p>
</li>
<li><p>tape.gradient</p>
<p> ì´ í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•˜ë©´ êµ¬ì¶•ëœ AutoGrad ë¯¸ë¶„ ê·¸ë˜í”„ë¥¼ ë”°ë¼ì„œ ë¯¸ë¶„ì´ ì‹œì‘ëœë‹¤. </p>
</li>
<li><p>apply_gradients</p>
<p> ì´ëŠ” optimizer(ex. Adam)ì˜ methodì´ê³  tape.gradientì—ì„œ êµ¬í•œ gradientë¥¼ ì‚¬ìš©ìê°€ ì •í•œ optimizing algorithmì„ í†µí•´ì„œ Weightë¥¼ ì—…ë°ì´íŠ¸ í•´ì¤€ë‹¤.</p>
</li>
<li><p>update_state</p>
<p>ì´ëŠ” í•™ìŠµ ê³¼ì •ì¤‘ì—ì„œ ì¸¡ì •í•  ìˆ˜ ìˆëŠ” Metricë“¤ì„ êµ¬í•˜ëŠ”ë° ì‚¬ìš©ëœë‹¤. (ex. ì •í™•ë„) kerasì—ì„œ ì œê³µí•˜ê±°ë‚˜ ì§ì ‘ ë§Œë“  Metric ê°ì²´ë¥¼ ìƒˆë¡œ Nerual Networkì—ì„œ ê³„ì‚°ëœ batchì— ì ìš©í•˜ê³  ì‹¶ì„ë•Œ ì‚¬ìš©í•œë‹¤.</p>
</li>
</ol>
<p>ì´ë ‡ê²Œ Custom Training loopëŠ” í¬ê²Œ 4ê°€ì§€ ìš”ì†Œë¡œ êµ¬ì„±ëœë‹¤.<br>í•„ìëŠ” ì´ê²ƒì„ ë³´í†µ í…œí”Œë¦¿ìœ¼ë¡œ ê°€ì§€ê³  ê°œë°œí• ë•Œë§ˆë‹¤ ì¡°ê¸ˆì”© ë°”ê¿”ì„œ ì‚¬ìš©í•˜ëŠ” í¸ì´ë‹¤. ë…ìë“¤ë„ ì¡°ê¸ˆ ë³µì¡í•œ íŠ¸ë¦­ì´ í•„ìš”í•œ Neural Networkë¥¼ êµ¬í˜„í• ë•Œ ë³¸ì¸ë§Œì˜ Training loopë¥¼ êµ¬ì„±í•˜ê³  ì¡°ê¸ˆì”© ë°”ê¿”ê°€ë©´ì„œ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ì¶”ì²œí•œë‹¤. </p>
<p>ë‹¤ìŒìœ¼ë¡œ ë‹¤ë£° ê²ƒì€ Custom lossì´ë‹¤. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Custom Loss</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomMSE</span>(keras.losses.Loss):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, y_true, y_pred</span>):</span><br><span class="line">        <span class="comment"># tf.math.square : ì„±ë¶„ë³„ ì œê³±</span></span><br><span class="line">        <span class="comment"># [1,2,3] ==&gt; [1,4,9]</span></span><br><span class="line">        L = tf.math.square(y_true - y_pred)</span><br><span class="line">        <span class="comment"># tf.math.reduce_sum: ë²¡í„° í•©.</span></span><br><span class="line">        <span class="comment"># [1,2,3] ==&gt; 6</span></span><br><span class="line">        L = tf.math.reduce_mean(L)</span><br><span class="line">        <span class="keyword">return</span> L</span><br><span class="line"></span><br><span class="line"><span class="comment"># Custom Regularizer(ê·œì œ ==&gt; MAP)</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomRegularizer</span>(keras.regularizers.Regularizer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, _factor</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.factor = _factor</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, weights</span>):</span><br><span class="line">        <span class="keyword">return</span> tf.math.reduce_sum(tf.math.<span class="built_in">abs</span>(self.factor * weights))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_config</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&quot;factor&quot;</span> : self.factor&#125; <span class="comment"># ëª¨ë¸ì„ ì €ì¥í• ë•Œ custom layer, loss, ë“±ë“±ì„ ì €ì¥í• ë•Œ ê°™ì´ ì €ì¥í•´ ì£¼ëŠ” ì—­í• .</span></span><br></pre></td></tr></table></figure>
<p>tensorflowëŠ” ì‚¬ìš©ìê°€ Lossë‚˜ Regularizerê¹Œì§€ë„ ììœ ë¡­ê²Œ êµ¬ì„±í•  ìˆ˜ ìˆë„ë¡ í—ˆë½í•´ì¤€ë‹¤. ì‚¬ìš© ë°©ë²•ì€ ê¸°ì¡´ lossë“¤ê³¼ ë˜‘ê°™ë‹¤. ê·¸ì € ìì‹ ì´ ì›í•˜ëŠ” ì—°ì‚°ë“¤ì„ ìƒê°í•´ì„œ êµ¬í˜„í•˜ë©´ ëœë‹¤.</p>
<p>ì´ê±¸ë¡œ Optimization part.Bë¥¼ ë§ˆì¹˜ë„ë¡ í•˜ê² ë‹¤. ì´ ì •ë„ê¹Œì§€ Customí•´ì„œ ì‚¬ìš©í•  ì‚¬ëŒë“¤ì€ ë§ì´ ì—†ê² ì§€ë§Œ í˜¹ì‹œë¼ë„ í•„ìš”í•œ ì‚¬ëŒë“¤ì´ ìˆì„ê¹Œ ì‹¶ì–´ì„œ ì ì–´ ë³´ì•˜ë‹¤.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kimh060612.github.io/2022/03/05/Optimization-A/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Michael Kim">
      <meta itemprop="description" content="Backend Engineering, Deep learning, Algorithm & DS, CS ì „ê³µ ì§€ì‹ ê³µë¶€ ì €ì¥ì†Œ">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Michael's Study Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/05/Optimization-A/" class="post-title-link" itemprop="url">Optimization ê°•ì˜ ë‚´ìš© part.A</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-03-05 19:27:37" itemprop="dateCreated datePublished" datetime="2022-03-05T19:27:37+09:00">2022-03-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-05-30 00:00:32" itemprop="dateModified" datetime="2022-05-30T00:00:32+09:00">2022-05-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2022/03/05/Optimization-A/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2022/03/05/Optimization-A/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><em>ë³¸ í¬ìŠ¤íŠ¸ëŠ” Hands-on Machine learning 2nd Edition, CS231n, Tensorflow ê³µì‹ document, Pattern Recognition and Machine Learning, Deep learning(Ian Goodfellow ì €)ì„ ì°¸ì¡°í•˜ì—¬ ì‘ì„±ë˜ì—ˆìŒì„ ì•Œë¦½ë‹ˆë‹¤.</em></p>
<h3 id="Index"><a href="#Index" class="headerlink" title="Index"></a>Index</h3><ol>
<li>Essential Mathematics  <ul>
<li>Basic of Bayesian Statistics</li>
<li>Information Theory</li>
<li>Gradient</li>
</ul>
</li>
<li>Loss Function Examples</li>
<li>What is Optimizer?</li>
<li>Optimizer examples</li>
<li>Partice</li>
</ol>
<p>ì—¬ê¸°ì„œëŠ” 1~4ëŠ” Part. Aì´ê³  5ëŠ” Part. Bì—ì„œ ë‹¤ë£¨ë„ë¡ í•˜ê² ë‹¤.<br><br></p>
<h3 id="Essential-Mathematics"><a href="#Essential-Mathematics" class="headerlink" title="Essential Mathematics"></a>Essential Mathematics</h3><hr>
<h4 id="Basic-of-Bayesian-Statistics"><a href="#Basic-of-Bayesian-Statistics" class="headerlink" title="Basic of Bayesian Statistics"></a>Basic of Bayesian Statistics</h4><p>ì´ íŒŒíŠ¸ì—ì„œëŠ” Bayesian Statisticsì— ëŒ€í•´ì„œ ë§¤~ìš° ê°„ë‹¨í•˜ê²Œ ë‹¤ë£¨ì–´ ë³´ë„ë¡ í•˜ê² ë‹¤. ì‹¤ì€ ë‹¤ë£¬ë‹¤ê³  ë§í•˜ëŠ” ê²ƒë„ ë¶€ë„ëŸ¬ìš¸ ì •ë„ë¡ë§Œ í•  ì˜ˆì •ì´ë‹ˆ ë„ˆë¬´ ê¸°ëŒ€ëŠ” í•˜ì§€ ì•Šì•„ ì£¼ì—ˆìœ¼ë©´ í•œë‹¤.</p>
<p>ì—¬ê¸°ì„œëŠ” ì¡°ê¸ˆ ìµìˆ™í•˜ì§€ ì•Šì€ í†µê³„í•™, Bayesian í†µê³„í•™ì„ ì†Œê°œí•˜ë„ë¡ í•  ê²ƒì´ë‹¤. ì´ ë¶„ì•¼ëŠ” íŒ¨í„´ ì¸ì‹ì—ì„œ ì•„ì£¼ ë§ì´ í™œìš©ë˜ë©°, ê¸°ì¡´ì˜ í™•ë¥ ë¡ ì— ìµìˆ™í•´ì ¸ ìˆë‹¤ë©´ ì´ë¥¼ ë°›ì•„ë“¤ì´ê¸° ë§¤ìš° í˜ë“¤ë‹¤.</p>
<p>ìš°ì„ , â€œí™•ë¥ â€ì´ë€ ë¬´ì—‡ì¸ì§€ì— ëŒ€í•´ ë…¼í•˜ê³  ë„˜ì–´ê°€ì.<br>Frequentist â€“ â€œë¹ˆë„â€ì— ëŒ€í•œ ì²™ë„, ì–´ë–¤ ì¼ì´ ì•ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ì¼ì–´ë‚  ìˆ˜ ìˆê² ëŠ”ê°€?<br>Bayesian â€“ ë¶ˆí™•ì‹¤ì„±ì— ëŒ€í•œ ì²™ë„, ì´ ê°€ì„¤ì— ëŒ€í•´ ì–¼ë§ˆë‚˜ í™•ì‹ í•  ìˆ˜ ìˆëŠ”ê°€?</p>
<p>Bayesian ê´€ì ì—ì„œì˜ í™•ë¥ ì„ ê·¸ë¦¼ìœ¼ë¡œ í‘œí˜„í•˜ìë©´ ëŒ€ì¶© ì•„ë˜ì™€ ê°™ë‹¤</p>
<p align="center"><img src="https://kimh060612.github.io/img/BP.png" width="100%"></p>

<p>ê°„ë‹¨í•˜ê²Œë§Œ ë§í•˜ìë©´, í™•ë¥ ì´ 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ â€œëª…ì œì˜ ì°¸ì— ëŒ€í•œ í™•ì‹ â€ì„ ê°€ì§ˆ ìˆ˜ ìˆê³  0ì— ê°€ê¹Œìš¸ ìˆ˜ë¡ ê·¸ ë°˜ëŒ€ì¸ ê²ƒì´ë‹¤. ê·¸ë¦¬ê³  0.5ì— ê°€ê¹Œì›Œ ì§ˆìˆ˜ë¡ ì ì  ì• ë§¤í•´ ì§€ëŠ” ê²ƒì´ë‹¤.</p>
<p>ê·¸ë¦¬ê³  ì´ ê´€ì ì—ì„œ ìš°ë¦¬ëŠ” ë‹¤ìŒ 3ê°€ì§€ ê°œë…ì„ ë‹¤ë¤„ ë³¼ ê²ƒì´ë‹¤.</p>
<ol>
<li>Prior Probability</li>
<li>Likelihood</li>
<li>Posterior Probability</li>
</ol>
<p>ì´ 3ê°€ì§€ë¥¼ ë§ë¡œ í’€ì–´ì„œ ì„¤ëª…í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>
<ol>
<li>Prior Probability: ë°ì´í„°ê°€ ì£¼ì–´ì§€ê¸° ì „ì— ìš°ë¦¬ì˜ ê°€ì„¤ì€ ì–¼ë§ˆë‚˜ íƒ€ë‹¹í•œê°€?</li>
<li>Likelihood: ì–´ë–¤ ë°ì´í„°ë“¤ì´ íŠ¹ì • í™•ë¥  ë¶„í¬ì—ì„œ ì¶”ì¶œë˜ì—ˆì„ í™•ë¥ </li>
<li>Posterior Probability: ì£¼ì–´ì§„ ë°ì´í„°ì— í•œì—ì„œ ìš°ë¦¬ì˜ ê°€ì„¤ì´ ì–¼ë§ˆë‚˜ íƒ€ë‹¹í•œê°€?</li>
</ol>
<p>ìš°ë¦¬ëŠ” ì´ë“¤ì•  ëŒ€í•´ì„œ ì—„ë°€í•œ ì •ì˜ë¥¼ ë‹¤ë£¨ëŠ” ê²ƒì´ ì•„ë‹Œ, ì¢€ ë” ì‹¤ìš©ì ì¸ ì¸¡ë©´ì—ì„œ ì˜ˆì œì™€ í•¨ê»˜ ë‹¤ë£¨ì–´ ë³¼ ê²ƒì´ë‹¤.</p>
<p>ì´ë¥¼ í™œìš©í•˜ëŠ” ì˜ˆì œë¡œì¨ í•œë²ˆ Curve fitting ë¬¸ì œë¥¼ ìƒê°í•´ ë³´ì. Curve fittingì„ í•˜ê¸° ìœ„í•´ì„œ ìš°ë¦¬ëŠ” ì–´ë–¤ parameter $w$ë¥¼ ê°€ì§€ê³  ì´ë¥¼ ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê³¡ì„ ì— ë§ê²Œ fittingí•˜ëŠ” ê³¼ì •ì„ ê±°ì¹  ê²ƒì´ë‹¤.</p>
<p>ìš°ë¦¬ëŠ” ì´ë•Œ ì´ëŸ° ê°€ì •ì„ í•  ìˆ˜ ìˆë‹¤.<br>â€œparameter $ğ‘¤$ë¡œ ìƒì„±ëœ ê³¡ì„  $ğ¶$ëŠ” ì£¼ì–´ì§„ ë°ì´í„° $ğ·$ë¥¼ ì˜ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤.â€<br>ì´ ë¬¸ì¥ì€ íŠ¹ì •í•œ ë¶ˆí™•ì‹¤ì„±ì„ ê°€ì§€ê³  ìˆë‹¤. ë‹¹ì—°íˆ ì²˜ìŒë¶€í„° Fittingì´ ì˜ ë  ë¦¬ë„ ì—†ê³ , íŠ¹ì •í•œ ì—ëŸ¬ë¥¼ ê°€ì§€ê³  ìˆìŒì´ ë¶„ëª…í•˜ë‹¤.</p>
<p>ì´ë ‡ë‹¤ë©´ ìš°ë¦¬ëŠ” ê·¸ ë¶ˆí™•ì‹¤ì„±ì„ ì–´ë–»ê²Œ í™•ë¥ ë¡œ í‘œí˜„ ê°€ëŠ¥í•˜ê² ëŠ”ê°€? í•œë²ˆ ë‹¤ìŒê³¼ ê°™ì´ í•´ë³´ì.</p>
<p>$Pr(w)$: ë°ì´í„°ê°€ ì£¼ì–´ì§€ê¸° ì´ì „ì— ìœ„ì˜ ê°€ì„¤ì´ ì–¼ë§ˆë‚˜ í™•ì‹¤í•œê°€<br>$Pr(D|w)$: ì£¼ì–´ì§„ ëª¨ë¸ì—ì„œ ë°ì´í„°ê°€ ì–¼ë§ˆë‚˜ ì˜ ì„¤ëª…ë˜ëŠ” ê°€ì˜ ì²™ë„<br>$Pr(w|D)$: ë°ì´í„°ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ì´ ëª¨ë¸ì´ ë°ì´í„°ë¥¼ ì˜ ì„¤ëª…í•˜ëŠ” ê°€ì˜ ì²™ë„</p>
<p>ì´ë•Œ, ì¢‹ì€ ëª¨ë¸ì„ ë§Œë“œëŠ” ë°©ë²•ì€ í¬ê²Œ 2ê°€ì§€ê°€ ìˆë‹¤.</p>
<ol>
<li>Likelihoodë¥¼ ìµœëŒ€í™”í•˜ëŠ” parameter ë§Œë“¤ê¸° ==&gt; MLE(Maximum Likelihood Estimation)</li>
<li>Posterior Probabilityë¥¼ ìµœëŒ€í™”í•˜ëŠ” parameter ë§Œë“¤ê¸° ==&gt;  MAP (Maximum A Posterior)</li>
</ol>
<p>ì¦‰, 1ë²ˆ ë°©ë²•ì€ ìµœëŒ€í•œ ì¢‹ì€ ì£¼ì–´ì§„ ëª¨ë¸ì—ì„œ ìµœëŒ€í•œ ì¢‹ì€ ë°ì´í„°ì˜ ì„¤ëª…ì„ ì–»ì–´ë‚´ëŠ” ë°©ë²•ì´ê³ , 2ë²ˆ ë°©ë²•ì€ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ìµœëŒ€í•œ ì¢‹ì€ ëª¨ë¸ì„ ì–»ì–´ë‚´ëŠ” ë°©ë²•ì´ë‹¤.</p>
<p>ì´ ê´€ì ì—ì„œ 1ë²ˆë¶€í„° ì°¨ê·¼ì°¨ê·¼ ì„¤ëª…í•´ ë³´ë„ë¡ í•˜ê² ë‹¤. ì–´ë–»ê²Œ í•˜ë©´ likelihoodë¥¼ ìµœëŒ€í™”í•  ìˆ˜ ìˆì„ê¹Œ?</p>
<p>ìš°ì„  $Pr(D|w)$ì˜ í•¨ìˆ˜ë¥¼ ì°¾ì•„ì„œ ì´ë¥¼ ìµœëŒ€ë¡œ ë§Œë“¤ì–´ ì£¼ë©´ ë˜ëŠ” ê²ƒì´ë‹¤. ìµœëŒ€ë¡œ ë§Œë“¤ì–´ ì£¼ëŠ” ë°©ë²•ì€ ì—¬ëŸ¬ê°€ì§€ ë°©ë²•ì´ ìˆê² ì§€ë§Œ, ìš°ì„  ê·¸ê²ƒë³´ë‹¤ $Pr(D|w)$ë¥¼ ì°¾ëŠ” ê²ƒë¶€í„° ì§„í–‰í•˜ëŠ”  ê²ƒì´ ìˆœì„œì¼ ê²ƒì´ë‹¤.</p>
<p>ê·¸ ì „ì— í‘œí˜„ì„ ì¢€ ì •ë¦¬í•˜ê³  ê°€ê² ë‹¤.</p>
<script type="math/tex; mode=display">
\begin{aligned}
    x_i, t_i \text{: ì£¼ì–´ì§„ ë°ì´í„°ì˜ ì ë“¤. ì¦‰,} (x_i, t_i) \in D \\
    y(x_i, w) \text{: parameterê°€ $w$ì¼ ë•Œì˜ ëª¨ë¸ì˜ ì˜ˆì¸¡ ê°’.}
\end{aligned}
\tag{definition 1}</script><p>ì, ê·¸ë¦¬ê³  ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì´ â€œê°€ì •â€ì„ í•´ë³´ì.<br><em>ë°ì´í„°ì—ì„œ ì£¼ì–´ì§„ targetê°’ê³¼ ìš°ë¦¬ì˜ ì˜ˆì¸¡ ê°’ì˜ ì°¨ì´ê°€ Gaussian Distributionì„ ë”°ë¥¸ë‹¤</em></p>
<p>ì´ë¥¼ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>
<script type="math/tex; mode=display">
\epsilon_i \sim \mathcal{N}(0, \beta^{-1})
\tag{definition 2}</script><p>ì—¬ê¸°ì„œ $\epsilon_i$ëŠ” ì‹¤ì¸¡ê°’ê³¼ ë°ì´í„° ê°„ì˜ ì˜¤ì°¨ì´ë‹¤. ($\epsilon_i = t_i - y(x_i;w)$) </p>
<p>ì¦‰, ìš°ë¦¬ê°€ ê³ ë“±í•™êµì—ì„œ í™•ë¥ ê³¼ í†µê³„ë¥¼ ì°©ì‹¤íˆ ë°°ì› ë‹¤ë©´ ë‹¤ìŒê³¼ ê°™ì´ ë³€í˜•ì´ ê°€ëŠ¥í•  ê²ƒì´ë‹¤.</p>
<script type="math/tex; mode=display">
\begin{aligned}
    t_i \sim \mathcal{N}(y(x_i;w), \beta^{-1}) \\
    Pr(t_i | x_i, w, \beta) = \mathcal{N}(y(x_i;w), \beta^{-1})
\end{aligned}
\tag{equation 1}</script><p>ìœ„ì˜ ë‚´ìš©ì„ ì‹œê°í™” í•˜ë©´ ëŒ€ì¶© ë‹¤ìŒê³¼ ê°™ë‹¤.</p>
<p align="center"><img src="https://kimh060612.github.io/img/CF.png" width="100%"></p>

<p>ì´ì œ ê°ê°ì˜ ë°ì´í„°ì— ëŒ€í•´ì„œ ì´ë¥¼ ì •ì˜í•´ ë†“ì•˜ìœ¼ë‹ˆ, ë°ì´í„° ì „ì²´ì— ëŒ€í•´ì„œëŠ” ê°ê°ì˜ ë°ì´í„°ê°€ i.i.dë¼ëŠ” ê°€ì • í•˜ê²Œ ê·¸ëƒ¥ ê³±í•´ì£¼ë©´ ëœë‹¤. ë‹¤ìŒê³¼ ê°™ì´ ë§ì´ë‹¤.</p>
<script type="math/tex; mode=display">
\begin{aligned}
    D = \{(x_i, t_i) | 1 \leq i \leq N\} \\
    Pr(\textbf{t} | \textbf{x}, w, \beta) = \prod^{N}_{n = 1} \mathcal{N}(t_n | y(x_n;w), \beta^{-1})
\end{aligned}
\tag{equation 2}</script><p>ì, ì´ì œì•¼ ë­”ê°€ ì†ì— ì¡íˆëŠ” ëŠë‚Œì´ ë“ ë‹¤(ì•„ë‹Œê°€?!?)<br>ìì„¸íˆ ë³´ë©´ ìœ„ ì‹ì´ likelihood ì•„ë‹Œê°€? ì •ì˜ ê·¸ëŒ€ë¡œì´ë‹¤.</p>
<p>ì´ì œ ìœ„ì˜ Probabilityë¥¼ ìµœëŒ€í™” í•˜ë©´ ë˜ëŠ” ê²ƒì´ë‹¤.<br>ê·¼ë° productëŠ” ë‹¤ë£¨ê¸° ì–´ë ¤ìš°ë‹ˆ ë§ŒëŠ¥ íˆ´ì¸ ë¡œê·¸ë¥¼ ì”Œì›Œ ë³´ì.</p>
<script type="math/tex; mode=display">
\ln Pr(\textbf{t} | \textbf{x}, w, \beta) = -\frac{\beta}{2} \sum^{N}_{n = 1} (y(x_n;w) - t_n)^2 + \frac{N}{2} \ln \beta - \frac{N}{2} \ln 2\pi
\tag{equation 3}</script><p>ì´ë•Œ, ìš°ë¦¬ëŠ” 2ê°€ì§€ parameterì— ëŒ€í•´ì„œ ìœ„ ì‹ì˜ ìµœëŒ€ê°’ì„ êµ¬í•´ì•¼ í•œë‹¤. ì²«ë²ˆì§¸ê°€ $w$ì´ê³  ë‘ë²ˆì§¸ê°€ $\beta$ì´ë‹¤.<br>ìš°ì„  $\beta$ë¶€í„° í•´ë³´ìë©´ ë‹¤ìŒê³¼ ê°™ì´ ê°€ëŠ¥í•˜ë‹¤.</p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial \beta} \ln Pr(\textbf{t} | \textbf{x}, w, \beta) = -\frac{1}{2} \sum^{N}_{n = 1} (y(x_n;w) - t_n)^2 + \frac{N}{2} \frac{1}{\beta}
\tag{equation 4}</script><p>ê²°êµ­ ì´ ë¯¸ë¶„ ê°’ì„ 0ìœ¼ë¡œ ë§Œë“œëŠ” ê°’ì´ ìµœëŒ€ê°’ì¼ ê²ƒì´ë¯€ë¡œ ë‹¤ìŒê³¼ ê°™ì´ ì‹ì„ í’€ì–´ë‚¼ ìˆ˜ ìˆë‹¤.</p>
<script type="math/tex; mode=display">
\frac{1}{\beta_{ML}} = \frac{1}{2} \sum^{N}_{n = 1} (y(x_n;w) - t_n)^2
\tag{equation 5}</script><p>ì´ì œ $w$ì— ëŒ€í•´ì„œ ìµœëŒ€ê°’ì„ êµ¬í•´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>
<script type="math/tex; mode=display">
w_{ML} = \argmax_w \ln Pr(\textbf{t} | \textbf{x}, w, \beta) = \argmin_w \sum^{N}_{n = 1} (y(x_n;w) - t_n)^2</script><p>ì–´..? ì–´ë””ì„œ ë§ì´ ë³¸ ê²ƒ ê°™ì§€ë§Œ ì¼ë‹¨ ë„˜ì–´ê°€ì. (ì‹¤ì€ ë‚˜ì¤‘ì— loss function ë¶€ë¶„ì—ì„œ í•œë²ˆ ë” ë‹¤ë£¨ê² ë‹¤.)</p>
<p>ì—¬ê¸°ì„œì˜ ìµœì í™”ëŠ” ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ ì‹œë„í•  ìˆ˜ ìˆë‹¤.</p>
<p>ì´ì˜ ì›ë¦¬ë¥¼ ë‹¤ì‹œ í•œë²ˆ ë– ì˜¬ë ¤ë³´ì.</p>
<ol>
<li>ì—ëŸ¬ê°€ Gaussian Distributionì„ ë”°ë¥¼ ë•Œ,</li>
<li>ì£¼ì–´ì§„ ë°ì´í„° ğ·ë¥¼ ìš°ë¦¬ì˜ modelì´ ë°ì´í„°ë¥¼ ì˜ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤ëŠ” ê°€ì„¤ì´</li>
<li>ì°¸ì¼ ê²ƒì´ë¼ëŠ” í™•ë¥ (í™•ì‹¤ì„±)ì„ ë†’ì´ëŠ” ê³¼ì • </li>
</ol>
<p>ì´ê²ƒì´ ë°”ë¡œ ìš°ë¦¬ê°€ ì§„í–‰í•œ ê²ƒì´ë‹¤.  </p>
<p>ê·¸ë ‡ë‹¤ë©´, ì´ì œ MAPë¥¼ í™œìš©í•´ì„œ ì´ë¥¼ êµ¬í•´ë³´ë©´ ì–´ë–»ê²Œ ë ê¹Œ?<br>Bayesâ€™ Theoremì— ë”°ë¼ êµ¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ì‘ì„± ê°€ëŠ¥í•˜ë‹¤.</p>
<script type="math/tex; mode=display">
Pr(w | \textbf{t}, \textbf{x}, \alpha, \beta) \propto Pr(\textbf{t} | \textbf{x}, w, \beta) Pr(w|\alpha) 
\tag{eqaution 6}</script><p>ìœ„ ì‹ì—ì„œ ë­”ê°€ ë‚¯ì„¤ì§€ë§Œ ìµìˆ™í•œ ê²ƒì´ ë³´ì¸ë‹¤. ë°”ë¡œ $Pr(w|\alpha)$ì´ë‹¤. ì´ê²ƒì´ ë°”ë¡œ â€œì‚¬ì „ í™•ë¥ â€ì´ë‹¤.</p>
<p>ì‚¬ì „ í™•ë¥  ë˜í•œ ìš°ë¦¬ê°€ ëª¨ë¥´ì§€ë§Œ, ì¼ë‹¨ ë§ŒëŠ¥ì¸ Gaussian ì´ë¼ê³  ê°€ì •í•´ ë³´ì. (ì´ ê°€ì •ì´ ë‚˜ë¦„ì˜ íƒ€ë‹¹ì„±ì„ ê°–ì¶˜ë‹¤ëŠ” ê²ƒì„ ì•Œê³  ì‹¶ìœ¼ë©´ í™•ë¥  ë° ëœë¤ ë³€ìˆ˜ë¥¼ ì¡°ê¸ˆ ê¹Šê²Œ ê³µë¶€í•´ ë³´ì.)</p>
<script type="math/tex; mode=display">
\begin{aligned}
    w \sim \mathcal{N}(0, \alpha^{-1}\textbf{I}) \\ 
    Pr(w|\alpha) = \mathcal{N}(w|0, \alpha^{-1}\textbf{I}) = (\frac{\alpha}{2\pi})^{\frac{M + 1}{2}} e^{-\frac{\alpha}{2}w^{\textbf{T}}w}
\end{aligned}
\tag{equation 7}</script><p>ê·¸ë ‡ë‹¤ë©´, ìš°ë¦¬ëŠ” ì´ë¥¼ í†µí•´ì„œ ìµœëŒ€ (ë¡œê·¸)ì‚¬í›„ í™•ë¥ ì„ ë§ˆì°¬ê°€ì§€ë¡œ ë¯¸ë¶„ì„ í†µí•´ì„œ êµ¬í•´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>
<script type="math/tex; mode=display">
w_{ML} = \argmax_w \ln Pr(w | \textbf{t}, \textbf{x}, \alpha, \beta) = \argmin_w \frac{\beta}{2} \sum^{N}_{n = 1} (y(x_n;w) - t_n)^2 + \frac{\alpha}{2}w^{T}w
\tag{equation 8}</script><p>ì´ê²ƒë„ ë­”ê°€ ì–´ë””ì„œ ë³¸ ê²ƒ ê°™ì€ë°..? ë¼ê³  ìƒê°í•œë‹¤ë©´ ë‹¹ì‹ ì€ ë©‹ì§„ ì‚¬ëŒ :)</p>
<p>ì´ê±¸ë¡œ Bayesian Statisticsì—ì„œ ë§í•˜ëŠ” likelihoodì™€ prior/posterior í™•ë¥ ì´ ì–´ë–¤ ê°œë…ì¸ì§€ ì¡°ê¸ˆì´ë¼ë„ ì™€ ë‹¿ì•˜ìœ¼ë©´ ì¢‹ê² ë‹¤ ã…ã….</p>
<h4 id="Information-Theory"><a href="#Information-Theory" class="headerlink" title="Information Theory"></a>Information Theory</h4><p>ì´ íŒŒíŠ¸ì—ì„œëŠ” ì •ë³´ ì´ë¡ ì„ ë§¤~~~~ìš° ê°„ë‹¨í•˜ê²Œë§Œ ë‹¤ë£¨ì–´ë³¼ ì˜ˆì •ì´ë‹¤. ì´ê²ƒë„ ë‹¤ë£¬ë‹¤ê³  ë§í•˜ê¸°ë„ ë¶€ë„ëŸ½ë‹¤ê³  í•  ìˆ˜ ìˆì„ ì •ë„ë¡œë§Œ ë‹¤ë£¨ë„ë¡ í•˜ê² ë‹¤.</p>
<p>ì •ë³´ ì´ë¡ ì€ â€œì •ë³´ëŸ‰â€ì˜ ê°œë…ìœ¼ë¡œë¶€í„° ì‹œì‘í•œë‹¤. ì´ê²ƒì´ í˜„ëŒ€ì— ë“¤ì–´ì„œ í†µì‹ , íŒ¨í„´ ì¸ì‹ ë“± ì—¬ëŸ¬ ë¶„ì•¼ì— ì‚¬ìš©ë˜ê³  ìˆë‹¤.<br>ê·¸ë¦¬ê³  ì´ëŸ¬í•œ ì •ë³´ëŸ‰ì˜ í‰ê· ì„ â€œì—”íŠ¸ë¡œí”¼â€ë¼ê³  ë¶€ë¥¸ë‹¤.</p>
<p>ì •ë³´ëŸ‰ì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤.</p>
<script type="math/tex; mode=display">
h(x) = -\log_2 p(x) \space \text{bits}</script><script type="math/tex; mode=display">
h(x) = -\ln p(x) \space \text{nats}</script><p>í•œê°€ì§€ ì˜ˆë¥¼ ë“¤ì–´ ë³´ê² ë‹¤. ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ë˜ëŠ” ëœë¤ ë³€ìˆ˜ê°€ ìˆë‹¤ê³  ê°€ì •í•´ ë³´ì.</p>
<p align="center"><img src="https://kimh060612.github.io/img/PVR.png" width="100%"></p>

<p>ì´ì˜ ì—”íŠ¸ë¡œí”¼ë¥¼ êµ¬í•´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>
<p align="center"><img src="https://kimh060612.github.io/img/PVR_Entropy.png" width="100%"></p>

<p>ì´ ë‚´ìš©ì„ ê°‘ìê¸° ì™œ í•˜ëŠëƒ? ìš°ë¦¬ëŠ” ë‹¤ë¥¸ 2ê°œì˜ í™•ë¥ ë¶„í¬ì˜ ìƒëŒ€ì  ì—”íŠ¸ë¡œí”¼ì™€ ìƒí˜¸ ì •ë³´ëŸ‰ì„ ê³„ì‚°í•˜ì—¬ 2ê°œì˜ í™•ë¥ ë¶„í¬ì˜ ì°¨ì´ë¥¼ ì •ëŸ‰í™” í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤.</p>
<p>ìš°ì„ , ìš°ë¦¬ê°€ ê·¼ì‚¬ë¥¼ ëª©í‘œë¡œ í•˜ëŠ” í™•ë¥  ë¶„í¬ë¥¼ $ğ‘(ğ‘¥)$ë¼ê³  í•´ë³´ì.<br>ì´ë¥¼ ê·¼ì‚¬í•˜ê¸° ìœ„í•´ì„œ ëª¨ë¸ì„ í•™ìŠµì‹œì¼œì„œ í™•ë¥  ë¶„í¬ $ğ‘(ğ‘¥)$ë¥¼ ì–»ì–´ ëƒˆë‹¤ê³  ì¹˜ì.<br>ì´ë•Œ, $ğ‘(ğ‘¥)$ë¥¼ í†µí•´ì„œ ì–»ì–´ë‚¸ ì •ë³´ëŠ” ì›ë˜ $ğ‘(ğ‘¥)$ë¥¼ í†µí•´ì„œ ì–»ì„ ìˆ˜ ìˆëŠ” ì •ë³´ì™€ ìƒì´í•  ê²ƒì´ê³ , ìš°ë¦¬ëŠ” ì´ì— ì¶”ê°€ë¡œ í•„ìš”í•œ ì •ë³´ëŸ‰ì˜ í‰ê· ì„ ë‹¤ìŒê³¼ ê°™ì´ êµ¬í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.</p>
<script type="math/tex; mode=display">
KL(p||q) = - \int p(x)\ln q(x) dx - ( - \int p(x)\ln p(x)dx) \\
= - \int p(x)\ln q(x) - H_p[x]
\tag{definition 3}</script><p>ì, ê·¸ë ‡ë‹¤ë©´ ìš°ë¦¬ëŠ” ëŒ€ì¶© ê¶¤ê°€ ë³´ì¸ë‹¤. ìš°ë¦¬ê°€ ê²°êµ­ í•™ìŠµì‹œì¼œì•¼ í•˜ëŠ” ë¶„í¬ $ğ‘(ğ‘¥)$ì™€ ì •ë‹µì¸ ë¶„í¬ $ğ‘(ğ‘¥)$ì‚¬ì´ì˜ ì°¨ì´ë¥¼ ì •ëŸ‰í™” í•´ì£¼ëŠ” í•¨ìˆ˜ê°€ ë°”ë¡œ ì´ê²ƒì¸ ê²ƒì´ë‹¤.<br>ì´ë¥¼ â€œì¿¨ë°±-ë¼ì´ë¸ŒëŸ¬ë¦¬ ë°œì‚°â€ì´ë¼ê³  í•˜ë©°, ì´ë¥¼ í†µí•´ì„œ ìš°ë¦¬ëŠ” ë‘ í™•ë¥  ë¶„í¬ì˜ ì°¨ì´ë¥¼ ì•Œ ìˆ˜ ìˆëŠ”ë°,<br>ìì„¸íˆ ë³´ë‹ˆ, ë’¤ì˜ ëª©í‘œ ë¶„í¬ì˜ ì—”íŠ¸ë¡œí”¼ëŠ” ê·¸ëƒ¥ ìƒìˆ˜ë‚˜ ë‹¤ë¦„ì´ ì—†ë‹¤. ë”°ë¼ì„œ ì•ì˜ í•­ë§Œì„ ë”°ì„œ ë”°ë¡œ â€œCross-Entropyâ€ë¼ê³  í•˜ë©°, ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.</p>
<script type="math/tex; mode=display">
KL(p||q) = CE(p||q) - H_p[x]
\tag{definition 3}</script><h4 id="Gradient"><a href="#Gradient" class="headerlink" title="Gradient"></a>Gradient</h4><p>ì´ íŒŒíŠ¸ì—ì„œëŠ” ëŒ€í•™êµì—ì„œ Calculusë¥¼ ë°°ìš°ì§€ ì•Šì€ (ë°°ì› ë‹¤ í•˜ë”ë¼ë„ ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì˜ ë¯¸ë¶„ íŒŒíŠ¸ë¥¼ ì•ˆ ë“¤ì€ ì´ë“¤) ì‚¬ëŒë“¤ì´ ê¼­ ë³´ì•„ ì£¼ì—ˆìœ¼ë©´ í•œë‹¤. Gradientì˜ ê¸°ì´ˆ ì¤‘ì˜ ê¸°ì´ˆë¥¼ ë‹¤ë£° ì˜ˆì •ì´ë‹¤.</p>
<p>ìš°ë¦¬ëŠ” ì´ì „ ì‹œê°„ì— ê²°êµ­ì—ëŠ” ë¯¸ë¶„ì„ êµ¬í•´ì„œ ê°€ì¤‘ì¹˜ë¥¼ updateí•˜ëŠ” ê²ƒì´ë¼ê³  ë°°ì›Œì™”ë‹¤.<br>ê·¼ë°, ì´ë•Œ â€œGradientâ€ë¼ëŠ” ê²ƒì— ëŒ€í•œ ëª…í™•í•œ ì„¤ëª… ì—†ì´, ê·¸ëƒ¥ í•˜ë©´ ëœë‹¤ëŠ” ì‹ìœ¼ë¡œ ì§šê³  ë„˜ì–´ê°”ë˜ ê¸°ì–µì´ ë‚œë‹¤.</p>
<p>ìš°ì„ , í•¨ìˆ˜ì—ì„œ ëª‡ê°€ì§€ ì˜ˆì‹œë¶€í„° ìƒê°í•˜ê³  ë„˜ì–´ê°€ì.</p>
<ol>
<li>$f: \mathbb{R}^1 \rightarrow \mathbb{R}^1$</li>
<li>$f: \mathbb{R}^N \rightarrow \mathbb{R}^1$</li>
<li>$f: \mathbb{R}^1 \rightarrow \mathbb{R}^N$</li>
<li>$f: \mathbb{R}^N \rightarrow \mathbb{R}^M$</li>
</ol>
<p>ì´ë•Œ, $N,M \geq 2$ì´ë‹¤.</p>
<p>$\mathbb{R}^1$ì„ ë”°ë¡œ ë¶„ë¦¬í•œ ì´ìœ ê°€ ìˆë‹¤. í•¨ìˆ˜ì—ì„œ ì…/ì¶œë ¥ì´ ë²¡í„°(ë˜ëŠ” í–‰ë ¬)ì¸ ê²ƒê³¼ ì…/ì¶œë ¥ì´ Scalarì¸ ê²ƒì€ ë‹¤ì†Œ ìƒì´í•œ ê³¼ì •ì„ ë„ì…í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ìš°ë¦¬ëŠ” ì´ ì¤‘ì—ì„œë„ 2ë²ˆì„ íŠ¹íˆ ì¤‘ìš”í•˜ê²Œ ë‹¤ë£° ê²ƒì´ë‹¤.</p>
<p>ì´ë ‡ê²Œ í•¨ìˆ˜ëŠ” ì—¬ëŸ¬ê°€ì§€ í˜•íƒœê°€ ìˆì„ ìˆ˜ ìˆëŠ”ë° ì´ë•Œ, ê°ê° ì •ì˜ì—­ì— ëŒ€í•œ ë¯¸ë¶„ì´ ì–´ë–»ê²Œ ì •ì˜ë ê¹Œ?<br>ì´ íŒŒíŠ¸ì—ì„œëŠ” ê·¸ ì˜ˆì‹œë¥¼ ë“¤ì–´ë³¼ ê²ƒì´ë‹¤. </p>
<p>ìš°ì„ , $f: \mathbb{R}^N \rightarrow \mathbb{R}^1$ ì—ì„œì˜ ê²½ìš°ë¥¼ ë³´ì. ì¼ ë³€ìˆ˜ ìŠ¤ì¹¼ë¼ í•¨ìˆ˜ì˜ ê²½ìš°ëŠ” ë¹¼ê² ë‹¤. ê·¸ê±¸ ëª¨ë¥´ë©´ ì´ê±¸ ë“¤ì„ ìê²©ì´ ì—†ë‹¤.</p>
<p>ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ í•¨ìˆ˜ë¥¼ vector-scalarí•¨ìˆ˜ë¼ê³  ë¶€ë¥´ê² ë‹¤.<br>ì´ëŸ¬í•œ í•¨ìˆ˜ì˜ ë¯¸ë¶„ì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆë‹¤. </p>
<script type="math/tex; mode=display">
\textbf{x} \in \mathbb{R}^N, y \in \mathbb{R}^1</script><script type="math/tex; mode=display">
\textbf{x} = [x_1, x_2, ..., x_N], y = f(\textbf{x})</script><p>ì´ë ‡ê²Œ ì •ì˜ë˜ì–´ ìˆì„ ë•Œ, í•¨ìˆ˜ $f$ì— ëŒ€í•œ GradientëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ë˜ê³  ì •ì˜ëœë‹¤.</p>
<script type="math/tex; mode=display">
\nabla_{\textbf{x}} f(\textbf{x}) = [\frac{\partial y}{\partial x_1}, \frac{\partial y}{\partial x_2}, ...,\frac{\partial y}{\partial x_N}]</script><p>ì—¬ê¸°ì„œ ë§Œì•½ ì…ë ¥ì´ í–‰ë ¬ì´ë©´ ì–´ë–»ê²Œ ë˜ì–´ì•¼ í• ê¹Œ?<br>ì¦‰, ë‹¤ìŒê³¼ ê°™ì´ ì¶œë ¥ì´ Scalarì´ê³  ì…ë ¥ì´ ë²¡í„°ì¸ í•¨ìˆ˜ì´ë‹¤. (Matrix-Scalarí•¨ìˆ˜)</p>
<script type="math/tex; mode=display">
\textbf{x} \in \mathbb{R}^{N*M}, y \in \mathbb{R}^1</script><script type="math/tex; mode=display">
\nabla_{\textbf{x}} f(\textbf{x}) = 
\begin{bmatrix}
    \frac{\partial y}{\partial x_{11}} \cdots \frac{\partial y}{\partial x_{1M}} \\
    \vdots \space \space \space \ddots \space \space \space \vdots \\
    \frac{\partial y}{\partial x_{N1}} \cdots \frac{\partial y}{\partial x_{NM}}
\end{bmatrix}</script><h3 id="Loss-Function-Examples"><a href="#Loss-Function-Examples" class="headerlink" title="Loss Function Examples"></a>Loss Function Examples</h3><hr>
<p>ì´ íŒŒíŠ¸ì—ì„œëŠ” ìœ„ì—ì„œ ë°°ìš´ ìˆ˜í•™ì ì¸ ê¸°ì´ˆë¥¼ í† ëŒ€ë¡œ Loss functionì˜ ì˜ˆì‹œë¥¼ í•œë²ˆ ë³¼ ê²ƒì´ë‹¤. </p>
<p>ìš°ì„  MSEë¶€í„° ìƒê°í•´ ë³´ì. ê²°êµ­ MSEëŠ” MLE/MAPë¥¼ ì´ë¡ ì ì¸ ê¸°ì €ë¡œ ë‘ê³  ìˆì—ˆë‹¤ëŠ” ê²ƒì„ ìœ„ ê¸€ì„ ì½ì—ˆë‹¤ë©´ ì´í•´í•  ìˆ˜ ìˆì—ˆì„ ê²ƒì´ë‹¤. $equation 5$ë¥¼ ë‹¤ì‹œ í•œë²ˆ ë´ë³´ì.</p>
<p>ê·¸ë ‡ë‹¤ë©´ Cross-Entropyë¥¼ ì–´ë–»ê²Œ ì‚¬ìš©í•  ê²ƒì¸ê°€?<br>ì‹ ê²½ë§ì—ì„œëŠ” ì´ë¥¼ ë¶„ë¥˜ ë¬¸ì œë¥¼ ì–´ë–»ê²Œ í’€ê¹Œ? ìš°ë¦¬ëŠ” Softmax í•¨ìˆ˜ë¥¼ ê°™ì´ ì‚¬ìš©í•˜ì—¬ ì´ë¥¼ í•´ê²°í•  ìˆ˜ ìˆë‹¤.</p>
<p>ë¨¼ì € softmax í•¨ìˆ˜ë¶€í„° ìƒê°í•´ ë³´ì.</p>
<script type="math/tex; mode=display">
y_i = \frac{e^{u^K_i}}{\sum^{n_{out}}_{j = 1}e^{u^K_j}}</script><p>ìœ„ í•¨ìˆ˜ë¥¼ Neural Networkì—ì„œ output layerì˜ ê° ì¶œë ¥ ê°’ì„ í™•ë¥  ë¶„í¬ë¡œ ë°”ê¾¸ì–´ ì¤€ë‹¤ê³  ìƒê°í•˜ë©´ ëœë‹¤. ì‹¤ì œë¡œ ëª¨ë“  unitì˜ ê°’ì„ ë‹¤ ë”í•˜ë©´ 1ì´ ë˜ë‹ˆê¹ ë§ì´ë‹¤.</p>
<p>ì´ì œ ì´ëŸ¬í•œ í™•ë¥  ë¶„í¬ë¥¼ í† ëŒ€ë¡œ ì‹¤ì œ ìš°ë¦¬ê°€ ì›í•˜ëŠ” target í™•ë¥  ë¶„í¬ì™€ì˜ ê±°ë¦¬ë¥¼ êµ¬í•˜ëŠ” ê³¼ì •ì„ Cross entropyë¡œ ì§„í–‰í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. ë‹¤ìŒê³¼ ê°™ì´ ë§ì´ë‹¤.</p>
<script type="math/tex; mode=display">
E = -\sum^{n_{out}}_{i = 1} t_i \ln y_i</script><p>ì´ë•Œ, target ë¶„í¬ëŠ” classì— ë”°ë¼ one-hot encodingì´ ë˜ì–´ ìˆìŒ<br>ì´ë•Œ, ì´ í•¨ìˆ˜ë¥¼ ë¯¸ë¶„í•˜ë©´ ì–´ë–»ê²Œ ë ê¹Œ? ë‹¤ìŒì„ í•œë²ˆ ë³´ì.</p>
<p align="center"><img src="https://kimh060612.github.io/img/CE_Loss.png" width="100%"></p>

<p>ê²°êµ­ í˜•íƒœê°€ One hot encodingì´ ë˜ì–´ ìˆë‹¤ë©´, MSEì™€ ë”±íˆ ë‹¤ë¥¼ ê²ƒì´ ì—†ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.</p>
<h3 id="What-is-Optimizer"><a href="#What-is-Optimizer" class="headerlink" title="What is Optimizer?"></a>What is Optimizer?</h3><hr>
<p>ì´ íŒŒíŠ¸ì—ì„œëŠ” ìœ„ì—ì„œ ë°°ìš´ ìˆ˜í•™ì  ê¸°ì´ˆë¥¼ í† ëŒ€ë¡œ Optimizerê°€ ì–´ë–»ê²Œ ë™ì‘ì„ í•˜ëŠ” ë†ˆë“¤ì¸ì§€ ë°°ì›Œ ë³´ë„ë¡ í•˜ê² ë‹¤.</p>
<p>ê·¸ë˜ì„œ Deep learningì—ì„œì˜ OptimizerëŠ” ì–´ë–¤ ì—­í• ì¸ì§€ ì•Œì•„ë³´ì. ê²°êµ­ Deep learningì€ í•´ë‹¹ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ê²ƒì— ì¤‘ì‹¬ì„ ë‘ê³  ìˆë‹¤. ì´ë•Œ, ìš°ë¦¬ëŠ” íŠ¹ì •í•œ ë¬¸ì œë¥¼ í•´ê²°í•œë‹¤ê³  í–ˆì„ ë•Œ, íŠ¹ì • ì§€í‘œ(ex. ì •í™•ë„)ë¥¼ ìµœëŒ€í™” í•œë‹¤ëŠ” ê²ƒì´ë‹¤.<br>ìš°ë¦¬ëŠ” ì´ë•Œ ì •í™•ë„ë¥¼ ìµœëŒ€í™” í•˜ê¸° ìœ„í•´ì„œ ë°ì´í„°ì—ì„œ ì–»ì„ ìˆ˜ ìˆëŠ” ì†ì‹¤(Loss)ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒì„ ì§„í–‰í•œë‹¤. ì´ëŠ” ê¸°ì¡´ ìµœì í™”ì™€ëŠ” ì¡°ê¸ˆ ë‹¤ë¥´ë‹¤. ì§ì ‘ ì§€í‘œë¥¼ ìµœëŒ€í™”í•˜ëŠ” ê²ƒì´ ì•„ë‹Œ, ê°„ì ‘ì ì¸ ì§€í‘œë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ê°€ëŠ” ê²ƒì´ë‹ˆ ë§ì´ë‹¤. </p>
<p>ê·¸ë˜ì„œ, ê²°êµ­ Optimizerì˜ ì—­í• ì´ ë¬´ì—‡ì¸ê°€?<br>ë‹¤ìŒê³¼ ê°™ë‹¤.</p>
<script type="math/tex; mode=display">
\min_w \mathbf{E}_{x,y \sim \hat{P}(x,y)} [L(f(x^i;w), y^i)]</script><p>ì´ê²Œ ë­ëƒ? ì¦‰ ë°ì´í„°ì— ëŒ€í•œ ì†ì‹¤ì„ ìµœì†Œí™” í•˜ëŠ” ê²ƒì´ë‹¤. ì´ê²ƒì´ Optimizer ì˜ ê¶ê·¹ì ì¸ ì—­í• ì¸ ê²ƒì´ë‹¤.<br>ì´ ë¬¸ì œëŠ” 1ì¤„ë§Œ ìƒê°í•´ ë³´ë©´ ê°„ë‹¨í•´ ë³´ì´ì§€ë§Œ ì‹¤ì€ ê²ë‚˜ê²Œ ì–´ë µê³  ë³µì¡í•œ ë¬¸ì œì´ë‹¤.<br>ì´ ë¬¸ì œë¥¼ í’€ê¸° ìœ„í•´ì„œ ìš°ë¦¬ëŠ” Gradient Descentë¼ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤. </p>
<p>ì—¬ê¸°ì„œ Gradient Descentì˜ ìˆ˜ë ´ì„±ì„ ì„¤ëª…í•˜ê³  ì‹¶ì§€ë§Œâ€¦. ê°•ì˜ì—ì„œëŠ” ìƒëµí•˜ë„ë¡ í•˜ê² ë‹¤. ë” ìì„¸íˆ ì•Œê³  ì‹¶ìœ¼ë©´ Talyor ê¸‰ìˆ˜ë¥¼ í‚¤ì›Œë“œë¡œ ì˜ ì°¾ì•„ë³´ê¸°ë¥¼ ë°”ë€ë‹¤. ë§Œì•½ ë‚˜ì¤‘ì— ì‹œê°„ì´ ëœë‹¤ë©´ ë‹¤ì‹œ ë‹¤ë£¨ì–´ ë³´ë„ë¡ í•˜ê² ë‹¤.</p>
<p>ì´ì¯¤ì—ì„œ ë¯¸ë‹ˆ ë°°ì¹˜ì˜ ì˜ë¯¸ë¥¼ ì„¤ëª…í•˜ê³  ê°€ë„ë¡ í•˜ê² ë‹¤.<br>ì´ ê²ƒì€ ë‹¤ìŒ ê·¸ë¦¼ìœ¼ë¡œ ì„¤ëª…ì´ ë  ìˆ˜ ìˆë‹¤.</p>
<p align="center"><img src="https://kimh060612.github.io/img/MiniBatch.png" width="100%"></p>

<p>~ì—­ì‹œ í•„ìëŠ” í•„ì ìŠ¤ìŠ¤ë¡œ ìƒê°í•´ë„ ë°œ ê·¸ë¦¼ì´ë‹¤. ë‹¹ë„ì·Œ ì–´ë–¤ ì •ì‹ ë¨¸ë¦¬ë¡œ ì´ë”´ê±¸ ê·¸ë¦¬ëŠ”ì§€ ëª¨ë¥´ê² ë‹¤.~</p>
<p>ì ê°ì„¤í•˜ê³ , ì „ì²´ ë°ì´í„°ì—ì„œ ì •í•´ì§„ Batch Size ë§Œí¼ì„ ë½‘ì•„ì„œ ë§Œë“  ë°ì´í„°ë¥¼ Mini Batchë¼ê³  í•œë‹¤. ì´ë¥¼ ì‹ ê²½ë§ì— ë„£ì–´ì„œ Feed Forwardë¥¼ í•˜ê³  Back propagation ì—°ì‚°ì„ í•˜ë©´ì„œ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤. ì´ê²ƒì´ í•™ìŠµì˜ 1ê°œì˜ stepì´ë‹¤. ê·¸ë¦¬ê³  ì´ mini batch setìœ¼ë¡œ ì „ë¶€ì˜ ë°ì´í„°ë¥¼ í•™ìŠµ í–ˆì„ë•Œ, ê·¸ê²ƒì„ 1ê°œì˜ epochì´ë¼ê³  í•œë‹¤. (í†µìƒì ìœ¼ë¡œ) ì´ë ‡ê²Œë§Œ ê°„ë‹¨í•˜ê²Œ ìƒê°í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” Optimizer ë³„ë¡œ ë”°ë¡œ ìƒê°í•˜ë©´ í¸í•  ê²ƒì´ë‹¤.</p>
<h3 id="Optimizer-examples"><a href="#Optimizer-examples" class="headerlink" title="Optimizer examples"></a>Optimizer examples</h3><hr>
<p>ì´ íŒŒíŠ¸ì—ì„œëŠ” Optimizerì˜ ì¢…ë¥˜ê°€ ì–´ë–¤ ê²ƒë“¤ì´ ìˆëŠ”ì§€ ì•Œì•„ë³´ë„ë¡ í•˜ê² ë‹¤.</p>
<ol>
<li>Stochastic Gradient Descent</li>
</ol>
<p>ì´ OptimizerëŠ” ê°€ì¥ ê°„ë‹¨í•œ Optimizerë¼ê³  ìƒê°í•´ë„ ëœë‹¤. ìœ„ì—ì„œ ì„¤ëª…í•œ ëŒ€ë¡œ, ê°„ë‹¨í•˜ê²Œ Mini Batchë¥¼ ë½‘ì•„ì„œ Updateë¥¼ ì§„í–‰í•œë‹¤.</p>
<p align="center"><img src="https://kimh060612.github.io/img/SGD.png" width="100%"></p>

<ol>
<li>Adagrad</li>
</ol>
<p>ì´ ë°©ë²•ì€ ì´ì „ì— ì‚¬ìš©í–ˆë˜ gradientë“¤ì„ ì¶•ì í•´ì„œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì´ë‹¤. gradientë¥¼ ì›ì†Œë³„ë¡œ ì œê³±í•´ì„œ stepë³„ë¡œ ë”í•´ê°€ëŠ” í–‰ë ¬ì„ í•˜ë‚˜ ë§Œë“¤ê³ , ê·¸ í–‰ë ¬ì„ updateí• ë•Œ gradientì— ì›ì†Œë³„ë¡œ ê³±í•´ì£¼ëŠ” ê²ƒì´ë‹¤. ì œëª© ê·¸ëŒ€ë¡œ Adaptive gradient ë°©ë²•ì¸ ê²ƒì´ë‹¤.</p>
<p align="center"><img src="https://kimh060612.github.io/img/Adagrad.png" width="100%"></p>

<ol>
<li>RMSProp</li>
</ol>
<p>ì´ ë°©ë²•ì€ ìœ„ì˜ Adagrad ë°©ë²•ì—ì„œ ì¡°ê¸ˆ ë” ë‚˜ì•„ê°€, gradientë¥¼ ë”í•´ë‚˜ê°ˆë•Œ ê°€ì¤‘ì¹˜ë¥¼ ë‘ëŠ” ë°©ë²•ì´ë‹¤. </p>
<p align="center"><img src="https://kimh060612.github.io/img/RMSProp.png" width="100%"></p>

<ol>
<li>Adam</li>
</ol>
<p>í˜„ì¬ ê°€ì¥ ë§ì´ë“¤ ì“°ëŠ” Optimizerì´ë‹¤. ì´ê²ƒë„ ë³„ê±° ì—†ë‹¤. RMSProp ì²˜ëŸ¼ ê°€ì¤‘í•©ì„ ì§„í–‰í•˜ëŠ”ë°, ì´ë²ˆì—ëŠ” ì›ì†Œë³„ë¡œ ì œê³±í•˜ì§€ ì•Šì€ gradientë„ ëˆ„ì  í•©ì„ ì €ì¥í•´ì„œ ì‚¬ìš©í•œë‹¤. ìì„¸í•œ ê²ƒì„ ìˆ˜ì‹ì„ ë³´ë©´ ë°”ë¡œ ê°ì´ ì˜¬ ê²ƒì´ë‹¤.</p>
<p align="center"><img src="https://kimh060612.github.io/img/Adam.png" width="100%"></p>

<p>ì´ê±¸ë¡œ Optimization íŒŒíŠ¸ AëŠ” ëì´ ë‚¬ë‹¤. ë‹¤ìŒ íŒŒíŠ¸ëŠ” ìœ„ì—ì„œ ë°°ìš´ ê²ƒë“¤ì„ êµ¬í˜„í•˜ë©´ì„œ ì°¾ì•„ ì˜¤ë„ë¡ í•  ê²ƒì´ë‹¤.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kimh060612.github.io/2022/03/05/RNN-A/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Michael Kim">
      <meta itemprop="description" content="Backend Engineering, Deep learning, Algorithm & DS, CS ì „ê³µ ì§€ì‹ ê³µë¶€ ì €ì¥ì†Œ">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Michael's Study Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/05/RNN-A/" class="post-title-link" itemprop="url">Recurrent Neural Network ê°•ì˜ ë‚´ìš© part.A</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-03-05 19:26:50" itemprop="dateCreated datePublished" datetime="2022-03-05T19:26:50+09:00">2022-03-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-05-30 00:00:32" itemprop="dateModified" datetime="2022-05-30T00:00:32+09:00">2022-05-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2022/03/05/RNN-A/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2022/03/05/RNN-A/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><em>ë³¸ í¬ìŠ¤íŠ¸ëŠ” Hands-on Machine learning 2nd Edition, CS231n, Tensorflow ê³µì‹ documentë¥¼ ì°¸ì¡°í•˜ì—¬ ì‘ì„±ë˜ì—ˆìŒì„ ì•Œë¦½ë‹ˆë‹¤.</em></p>
<h3 id="Index"><a href="#Index" class="headerlink" title="Index"></a>Index</h3><ol>
<li>Definition of Recurrent Neural Network(RNN)</li>
<li>Back Propagation of RNN</li>
<li>Partice</li>
</ol>
<p>ì—¬ê¸°ì„œëŠ” 1,2ëŠ” Part. Aì´ê³  3ì€ Part. Bì—ì„œ ë‹¤ë£¨ë„ë¡ í•˜ê² ë‹¤.<br><br></p>
<h3 id="Definition-of-Recurrent-Neural-Network-RNN"><a href="#Definition-of-Recurrent-Neural-Network-RNN" class="headerlink" title="Definition of Recurrent Neural Network(RNN)"></a>Definition of Recurrent Neural Network(RNN)</h3><hr>
<p>RNN(Recurrent Neural Network)ì€ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ”ë° íŠ¹í™”ëœ ì‹ ê²½ë§ êµ¬ì¡°ì´ë‹¤.<br>ì´ëŠ” ì „ì˜ ì…ë ¥ì´ ì—°ì†í•´ì„œ ë‹¤ìŒ ì…ë ¥ì— ì˜í–¥ì„ ì£¼ëŠ” ì‹ ê²½ë§ êµ¬ì¡°ë¡œì¨, ê°™ì€ êµ¬ì¡°ê°€ ê³„ì† ìˆœí™˜ë˜ì–´ ë‚˜íƒ€ë‚˜ê¸° ë•Œë¬¸ì— ì´ëŸ¬í•œ ì´ë¦„ì´ ë¶™ì–´ì ¸ ìˆë‹¤.<br>ë‹¤ìŒ ê·¸ë¦¼ì„ ë³´ì. </p>
<p align="center"><img src="https://kimh060612.github.io/img/RNN.png" width="100%"></p>

<p>í•˜ì§€ë§Œ ì´ ê·¸ë¦¼ìœ¼ë¡œëŠ” ìì„¸í•œ êµ¬ì¡°ê¹Œì§€ëŠ” ì˜ ëª¨ë¥´ê² ë‹¤. ê·¸ë˜ì„œ í•„ìê°€ ì§ì ‘ ê·¸ë¦° ê·¸ë¦¼ì„ ë³´ë©´ì„œ ì„¤ëª…í•˜ë„ë¡ í•˜ê² ë‹¤. ë§í•´ë‘ê² ì§€ë§Œ í•„ìëŠ” ê·¸ë¦¼ì„ ì‹¬ê°í•˜ê²Œ ëª» ê·¸ë¦¬ë‹ˆ ì–‘í•´ ë°”ë€ë‹¤.</p>
<p align="center"><img src="https://kimh060612.github.io/img/RNND.png" width="100%"></p>

<p>ì´ì²˜ëŸ¼ ê° RNN Cellì— FCNNì˜ Unitì´ ë“¤ì–´ê°€ ìˆëŠ” í˜•ì‹ìœ¼ë¡œ êµ¬í˜„ëœë‹¤. ì´ì˜ Feed Forward ì—°ì‚°ì„ ìƒê°í•´ ë³´ë©´ ì •ë§ ê°„ë‹¨í•˜ë‹¤.</p>
<script type="math/tex; mode=display">
z_t = f(z_{t - 1}W_{re} + x_tW_{in} + b)
\tag{equation 1}</script><p>$equation\space 1$ì—ì„œ ê° ë³€ìˆ˜ë“¤ì˜ ì •ì˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.</p>
<blockquote>
<p>Definition 1</p>
</blockquote>
<ul>
<li>$z_t$: time step $t$ì— unitì˜ ê°’ì— activation functionì— ë„£ì€ ê°’</li>
<li>$W_{re}$: Reccurent ê°€ì¤‘ì¹˜</li>
<li>$W_{in}$: input layerì—ì„œì˜ ê°€ì¤‘ì¹˜</li>
<li>$x_t$: input vector</li>
</ul>
<p>ê·¸ë¦¼ìœ¼ë¡œ ì •ë¦¬í•˜ë©´ ëŒ€ì¶© ë‹¤ìŒê³¼ ê°™ë‹¤.</p>
<p align="center"><img src="https://kimh060612.github.io/img/RNND2.png" width="100%"></p>

<p>ê²°êµ­ ì´ë ‡ê²Œ Sequentialí•œ ë°ì´í„° $x$ì— ëŒ€í•´ì„œ Sequentialí•œ output $y$ë¥¼ ë½‘ì„ ìˆ˜ ìˆëŠ” ê²ƒì´ë‹¤.</p>
<h3 id="Back-Propagation-of-RNN"><a href="#Back-Propagation-of-RNN" class="headerlink" title="Back Propagation of RNN"></a>Back Propagation of RNN</h3><hr>
<p>RNNì˜ Back Propagation ë°©ë²•ì€ ë‹¤ìŒì˜ 2ê°€ì§€ê°€ ìˆë‹¤. </p>
<ul>
<li>Back Propagation Through Time - (BPTT)</li>
<li>Real Time Recurrent Learning (RTRL)</li>
</ul>
<p>ì—¬ê¸°ì„œëŠ” BPTTë§Œì„ ë‹¤ë£¨ë„ë¡ í•˜ê² ë‹¤. RNNì„ ê·¸ë ‡ê²Œ ìì„¸í•˜ê²Œ ë‹¤ë£¨ì§€ ì•ŠëŠ” ì´ìœ ëŠ” FCNNì˜ ì—°ì¥ì„  ëŠë‚Œì´ ê°•í•´ì„œì´ê³  ë˜í•œ í˜„ëŒ€ì—ì„œëŠ” ë”±íˆ ì˜ ì“°ì´ì§€ ì•Šê¸° ë•Œë¬¸ì´ë‹¤. ê·¸ëƒ¥ ì§€ì  ìœ í¬ë¥¼ ìœ„í•´ì„œ ë˜ëŠ” ê¸°ë³¸ê¸°ë¥¼ ì˜ ë‹¦ê¸° ìœ„í•´ì„œë¡œë§Œ ì½ì–´ì£¼ê¸°ë¥¼ ë°”ë€ë‹¤.</p>
<p>ë˜ëŠ” AutoGrad ê³„ì—´ì˜ ì•Œê³ ë¦¬ì¦˜ì„ ê³µë¶€í•˜ëŠ” ì‚¬ëŒë“¤ì€ ë¯¸ë¶„ ê·¸ë˜í”„ë¥¼ ì‚¬ìš©í•œ AutoGradì´ì „ì—ëŠ” ì´ëŸ° ì‹ìœ¼ë¡œ ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í–ˆì—ˆêµ¬ë‚˜ ë¼ê³  ì—­ì‚¬ì±… ì½ëŠ” ëŠë‚Œìœ¼ë¡œ ì½ì–´ì£¼ë©´ ë§¤ìš° ê°ì‚¬í•˜ê² ë‹¤. </p>
<p>ì´ëŸ¬í•œ RNNì˜ Back Propagationì„ ì§„í–‰í•˜ê¸° ìœ„í•´ì„œëŠ” ë‹¤ìŒì„ êµ¬í•˜ë©´ ëœë‹¤.</p>
<script type="math/tex; mode=display">
\begin{aligned}
    \frac{\partial E}{\partial w^{out}_{ij}} =\space ? \\
    \frac{\partial E}{\partial w^{re}_{ij}} = \space ? \\
    \frac{\partial E}{\partial w^{in}_{ij}} = \space?
\end{aligned}
\tag{definition 2}</script><p>ì´ì œ ìœ„ ë¯¸ë¶„ë“¤ì— chain ruleì„ ì ìš©í•´ ë³´ì. ê·¸ë ‡ë‹¤ë©´ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial w^{out}_{ij}} = \sum^T_{t = 1} \frac{\partial E}{\partial v^{out}_{i}} \frac{\partial v^{out}_{i}}{\partial w^{out}_{ij}}
\tag{equation 2}</script><script type="math/tex; mode=display">
\frac{\partial E}{\partial w^{re}_{ij}} = \sum^T_{t = 1} \frac{\partial E}{\partial u^{t + 1}_{i}} \frac{\partial u^{t + 1}_{i}}{\partial w^{re}_{ij}}
\tag{equation 3}</script><script type="math/tex; mode=display">
\frac{\partial E}{\partial w^{in}_{ij}} = \sum^T_{t = 1} \frac{\partial E}{\partial u^{in}_{i}} \frac{\partial u^{in}_{i}}{\partial w^{out}_{ij}}
\tag{equation 4}</script><p>ì, ê·¸ë ‡ë‹¤ë©´ ì—¬ê¸°ì„œ deltaë¥¼ ì •ì˜í•´ì„œ ì¼ë°˜í™”ëœ delta ê·œì¹™ì„ ì ìš©í•´ ë³´ì•„ì•¼ Back Propagationì´ íš¨ìœ¨ì ìœ¼ë¡œ ë  ê²ƒì´ë‹¤. </p>
<p>ê·¸ë ‡ë‹¤ë©´ ê° ë¯¸ë¶„ë“¤ì— ëŒ€í•œ deltaëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ë  ìˆ˜ ìˆë‹¤.</p>
<script type="math/tex; mode=display">
\delta^{out}_{k,t} = \frac{\partial E}{\partial v^t_k} = \frac{\partial E}{\partial y^t_k} \frac{\partial y^t_k}{\partial v^t_k} = \frac{\partial E}{\partial y^t_k} f'(v^t_k)
\tag{definition 3}</script><p>input layerì—ì„œì˜ ê°€ì¤‘ì¹˜ëŠ” output layerì²˜ëŸ¼ FCNNê³¼ ì™„ì „íˆ ê°™ë‹¤. êµ³ì´ ì ì–´ë‘ì§€ëŠ” ì•Šë„ë¡ í•˜ê² ë‹¤. ê·¸ë ‡ë‹¤ë©´ ë‚¨ì€ ê²ƒì€ recurrent layerì—ì„œì˜ deltaì´ë‹¤. ì „ê°œí•´ë³´ë©´ ëŒ€ëµ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.</p>
<p align="center"><img src="https://kimh060612.github.io/img/RNNDBPTT.png" width="100%"></p>

<p>ì´ë¥¼ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.</p>
<script type="math/tex; mode=display">
\delta^{re}_{j,t} = \sum^{n_{out}}_{i = 1} \frac{\partial E}{\partial v^t_i} \frac{\partial v^t_i}{\partial u^t_j} + \sum^{n_{re}}_{i = 1} \frac{\partial E}{\partial u^{t + 1}_i} \frac{\partial u^{t + 1}_i}{\partial u^t_j}
\tag{definition 4}</script><p>ê° Summation Termë“¤ì˜ deltaë¥¼ ì´ìš©í•´ì„œ í‘œí˜„í•´ ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>
<script type="math/tex; mode=display">
\delta^{re}_{j,t} = \sum^{n_{out}}_{i = 1} \delta^{out}_{i,t} \frac{\partial v^t_i}{\partial u^t_j} + \sum^{n_{re}}_{i = 1} \delta^{re}_{i, t} \frac{\partial u^{t + 1}_i}{\partial u^t_j}  \\ 
= \sum^{n_{out}}_{i = 1} \delta^{out}_{i,t} w^{out}_{ij} f'(u^t_j) + \sum^{n_{re}}_{i = 1} \delta^{re}_{i, t} w^{re}_{ij} f'(u^t_j)
\tag{equation 4}</script><p>ì¦‰, ì´ì™€ ê°™ì´ RNN Cellì˜ ê¸°ë³¸ í˜•íƒœì˜ BPTTëŠ” ì •ë§ ë‹¨ìˆœí•˜ê²Œë„ FCNNì˜ ì—°ì¥ì„ ì´ë‹¤. ë³„ê²Œ ì—†ë‹¤.</p>
<p>ê·¸ë¦¬ê³  ì´ì˜ ë³€í˜•íŒìœ¼ë¡œ ì‹œê°„ì„ ë¶„í• í•´ì„œ Updateí•˜ëŠ” ë°©ë²•ë„ ìˆë‹¤. ì´ë¥¼ Truncated BPTTë¼ê³  í•˜ëŠ”ë° ê´€ì‹¬ì´ ìˆë‹¤ë©´ ì°¾ì•„ë³´ë„ë¡ í•˜ì. ì´ê²ƒë„ ì§„ì§œ ë³„ê±° ì—†ë‹¤.</p>
<p>ê·¸ì € ìœ„ì˜ ë¯¸ë¶„ì—ì„œ ì‹œê°„ termì„ ì˜ë¼ì„œ update í•´ì£¼ë©´ ëœë‹¤.</p>
<p>ì´ê±¸ë¡œ RNN ë˜í•œ ëì´ ë‚¬ë‹¤. ë‹¤ìŒ íŒŒíŠ¸ì—ì„œëŠ” ì´ë¥¼ êµ¬í˜„í•´ ë³´ë„ë¡ í•˜ê² ë‹¤.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kimh060612.github.io/2022/03/05/RNN-B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Michael Kim">
      <meta itemprop="description" content="Backend Engineering, Deep learning, Algorithm & DS, CS ì „ê³µ ì§€ì‹ ê³µë¶€ ì €ì¥ì†Œ">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Michael's Study Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/05/RNN-B/" class="post-title-link" itemprop="url">Recurrent Neural Network ê°•ì˜ ë‚´ìš© part.B</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-03-05 19:26:47" itemprop="dateCreated datePublished" datetime="2022-03-05T19:26:47+09:00">2022-03-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-05-30 00:00:32" itemprop="dateModified" datetime="2022-05-30T00:00:32+09:00">2022-05-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2022/03/05/RNN-B/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2022/03/05/RNN-B/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><em>ë³¸ í¬ìŠ¤íŠ¸ëŠ” Hands-on Machine learning 2nd Edition, CS231n, Tensorflow ê³µì‹ documentë¥¼ ì°¸ì¡°í•˜ì—¬ ì‘ì„±ë˜ì—ˆìŒì„ ì•Œë¦½ë‹ˆë‹¤.</em></p>
<h3 id="Index"><a href="#Index" class="headerlink" title="Index"></a>Index</h3><ol>
<li><del>Definition of Recurrent Neural Network(RNN)</del></li>
<li><del>Back Propagation of RNN</del></li>
<li>Partice</li>
</ol>
<p><br></p>
<h3 id="Partice"><a href="#Partice" class="headerlink" title="Partice"></a>Partice</h3><hr>
<p>ì§„ì§œ ì—¬ê¸°ê¹Œì§€ ê³¼ì—° ì½ì€ ì‚¬ëŒì´ ìˆì„ê¹Œ ì‹¶ë‹¤. ìˆë‹¤ë©´ ì••ë„ì  ê°ì‚¬ì˜ ì˜ë¯¸ë¡œ ê·¸ëœì ˆì„ ë°•ê³  ì‹¶ì€ ë§ˆìŒì´ë‹¤. ã…‹ã…‹ </p>
<p>ì¥ë‚œì€ ê·¸ë§Œí•˜ê³  ì˜¤ëŠ˜ë„ ì‹œì‘í•˜ì. ì˜¤ëŠ˜ë„ ì—­ì‹œ Model Subclassing APIë¥¼ í™œìš©í•˜ì—¬ ê°„ë‹¨í•˜ê²Œ RNNì„ í™œìš©í•˜ëŠ” ì˜ˆì œë¥¼ êµ¬í˜„í•´ë³¼ ê²ƒì´ë‹¤.</p>
<p>ìš°ì„  RNNì„ tensorflowì—ì„œ ì–´ë–»ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ”ì§€ë¶€í„° ë³´ì.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">RNNLayer</span>(keras.Model):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_hidden=<span class="number">128</span>, num_class=<span class="number">39</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(RNNLayer, self).__init__()</span><br><span class="line">        self.RNN1 = keras.layers.SimpleRNN(num_hidden, activation=<span class="string">&#x27;tanh&#x27;</span>, return_sequences=<span class="literal">True</span>)</span><br><span class="line">        self.out = keras.layers.Dense(num_class, activation=<span class="string">&quot;softmax&quot;</span>)</span><br><span class="line">        self.out = keras.layers.TimeDistributed(self.out)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.RNN1(x)</span><br><span class="line">        out = self.out(x)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<p>ë³´ë©´ ì•Œë‹¤ì‹œí”¼ ì•„ì£¼ ê°„ë‹¨í•˜ë‹¤. ê·¸ë˜ì„œ ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” ê°„ë‹¨í•˜ê²Œ 2ê°œì˜ ê´€ì „ í¬ì¸íŠ¸ë§Œì„ ë‹¤ë£¨ë ¤ê³  í•œë‹¤.</p>
<ol>
<li>SimpleRNN layerì˜ íŠ¹ì§•</li>
<li>TimeDistributed layerì˜ íŠ¹ì§•</li>
</ol>
<p>1ë²ˆë¶€í„° ì°¨ë¡€ë¡œ ì‹œì‘í•˜ì.</p>
<p>ìš°ì„  SimpleRNNì„ ì´í•´í•˜ë ¤ë©´ ì…ë ¥ìœ¼ë¡œëŠ” ì–´ë–¤ tensorë¥¼ ë°›ì•„ë¨¹ì–´ì„œ ì¶œë ¥ìœ¼ë¡œëŠ” ì–´ë–¤ tensorë¥¼ ë±‰ì–´ë‚´ëŠ”ì§€ë¥¼ ì•Œì•„ì•¼ í•œë‹¤.<br>ì§€ë‚œ í¬ìŠ¤íŠ¸ì—ì„œ RNNì˜ êµ¬ì¡°ë¥¼ ë³´ì•˜ë‹¤ë©´ ë‹¹ì—°íˆ ì…ë ¥ì€ ì‹œê°„ìˆœì„œëŒ€ë¡œ ë²¡í„°ê°€ ë“¤ì–´ê°€ë‹ˆ ì ì–´ë„ 3ì°¨ì›(ë°°ì¹˜ê¹Œì§€ í¬í•¨í•´ì„œ) ì¼ ê²ƒì´ê³  ì¶œë ¥ë„ ì‹œê°„ ìˆœì„œëŒ€ë¡œ ë‚˜ì™€ì•¼ í•˜ë‹ˆ ê°™ì€ 3ì°¨ì›ì´ë¼ëŠ” ê²ƒ ì¯¤ì€ ìœ ì¶”ê°€ ê°€ëŠ¥í•  ê²ƒì´ë‹¤. í•˜ì§€ë§Œ ì‹¤ì œë¡œëŠ” ì´ê²ƒ ë˜í•œ ì¡°ì ˆì´ ê°€ëŠ¥í•˜ë‹¤.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.SimpleRNN(</span><br><span class="line">    units, activation=<span class="string">&#x27;tanh&#x27;</span>, use_bias=<span class="literal">True</span>,</span><br><span class="line">    kernel_initializer=<span class="string">&#x27;glorot_uniform&#x27;</span>,</span><br><span class="line">    recurrent_initializer=<span class="string">&#x27;orthogonal&#x27;</span>,</span><br><span class="line">    bias_initializer=<span class="string">&#x27;zeros&#x27;</span>, kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">    recurrent_regularizer=<span class="literal">None</span>, bias_regularizer=<span class="literal">None</span>, activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">    kernel_constraint=<span class="literal">None</span>, recurrent_constraint=<span class="literal">None</span>, bias_constraint=<span class="literal">None</span>,</span><br><span class="line">    dropout=<span class="number">0.0</span>, recurrent_dropout=<span class="number">0.0</span>, return_sequences=<span class="literal">False</span>, return_state=<span class="literal">False</span>,</span><br><span class="line">    go_backwards=<span class="literal">False</span>, stateful=<span class="literal">False</span>, unroll=<span class="literal">False</span>, **kwargs</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>ì´ëŠ” tensorflow ê³µì‹ ë¬¸ì„œì— ì í˜€ ìˆëŠ” SimpleRNNì— ê´€í•œ ë‚´ìš©ì´ë‹¤. ì—¬ê¸°ì„œ ìš°ë¦¬ê°€ ê´€ì‹¬ ìˆê²Œ ë´ì•¼ í•  ê²ƒì€ return_sequencesì™€ return_stateì´ë‹¤.</p>
<p>ë§Œì•½, return_sequencesê°€ Falseë©´, SimpleRNN layerëŠ” ë§¨ ë§ˆì§€ë§‰ì˜ ì¶œë ¥ë§Œì„ ë±‰ì–´ë‚¸ë‹¤. ì¦‰, 2ì°¨ì› ì¶œë ¥ì´ ë˜ëŠ” ê²ƒì´ë‹¤. ([Batch_size, output_dim]) í•˜ì§€ë§Œ ì´ íŒŒë¼ë¯¸í„°ê°€ Trueì´ë©´ ëª¨ë“  ì‹œê°„ì˜ ì¶œë ¥ì„ ì¶œë ¥ìœ¼ë¡œ ë‚´ë±‰ëŠ”ë‹¤. ì¦‰, 3ì°¨ì› ì¶œë ¥ì´ ë˜ëŠ” ê²ƒì´ë‹¤. </p>
<p>ê·¸ë¦¬ê³  return_stateì˜ ê²½ìš°, ì´ê²ƒì´ Trueì´ë©´ ì¶œë ¥ì˜ Hidden Stateë¥¼ ì¶œë ¥ìœ¼ë¡œ ê°™ì´ ë‚´ë±‰ëŠ”ë‹¤. ì¦‰, ì¶œë ¥ì´ tupleì´ ë˜ëŠ” ê²ƒì´ë‹¤. (ì¶œë ¥ ë²¡í„°, hidden ë²¡í„°)</p>
<p>ì´ë ‡ê²Œ ë˜ë©´ ìš°ë¦¬ëŠ” ì—¬ëŸ¬ê°€ì§€ ê²½ìš°ì˜ ìˆ˜ë¥¼ ê³ ë ¤í•´ì„œ ëª¨ë¸ì„ ë§Œë“œëŠ” ê²ƒì´ ê°€ëŠ¥í•´ì§„ë‹¤.</p>
<ol>
<li>ë§¨ ë§ˆì§€ë§‰ ì¶œë ¥ë§Œì„ ê³ ë ¤í•˜ì—¬ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸</li>
<li>ì…ë ¥ë§ˆë‹¤ ë‹¤ìŒì— ë‚˜ì˜¬ ì¶œë ¥ì„ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸</li>
</ol>
<p>ì „ìë¥¼ Many to One, í›„ìë¥¼ Many to Manyë¼ê³  í•œë‹¤.</p>
<p>ê·¸ë ‡ë‹¤ë©´ Many to Manyë¥¼ í•´ì£¼ê¸° ìœ„í•´ì„œëŠ” ì‹œê°„ ìˆœì„œì— ë§ì¶°ì„œ <em>ê°™ì€</em> Dense layerë¥¼ ì ìš©í•´ì¤„ í•„ìš”ê°€ ìˆë‹¤. ì´ë¥¼ ìœ„í•´ì„œ í•„ìš”í•œ ê²ƒì´ ë°”ë¡œ TimeDistributed layerì´ë‹¤. ì´ëŠ” ê° ì‹œê°„ stepì— ëŒ€í•´ì„œ ê°™ì€ Dense layerì˜ weightë¡œ ì—°ì‚°ì„ ì§„í–‰í•´ ì¤€ë‹¤.</p>
<p>í•˜ì§€ë§Œ ê¸°ë³¸ì ìœ¼ë¡œ tensorflowì—ì„œ Dense layerëŠ” broadcast ê¸°ëŠ¥ì„ ì œê³µí•œë‹¤. ë”°ë¼ì„œ êµ³ì´ TimeDistributed layer ì—†ì´ë„ 3ì°¨ì› í…ì„œê°€ ë“¤ì–´ì˜¤ë©´ ë§¨ ë§ˆì§€ë§‰ ì°¨ì›ì— ëŒ€í•´ì„œ Dense ì—°ì‚°ì„ ì§„í–‰í•˜ê²Œ ëœë‹¤. </p>
<p>ë”°ë¼ì„œ, Denseë§Œì„ ì‚¬ìš©í• ê±°ë©´ êµ³ì´ TimeDistributed layerê°€ í•„ìš” ì—†ë‹¤.</p>
<p>RNNì˜ í¬ìŠ¤íŒ…ì€ ì´ê±¸ë¡œ ë§ˆì¹˜ë„ë¡ í•˜ê² ë‹¤. ë”±íˆ í¬ê²Œ ì–´ë ¤ìš´ ì ë„ ì—†ê³  ë‚˜ì¤‘ì— ë‚˜ì˜¬ Attentionì´ í›¨ì”¬ ë” ì–´ê·¸ë¡œê°€ ëŒë ¤ì•¼í•  ì£¼ì œì´ê¸° ë•Œë¬¸ì´ë‹¤.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kimh060612.github.io/2022/03/05/CNN-B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Michael Kim">
      <meta itemprop="description" content="Backend Engineering, Deep learning, Algorithm & DS, CS ì „ê³µ ì§€ì‹ ê³µë¶€ ì €ì¥ì†Œ">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Michael's Study Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/05/CNN-B/" class="post-title-link" itemprop="url">Convoltional Neural Network ê°•ì˜ ë‚´ìš© part.B</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-03-05 19:26:43" itemprop="dateCreated datePublished" datetime="2022-03-05T19:26:43+09:00">2022-03-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-05-30 00:00:32" itemprop="dateModified" datetime="2022-05-30T00:00:32+09:00">2022-05-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2022/03/05/CNN-B/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2022/03/05/CNN-B/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><em>ë³¸ í¬ìŠ¤íŠ¸ëŠ” Hands-on Machine learning 2nd Edition, CS231n, Tensorflow ê³µì‹ documentë¥¼ ì°¸ì¡°í•˜ì—¬ ì‘ì„±ë˜ì—ˆìŒì„ ì•Œë¦½ë‹ˆë‹¤.</em></p>
<h3 id="Index"><a href="#Index" class="headerlink" title="Index"></a>Index</h3><ol>
<li><del>Introduction of Convolution Operation</del></li>
<li><del>Definition of Convolutional Neural Network(CNN)</del></li>
<li><del>Back Propagation of CNN</del></li>
<li>Partice</li>
</ol>
<h4 id="6-Partice"><a href="#6-Partice" class="headerlink" title="6. Partice"></a>6. Partice</h4><hr>
<p>ì´ íŒŒíŠ¸ëŠ” Convolutional Nerual Networkë¥¼ ì§ì ‘ êµ¬í˜„í•˜ëŠ” íŒŒíŠ¸ì´ë‹¤. ë¹„ë‹¨ Convolutional Neural Network ë¿ë§Œ ì•„ë‹ˆë¼ ê·¸ë¥¼ ì´ìš©í•œ ë‹¤ì–‘í•œ í˜„ëŒ€ì ì¸ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ë“¤ì„ êµ¬í˜„í•´ ë³´ëŠ” ì‹œê°„ì„ ê°€ì§€ë„ë¡ í•˜ê² ë‹¤. í•„ìì˜ Deep learning êµ¬í˜„ ê´€ë ¨ ë¸”ë¡œê·¸ í¬ìŠ¤íŒ…ì€ ì „ë¶€ Model Subclassing APIë¡œ êµ¬í˜„ë  ì˜ˆì •ì´ë‹¤. ì™œëƒ? í•„ì ë§˜ì´ë‹¤ <del>(ê¼¬ìš°ë©´ ë³´ì§€ ë§ë“ ê°€)</del> ì¥ë‹Œì´ê³ , í•„ìê°€ ìƒê°í•˜ê¸°ì—ëŠ” Model Subclassing APIì˜ í™œìš© ì¥ì ì€ í™•ì‹¤íˆ ìˆëŠ” ê²ƒ ê°™ë‹¤.</p>
<ol>
<li><p>ëª¨ë¸ì„ ê°€ë…ì„± ìˆê²Œ ê´€ë¦¬í•  ìˆ˜ ìˆë‹¤.</p>
<p>ì´ëŠ” ì „ì ìœ¼ë¡œ OOPì— ëŒ€í•œ ê¸°ë³¸ ê°œë… ë° ë””ìì¸ íŒ¨í„´ì„ ì˜ ì•„ëŠ” ì‚¬ëŒì— í•œì—ì„œ ê·¸ëŸ°ê±°ë‹¤.<br>Vision Transformerì¯¤ ê°€ë©´ ì•Œê² ì§€ë§Œ, ì •ë§ ì§œì•¼í•˜ëŠ” ì—°ì‚°ë“¤ì´ ì—„ì²­ ë§ë‹¤. ê·¸ëŸ°ê±¸ í•˜ë‚˜í•˜ë‚˜ í•¨ìˆ˜ë¡œ ì§œê±°ë‚˜ Sequential APIë¡œ êµ¬ì„±í•˜ë©´ ì§€ì˜¥ë¬¸ì´ ì—´ë¦¬ê²Œ ëœë‹¤. ì•„ ë¬¼ë¡  ì§œëŠ”ê±´ ë¬´ë¦¬ê°€ ì—†ê² ì§€ë§Œ, ìœ ì§€ë³´ìˆ˜ ê´€ì ì—ì„œëŠ” ì •ë§ ì§€ì˜¥ì¼ ê²ƒì´ë‹¤.<br>ê·¸ëŸ° ì˜ë¯¸ì—ì„œ Model Subclassing APIëŠ” ì›í•˜ëŠ” ì—°ì‚°ì„ Classë‹¨ìœ„ë¡œ ë¬¶ì–´ì„œ ì„¤ê³„í•˜ê³  ê·¸ë“¤ì„ ì²´ê³„ì ìœ¼ë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆëŠ” ì§€ì‹ì´ ì¡°ê¸ˆì´ë¼ë„ ìˆë‹¤ë©´ (ë³µì¡í•œ ë””ìì¸ íŒ¨í„´ê¹Œì§€ëŠ” í•„ìš”ë„ ì—†ë‹¤) í›¨ì”¬ ê°€ë…ì„±ì´ ë†’ì€ ì½”ë“œë¥¼ ì§¤ ìˆ˜ ìˆë‹¤.</p>
</li>
<li><p>Low Levelí•œ ì—°ì‚°ì„ ììœ ë¡­ê²Œ ì •ì˜í•  ìˆ˜ ìˆë‹¤.</p>
<p> Model Subclassing APIë¥¼ ì‚¬ìš©í•˜ë©´ Custom Layer, Scheduler ë“±ë“± ì—¬ëŸ¬ ì—°ì‚°ì„ ì‚¬ìš©ìì˜ ì…ë§›ì— ë§ê²Œ ì •ì˜í•  ìˆ˜ ìˆë‹¤. ì´ëŸ¬ë©´ ë‚´ê°€ ì„¸ìš´ ìƒˆë¡œìš´ ê°€ì„¤, ì—°êµ¬ ì•„ì´ë””ì–´ë¥¼ ë³´ë‹¤ ì‰½ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆëŠ” íŒë¡œê°€ ì—´ë¦¬ëŠ” ê²ƒì´ë‹¤. ë¬¼ë¡  ì´ëŠ” ì „ì ìœ¼ë¡œ ìì‹ ì´ ìƒˆë¡œìš´ ì—°ì‚°ì„ êµ¬ìƒí•˜ê³  êµ¬í˜„í• ë§Œí•œ ê²½ì§€ì— ë„ë‹¬í–ˆì„ë•Œì˜ ì´ì•¼ê¸°ì´ë‹¤.</p>
</li>
<li><p>íŠ¹íˆ Pytorchë¡œ ì†ŒìŠ¤ì½”ë“œ ì „í™˜ì„ ë¹„êµì  ì‰½ê²Œ í•  ìˆ˜ ìˆë‹¤.</p>
<p>ì´ê±´ ì§€ê·¹íˆ í•„ìì˜ ê°œì¸ì ì¸ ìƒê°ì´ë‹¤. í•„ìëŠ” pytorchì™€ tensorflowë¥¼ ë™ì‹œì— ì¨ê°€ë©´ì„œ ì¼ì„ í•˜ê³  ìˆë‹¤. ëª¨ë¸ ê°œë°œ ë° ì—°êµ¬ëŠ” pytorchë¡œ ë°°í¬ëŠ” tensorflowë¥¼ ì‚¬ìš©í•˜ê³  ìˆëŠ”ë°, ëª¨ë¸ì„ tensorflowë¡œ ì™„ì „íˆ í¬íŒ…í•´ì•¼í•  ì¼ì´ ê°€ë”ì”© ìˆë‹¤. ì´ë•Œ model subclassing APIë¥¼ í™œìš©í•˜ëŠ” í¸ì´ ì†ŒìŠ¤ì½”ë“œì˜ êµ¬ì¡°ë‚˜ ë½„ìƒˆê°€ ë¹„ìŠ·í•´ì„œ í¸í–ˆë˜ ê¸°ì–µì´ ë‚œë‹¤.</p>
</li>
</ol>
<p>í•˜ì§€ë§Œ ë‹¨ì ë„ ëª…í™•í•˜ê²Œ ìˆë‹¤.</p>
<ol>
<li><p>ëª»ì“°ë©´ ì´ë„ ì €ë„ ì•ˆëœë‹¤.</p>
<p> ë³´ë©´ ì•Œë‹¤ì‹œí”¼, OOPì˜ ê¸°ì´ˆ ì§€ì‹ê³¼ low levelë¡œ ì—°ì‚°ì„ ì •ì˜í•´ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì‚¬ëŒì´ ì•„ë‹ˆë¼ë©´ êµ³ì´ Subclassing APIë¥¼ ì“°ê² ë‹¤ê³  ê¹ì¹˜ë‹¤ê°€ ë˜ë ¤ ì˜¤ë¥˜ë§Œ ë²”í•  ê°€ëŠ¥ì„±ì´ ë†’ë‹¤.</p>
</li>
</ol>
<p>í•˜ì§€ë§Œ í•„ìëŠ” ì•ìœ¼ë¡œ ì˜í•˜ê³  ì‹¶ì–´ì„œ í˜ë“  ê¸¸ì„ ê³¨ë¼ ë³´ì•˜ë‹¤. ë…ìë“¤ë„ ì´ì— ë™ì˜í•˜ë¦¬ë¼ê³  ë¯¿ëŠ”ë‹¤. <del>(ì•„ë‹ˆë©´ ë’¤ë¡œ ê°€ë“ ê°€)</del></p>
<p>ì‚¬ì¡±ì´ ê¸¸ì—ˆëŠ”ë°, ì•ìœ¼ë¡œë„ ê³„ì† Model Subclassing APIë§Œì„ ì‚¬ìš©í•´ì„œ í¬ìŠ¤íŒ…ì„ í•  ì˜ˆì •ì´ë‹¤.</p>
<p>ìš°ì„ , ì§€ë‚œ FCNNì²˜ëŸ¼ tensorflow 2ë¡œ ì–´ë–»ê²Œ CNN layerë¥¼ ì •ì˜í•  ìˆ˜ ìˆëŠ”ì§€ë¶€í„° ì•Œì•„ë³´ì.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CustomConv2D</span>(keras.layers.Layer):</span><br><span class="line">    <span class="comment">#              1. output imageì˜ ì±„ë„  2. ì»¤ë„ì˜ ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆ  3. Strideë¥¼ ì •í•´ì¤¬ì–´ì•¼í•¨. 4. Poolingì„ ì •í•´ì¤¬ì–´ì•¼í•¨.(Optional) 5. Paddingì„ ì •í•´ì•¼í•¨.</span></span><br><span class="line">    <span class="comment">#                                                       i  x ë°©í–¥ìœ¼ë¡œì˜ stride y ë°©í–¥ìœ¼ë¡œì˜ stride      i</span></span><br><span class="line">    <span class="comment"># &quot;SAME&quot; OH = H, &quot;VALID&quot; ê°€ëŠ¥í•œ íŒ¨ë”© í•„í„° ì¤‘ì—ì„œ ê°€ì¥ ì‘ì€ íŒ¨ë”©(ì–‘ìˆ˜)ìœ¼ë¡œ ì„¤ì •  </span></span><br><span class="line">    <span class="comment"># OH = (H + 2*P - KH)/S + 1 = 15.5</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, out_channel, kernel_size, Strides = (<span class="params"><span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span></span>), Padding = <span class="string">&quot;SAME&quot;</span>, trainable=<span class="literal">True</span>, name=<span class="literal">None</span>, dtype=<span class="literal">None</span>, dynamic=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(trainable=trainable, name=name, dtype=dtype, dynamic=dynamic, **kwargs)</span><br><span class="line">        <span class="comment"># &quot;3,4&quot; </span></span><br><span class="line">        self.out_channel = out_channel</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(kernel_size) == <span class="built_in">type</span>(<span class="number">1</span>):</span><br><span class="line">            self.kernel_size = (kernel_size, kernel_size)</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">type</span>(kernel_size) == <span class="built_in">type</span>(<span class="built_in">tuple</span>()):</span><br><span class="line">            self.kernel_size = kernel_size</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Not a Valid Kernel Type&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(Strides) == <span class="built_in">type</span>(<span class="number">1</span>):</span><br><span class="line">            self.Stride = (<span class="number">1</span>, Strides, Strides, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">type</span>(Strides) == <span class="built_in">type</span>(<span class="built_in">tuple</span>()):</span><br><span class="line">            self.Stride = Strides</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Not a Valid Stride Type&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(Padding) == <span class="built_in">type</span>(<span class="built_in">str</span>()):</span><br><span class="line">            self.Padding = Padding</span><br><span class="line">        <span class="keyword">else</span> :</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">&quot;Not a Valid Padding Type&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build</span>(<span class="params">self, input_shape</span>):</span><br><span class="line">        WeightShape = (self.kernel_size[<span class="number">0</span>], self.kernel_size[<span class="number">1</span>], input_shape[-<span class="number">1</span>], self.out_channel)</span><br><span class="line">        self.Kernel = self.add_weight(</span><br><span class="line">            shape=WeightShape,</span><br><span class="line">            initializer=<span class="string">&quot;random_normal&quot;</span>,</span><br><span class="line">            trainable= <span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.Bias = self.add_weight(</span><br><span class="line">            shape=(self.out_channel, ),</span><br><span class="line">            initializer=<span class="string">&quot;random_normal&quot;</span>,</span><br><span class="line">            trainable=<span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, Input</span>):</span><br><span class="line">        Out = tf.nn.conv2d(Input, self.Kernel, strides=self.Stride, padding=self.Padding)</span><br><span class="line">        Out = tf.nn.bias_add(Out, self.Bias, data_format=<span class="string">&quot;NHWC&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> Out</span><br></pre></td></tr></table></figure>
<p>ì´ì „ì—ë„ ì„¤ëª…í–ˆë“¯ì´ buildì—ì„œ í•„ìš”í•œ Weightë¥¼ ì •ì˜í•œ ë’¤ì— callì—ì„œ ê·¸ê²ƒì„ ì‚¬ìš©í•œë‹¤. ë‹¤í–‰ì´ê²Œë„ tensorflowì—ì„œëŠ” ìµœì†Œí•œ convolution ì—°ì‚°ì„ ì •ì˜í•´ ì£¼ì—ˆë‹¤.<br>ì•ìœ¼ë¡œë„ í•„ìš”í•œ ì—°ì‚°ì´ ìˆë‹¤ë©´ ì´ë ‡ê²Œ ì •ì˜í•´ ì£¼ë©´ ëœë‹¤.</p>
<p>í•˜ì§€ë§Œ ìš°ë¦¬ëŠ” êµ³ì´ ì´ë ‡ê²Œ convolution layerë¥¼ ì •ì˜í•´ì¤„ í•„ìš”ê°€ ì—†ë‹¤. ì™œëƒë©´ tensorflow kerasì—ì„œ ì´ë¯¸ ì •ì˜ë˜ì–´ ìˆëŠ” ì¢‹ì€ í•¨ìˆ˜ê°€ ìˆê¸° ë•Œë¬¸ì´ë‹¤. ì´ì— ëŒ€í•œ ì•„ì£¼ ê°„ë‹¨í•œ ì‚¬ìš© ì˜ˆì œë¡œì¨ Alexnetê³¼ ResNetì„ êµ¬í˜„í•´ ë³´ë„ë¡ í•˜ê² ë‹¤. ë¶€ë¡ìœ¼ë¡œ GoogLeNetì„ êµ¬í˜„í•œ ì˜ˆì œë„ ìˆëŠ”ë°, ì´ëŠ” í•„ìì˜ Githubì— ì˜¬ë ¤ ë‘ë„ë¡ í• í…Œë‹ˆ ì‹œê°„ì´ ë˜ë©´ ê°€ì„œ ë´ ì£¼ì—ˆìœ¼ë©´ í•œë‹¤.</p>
<p>ìš°ì„  AlexNetë¶€í„° ê°€ë³´ì. ëª¨ë¸ì˜ êµ¬ì¡°ë¥¼ ì‚¬ì§„ìœ¼ë¡œ í•œë²ˆ ë´ë³´ë„ë¡ í•˜ì.</p>
<p align="center"><img src="http://kimh060612.github.io/img/AlexNet.jpg" width="70%" height="70%"></p>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AlexNet</span>(keras.Model):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># ì›ë˜ ì—¬ê¸°ì—ëŠ” ì»¤ë„ ì‚¬ì´ì¦ˆë¡œ (11, 11)ì´ ë“¤ì–´ê°€ê³  paddingì€ validì´ë‹¤. í•˜ì§€ë§Œ ë©”ëª¨ë¦¬ ë•Œë¬¸ì— ëŒì•„ê°€ì§€ ì•ŠëŠ” ê´€ê³„ë¡œ ì´ë¯¸ì§€ í¬ê¸°ë¥¼ ì¤„ì•„ëŠë¼ ë¶€ë“ì´í•˜ê²Œ ëª¨ë¸ì„ ì¡°ê¸ˆ ë³€ê²½í–ˆë‹¤.</span></span><br><span class="line">        self.Conv1 = keras.layers.Conv2D(<span class="number">96</span>, (<span class="number">3</span>, <span class="number">3</span>), strides=(<span class="number">4</span>, <span class="number">4</span>), padding=<span class="string">&quot;SAME&quot;</span>, activation=<span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        <span class="comment"># LRN 1</span></span><br><span class="line">        self.BatchNorm1 = keras.layers.BatchNormalization()</span><br><span class="line">        self.MaxPool1 = keras.layers.MaxPool2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>), strides=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">&quot;VALID&quot;</span>)</span><br><span class="line"></span><br><span class="line">        self.Conv2 = keras.layers.Conv2D(<span class="number">256</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), padding=<span class="string">&quot;SAME&quot;</span>, activation=<span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        <span class="comment"># LRN2</span></span><br><span class="line">        self.BatchNorm2 = keras.layers.BatchNormalization()</span><br><span class="line">        self.MaxPool2 = keras.layers.MaxPool2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>), strides=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">&quot;VALID&quot;</span>)</span><br><span class="line"></span><br><span class="line">        self.Conv3 = keras.layers.Conv2D(<span class="number">384</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&quot;SAME&quot;</span>, activation=<span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        self.Conv4 = keras.layers.Conv2D(<span class="number">384</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&quot;SAME&quot;</span>, activation=<span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        self.Conv5 = keras.layers.Conv2D(<span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">&quot;SAME&quot;</span>, activation=<span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        self.MaxPool3 = keras.layers.MaxPool2D(pool_size=(<span class="number">3</span>, <span class="number">3</span>), strides=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">        self.Flat = keras.layers.Flatten()</span><br><span class="line"></span><br><span class="line">        self.Dense1 = keras.layers.Dense(<span class="number">4096</span>, activation=<span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        self.DropOut1 = keras.layers.Dropout(<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">        self.Dense2 = keras.layers.Dense(<span class="number">4096</span>, activation=<span class="string">&quot;relu&quot;</span>)</span><br><span class="line">        self.DropOut2 = keras.layers.Dropout(<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">        self.OutDense = keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, Input</span>):</span><br><span class="line">        X = self.Conv1(Input)</span><br><span class="line">        X = self.BatchNorm1(X)</span><br><span class="line">        X = self.MaxPool2(X)</span><br><span class="line">        X = self.Conv2(X)</span><br><span class="line">        X = self.BatchNorm2(X)</span><br><span class="line">        X = self.MaxPool2(X)</span><br><span class="line">        X = self.Conv3(X)</span><br><span class="line">        X = self.Conv4(X)</span><br><span class="line">        X = self.Conv5(X)</span><br><span class="line">        X = self.MaxPool3(X)</span><br><span class="line">        X = self.Flat(X)</span><br><span class="line">        X = self.Dense1(X)</span><br><span class="line">        X = self.DropOut1(X)</span><br><span class="line">        X = self.Dense2(X)</span><br><span class="line">        X = self.DropOut2(X)</span><br><span class="line">        X = self.OutDense(X)</span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure>
<p>ì, í•„ìëŠ” êµ³ì´ ë”ëŸ½ê²Œ ì§œ ë³´ì•˜ë‹¤. ì™œëƒ? <em>ì´ë ‡ê²Œ ì§¤ê±°ë©´ Model Subclassingì„ ì“°ì§€ ë§ë¼ëŠ” ì˜ë¯¸ë¡œ ì´ë ‡ê²Œ ì§œ ë³´ì•˜ë‹¤.</em> ì§„ì§œ ì´ë”°êµ¬ë¡œ ì§¤ê±°ë©´ ê·¸ëƒ¥ Sequential APIë‚˜ Functional APIë¥¼ ì‚¬ìš©í•˜ì. ê·¼ë° ì´ ì •ë„ë©´ ì„¤ëª…ì´ í•„ìš” ì—†ì„ ì •ë„ë¡œ ê·¸ëƒ¥ ë¬´ì§€ì„± êµ¬í˜„ì„ ì‹œì „í•œ ê²ƒì´ë‹¤. ê·¸ëŸ¬ë‹ˆ ê°„ë‹¨í•˜ê²Œ Kerasì˜ Conv2Dë¥¼ ì„¤ëª…í•˜ê³  ë„˜ì–´ ê°€ë„ë¡ í•˜ê² ë‹¤.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Conv2D(filters, kernel_size=(kernel_sz, kernel_sz), padding=<span class="string">&quot;SAME&quot;</span>, activation=<span class="string">&quot;relu&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>ì´ì „ì— ì´ë¡  ê¸€ì—ì„œ ì„¤ëª…í–ˆë˜ ë¶€ë¶„ì„ ë‹¤ì‹œ ë˜ì§šì–´ ë³´ê³  ìœ„ í•¨ìˆ˜ë¥¼ ë‹¤ì‹œ ì‚´í´ë³´ì.</p>
<blockquote>
<p><em>Definition 1</em></p>
</blockquote>
<ul>
<li>$w_{ijmk}^l$: $l$ë²ˆì§¸ ì¸µì˜ Weightì˜ $k$ë²ˆì§¸ Kernel Setì— $m$ë²ˆì§¸ Channel, $i$í–‰, $j$ì—´ì˜ ì„±ë¶„</li>
</ul>
<p>ìœ„ì˜ $w_{ijmk}^l$ë¥¼ ìš°ë¦¬ëŠ” ìœ„ì˜ Conv2D í•¨ìˆ˜ë¡œ ì •ì˜í•œ ê²ƒì´ë‹¤. filtersëŠ” kernel setì˜ ê°œìˆ˜ë¥¼ ì˜ë¯¸í•˜ë©°, kernel_sizeëŠ” Weight kernelì˜ ì´ë¯¸ì§€ í¬ê¸°ë¥¼ ì˜ë¯¸í•œë‹¤. paddingì€ â€œSAMEâ€ê³¼ â€œVALIDâ€ê°€ ìˆëŠ”ë°, â€œSAMEâ€ìœ¼ë¡œ í•˜ë©´ ì•Œì•„ì„œ í¬ê¸°ë¥¼ ê³„ì‚°í•´ì„œ ì…ë ¥ ì´ë¯¸ì§€ì™€ ì¶œë ¥ ì´ë¯¸ì§€ì˜ í¬ê¸°ë¥¼ ê°™ê²Œ ë§Œë“ ë‹¤. Validë¥¼ ì„ íƒí•˜ë©´ ê·¸ëƒ¥ paddingì´ ì—†ë‹¤ê³  íŒë‹¨í•˜ë©´ ëœë‹¤. </p>
<p>AlextNetì—ì„œ ëŒ€ì¶© Conv2Dë¥¼ ì–´ë–»ê²Œ ì‚¬ìš©í•˜ëŠ”ì§€ ê°ì´ ì™”ë‹¤ë©´, ResNetì„ í•œë²ˆ êµ¬í˜„í•´ ë³´ì.</p>
<p>ResNetì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ì€ ë‹¤ë¥¸ í¬ìŠ¤íŠ¸ì—ì„œ ì •ë§ ì´ê²Œ ë§ë‚˜ ì‹¶ì„ ì •ë„ë¡œ ë¶„í•´í•´ì„œ ì„¤ëª…í•˜ë„ë¡ í•˜ê² ë‹¤. ì§€ê¸ˆì€ ê·¸ì € ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°ê°€ ìˆêµ¬ë‚˜ ì •ë„ë§Œ ì´í•´í•˜ê³  ë„˜ì–´ê°€ë©´ ëœë‹¤.</p>
<p align="center"><img src="http://kimh060612.github.io/img/ResNet50.png" width="70%" height="70%"></p>

<p>ì´ê²ƒì´ ResNet50ì˜ êµ¬ì¡°ì¸ë°, 2ê°€ì§€ layerë¥¼ êµ¬í˜„í•´ ë³´ì•„ì•¼ í•œë‹¤. ì²«ë²ˆì§¸ëŠ” Conv Blockì´ê³  ë‘ë²ˆì§¸ëŠ” Identity Blockì´ë‹¤. í•˜ë‚˜ëŠ” Skip Connectionì— Convolution layerë¥¼ ì…íŒ ê²ƒì´ê³  ë‹¤ë¥¸ í•˜ë‚˜ëŠ” ê·¸ë ‡ì§€ ì•Šì€ ê²ƒ ë¿ì´ë‹¤.</p>
<blockquote>
<p>Residual Conv Block</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResidualConvBlock</span>(tfk.layers.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, InputChannel, OutputChannel, strides = (<span class="params"><span class="number">1</span>, <span class="number">1</span></span>), trainable=<span class="literal">True</span>, name=<span class="literal">None</span>, dtype=<span class="literal">None</span>, dynamic=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(trainable=trainable, name=name, dtype=dtype, dynamic=dynamic, **kwargs)</span><br><span class="line"></span><br><span class="line">        self.Batch1 = tfk.layers.BatchNormalization(momentum=<span class="number">0.99</span>, epsilon= <span class="number">0.001</span>)</span><br><span class="line">        self.conv1 = tfk.layers.Conv2D(filters=InputChannel, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), strides=strides)</span><br><span class="line">        self.LeakyReLU1 = tfk.layers.LeakyReLU()</span><br><span class="line">        self.Batch2 = tfk.layers.BatchNormalization(momentum=<span class="number">0.99</span>, epsilon= <span class="number">0.001</span>)</span><br><span class="line">        self.conv2 = tfk.layers.Conv2D(filters=InputChannel, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), strides=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">&quot;SAME&quot;</span>)</span><br><span class="line">        self.LeakyReLU2 = tfk.layers.LeakyReLU()</span><br><span class="line">        self.Batch3 = tfk.layers.BatchNormalization(momentum=<span class="number">0.99</span>, epsilon= <span class="number">0.001</span>)</span><br><span class="line">        self.conv3 = tfk.layers.Conv2D(filters=OutputChannel, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), strides=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.LeakyReLU3 = tfk.layers.LeakyReLU()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Skip Connection</span></span><br><span class="line">        self.SkipConnection = tfk.layers.Conv2D(filters=OutputChannel, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), strides=strides)</span><br><span class="line">        self.SkipBatch = tfk.layers.BatchNormalization(momentum=<span class="number">0.99</span>, epsilon= <span class="number">0.001</span>)</span><br><span class="line">        self.LeakyReLUSkip = tfk.layers.LeakyReLU()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, Input</span>):</span><br><span class="line">        Skip = Input</span><br><span class="line">        Skip = self.SkipConnection(Skip)</span><br><span class="line">        Skip = self.SkipBatch(Skip)</span><br><span class="line">        Skip = self.LeakyReLUSkip(Skip)</span><br><span class="line">        Z = Input</span><br><span class="line">        Z = self.conv1(Z)</span><br><span class="line">        Z = self.Batch1(Z)</span><br><span class="line">        Z = self.LeakyReLU1(Z)</span><br><span class="line">        Z = self.conv2(Z)</span><br><span class="line">        Z = self.Batch2(Z)</span><br><span class="line">        Z = self.LeakyReLU2(Z)  </span><br><span class="line">        Z = self.conv3(Z)</span><br><span class="line">        Z = self.Batch3(Z)</span><br><span class="line">        Z = self.LeakyReLU3(Z)</span><br><span class="line">        <span class="keyword">return</span> Z + Skip </span><br></pre></td></tr></table></figure>
<blockquote>
<p>Residual Identity Block</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResidualIdentityBlock</span>(tfk.layers.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, InputChannel, OutputChannel, trainable=<span class="literal">True</span>, name=<span class="literal">None</span>, dtype=<span class="literal">None</span>, dynamic=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(trainable=trainable, name=name, dtype=dtype, dynamic=dynamic, **kwargs)</span><br><span class="line">        self.Batch1 = tfk.layers.BatchNormalization(momentum=<span class="number">0.99</span>, epsilon= <span class="number">0.001</span>)</span><br><span class="line">        self.conv1 = tfk.layers.Conv2D(filters=InputChannel, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), strides=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.LeakyReLU1 = tfk.layers.LeakyReLU()</span><br><span class="line">        self.Batch2 = tfk.layers.BatchNormalization(momentum=<span class="number">0.99</span>, epsilon= <span class="number">0.001</span>)</span><br><span class="line">        self.conv2 = tfk.layers.Conv2D(filters=InputChannel, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), strides=(<span class="number">1</span>, <span class="number">1</span>), padding=<span class="string">&quot;SAME&quot;</span>)</span><br><span class="line">        self.LeakyReLU2 = tfk.layers.LeakyReLU()</span><br><span class="line">        self.Batch3 = tfk.layers.BatchNormalization(momentum=<span class="number">0.99</span>, epsilon= <span class="number">0.001</span>)</span><br><span class="line">        self.conv3 = tfk.layers.Conv2D(filters=OutputChannel, kernel_size=(<span class="number">1</span>, <span class="number">1</span>), strides=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.LeakyReLU3 = tfk.layers.LeakyReLU()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, Input</span>):</span><br><span class="line">        Skip = Input</span><br><span class="line">        Z = Input</span><br><span class="line">        Z = self.conv1(Z)</span><br><span class="line">        Z = self.Batch1(Z)</span><br><span class="line">        Z = self.LeakyReLU1(Z)</span><br><span class="line">        Z = self.conv2(Z)</span><br><span class="line">        Z = self.Batch2(Z)</span><br><span class="line">        Z = self.LeakyReLU2(Z)  </span><br><span class="line">        Z = self.conv3(Z)</span><br><span class="line">        Z = self.Batch3(Z)</span><br><span class="line">        Z = self.LeakyReLU3(Z)</span><br><span class="line">        <span class="comment"># Z : 256</span></span><br><span class="line">        <span class="keyword">return</span> Z + Skip</span><br></pre></td></tr></table></figure>
<p>ìš°ì„  ì´ ë˜í•œ ì •ë§ì´ì§€ Model Subclassingì„ ê·¸ì§€ê°™ì´ ì‚¬ìš©í•œ ì˜ˆì‹œì¤‘ í•˜ë‚˜ì´ë‹¤. ë¶€ë”” ë…ìë“¤ì€ ì´ë”°êµ¬ë¡œ êµ¬í˜„í• ê±°ë©´ ê·¸ëƒ¥ Functional APIë¥¼ ì‚¬ìš©í•˜ê¸° ë°”ë€ë‹¤.</p>
<p>ì´ì¯¤ë˜ë©´ ì´ëŸ° ì§ˆë¬¸ì´ ë‚˜ì˜¬ ê²ƒì´ë‹¤. </p>
<blockquote>
<p>Q: ì™œ ì €ê²Œ ê·¸ì§€ê°™ì´ êµ¬í˜„í•œ ì˜ˆì‹œì¸ê°€ìš”?<br>A: ì—¬ëŸ¬ ì´ìœ ê°€ ìˆì§€ë§Œ, ê°€ì¥ í° ì´ìœ ëŠ” êµ³ì´ ëª¨ë¸(Weight)ì˜ ì •ì˜ì™€ í˜¸ì¶œì„ ë¶„ë¦¬í•  ì´ìœ ê°€ ì „í˜€ ì—†ëŠ” êµ¬ì¡°ì´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.</p>
</blockquote>
<p>ì—¬ê¸°ê¹Œì§€ ì˜ ë”°ë¼ì™”ë‹¤ë©´ ì´ì œ ì´ 2ê°œì˜ layerë¥¼ ì‚¬ìš©í•´ì„œ ResNet50ì„ ë‹¤ìŒê³¼ ê°™ì´ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResNet50</span>(keras.Model):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># Input Shape 224*224*3</span></span><br><span class="line">        <span class="comment"># Conv 1 Block</span></span><br><span class="line">        self.ZeroPadding1 = keras.layers.ZeroPadding2D(padding=(<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">        self.Conv1 = keras.layers.Conv2D(filters = <span class="number">64</span>, kernel_size=(<span class="number">7</span>, <span class="number">7</span>), strides=(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        self.Batch1 = keras.layers.BatchNormalization()</span><br><span class="line">        self.ReLU1 = keras.layers.LeakyReLU()</span><br><span class="line">        self.ZeroPadding2 = keras.layers.ZeroPadding2D(padding=(<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        self.MaxPool1 = keras.layers.MaxPool2D(pool_size=(<span class="number">3</span>, <span class="number">3</span>), strides=<span class="number">2</span>)</span><br><span class="line">        self.ResConvBlock1 = ResidualConvBlock(<span class="number">64</span>, <span class="number">256</span>, strides = (<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.ResIdentityBlock1 = ResidualIdentityBlock(<span class="number">64</span>, <span class="number">256</span>)</span><br><span class="line">        self.ResIdentityBlock2 = ResidualIdentityBlock(<span class="number">64</span>, <span class="number">256</span>)</span><br><span class="line"></span><br><span class="line">        self.ResConvBlock2 = ResidualConvBlock(<span class="number">128</span>, <span class="number">512</span>, strides = (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        self.ResIdentityBlock3 = ResidualIdentityBlock(<span class="number">128</span>, <span class="number">512</span>)</span><br><span class="line">        self.ResIdentityBlock4 = ResidualIdentityBlock(<span class="number">128</span>, <span class="number">512</span>)</span><br><span class="line">        self.ResIdentityBlock5 = ResidualIdentityBlock(<span class="number">128</span>, <span class="number">512</span>)</span><br><span class="line"></span><br><span class="line">        self.ResConvBlock3 = ResidualConvBlock(<span class="number">256</span>, <span class="number">1024</span>, strides = (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        self.ResIdentityBlock6 = ResidualIdentityBlock(<span class="number">256</span>, <span class="number">1024</span>)</span><br><span class="line">        self.ResIdentityBlock7 = ResidualIdentityBlock(<span class="number">256</span>, <span class="number">1024</span>)</span><br><span class="line">        self.ResIdentityBlock8 = ResidualIdentityBlock(<span class="number">256</span>, <span class="number">1024</span>)</span><br><span class="line">        self.ResIdentityBlock9 = ResidualIdentityBlock(<span class="number">256</span>, <span class="number">1024</span>)</span><br><span class="line">        self.ResIdentityBlock10 = ResidualIdentityBlock(<span class="number">256</span>, <span class="number">1024</span>)</span><br><span class="line"></span><br><span class="line">        self.ResConvBlock4 = ResidualConvBlock(<span class="number">512</span>, <span class="number">2048</span>, strides = (<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.ResIdentityBlock11 = ResidualIdentityBlock(<span class="number">512</span>, <span class="number">2048</span>)</span><br><span class="line">        self.ResIdentityBlock12 = ResidualIdentityBlock(<span class="number">512</span>, <span class="number">2048</span>)</span><br><span class="line">        </span><br><span class="line">        self.GAP = keras.layers.GlobalAveragePooling2D()</span><br><span class="line">        self.DenseOut = keras.layers.Dense(<span class="number">1000</span>, activation=<span class="string">&#x27;softmax&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, Input</span>):</span><br><span class="line">        X = self.ZeroPadding1(Input)</span><br><span class="line">        X = self.Conv1(X)</span><br><span class="line">        X = self.Batch1(X)</span><br><span class="line">        X = self.ReLU1(X)</span><br><span class="line">        X = self.ZeroPadding2(X)</span><br><span class="line"></span><br><span class="line">        X = self.MaxPool1(X)</span><br><span class="line">        X = self.ResConvBlock1(X)</span><br><span class="line">        X = self.ResIdentityBlock1(X)</span><br><span class="line">        X = self.ResIdentityBlock2(X)</span><br><span class="line"></span><br><span class="line">        X = self.ResConvBlock2(X)</span><br><span class="line">        X = self.ResIdentityBlock3(X)</span><br><span class="line">        X = self.ResIdentityBlock4(X)</span><br><span class="line">        X = self.ResIdentityBlock5(X)</span><br><span class="line"></span><br><span class="line">        X = self.ResConvBlock3(X)</span><br><span class="line">        X = self.ResIdentityBlock6(X)</span><br><span class="line">        X = self.ResIdentityBlock7(X)</span><br><span class="line">        X = self.ResIdentityBlock8(X)</span><br><span class="line">        X = self.ResIdentityBlock9(X)</span><br><span class="line">        X = self.ResIdentityBlock10(X)</span><br><span class="line"></span><br><span class="line">        X = self.ResConvBlock4(X)</span><br><span class="line">        X = self.ResIdentityBlock11(X)</span><br><span class="line">        X = self.ResIdentityBlock12(X)</span><br><span class="line"></span><br><span class="line">        X = self.GAP(X)</span><br><span class="line">        Out = self.DenseOut(X)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> Out</span><br></pre></td></tr></table></figure>
<p>ì—¬ê¸°ì„œ í•˜ë‚˜ GAPë¡œ ì •ì˜ëœ Global Average Pooling layerê°€ ìˆë‹¤. ì´ê²ƒì— ëŒ€í•´ì„œ ê°„ë‹¨í•˜ê²Œë§Œ ì•Œì•„ë³´ì.</p>
<p>ì´ layerëŠ” ë‹¨ìˆœí•˜ê²Œ ë§í•˜ìë©´ feaeture mapì„ 1ì°¨ì›ì„ ë§Œë“¤ì–´ ì£¼ëŠ” layerì´ë‹¤. ëŒ€ê°œ, ImageëŠ” 3ì°¨ì›ì¸ë°, ì°¨ì›ë³„ë¡œ ì¡´ì¬í•˜ëŠ” imageë¥¼ í•˜ë‚˜ì˜ Scalar ê°’ìœ¼ë¡œ ë§Œë“ ë‹¤ëŠ” ëœ»ì´ë‹¤. (Global Pooling) ê·¸ë•Œ, Scalar ê°’ìœ¼ë¡œ ë§Œë“œëŠ” ê³¼ì •ì—ì„œ ì´ë¯¸ì§€ì˜ ê° í”½ì…€ ê°’ì„ í‰ê· ì„ ë‚´ëŠ” ë°©ë²•ì„ ì·¨í•œ ê²ƒ ë¿ì´ë‹¤. (Average)</p>
<p>ì´ë¥¼ ê°„ë‹¨íˆ ê·¸ë¦¼ìœ¼ë¡œ í‘œí˜„í•˜ìë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>
<p align="center"><img src="http://kimh060612.github.io/img/GAP.jpg" width="100%" height="100%"></p>

<p>ê·¸ë¦¼ì—ì„œ ë³´ì—¬ì§€ëŠ” ê²ƒê³¼ ê°™ì´, ê° ì±„ë„ì— ìˆëŠ” ì´ë¯¸ì§€ë“¤ì˜ í”½ì…€ê°’ì„ í‰ê· ì„ ë‚´ì„œ ê·¸ê²ƒì„ ëª¨ìœ¼ë©´ ì±„ë„ì˜ ê°œìˆ˜ ë§Œí¼ì˜ í¬ê¸°ë¥¼ ê°€ì§€ëŠ” 1-dimensional vectorê°€ ì™„ì„±ëœë‹¤.</p>
<p>ì—¬ê¸°ê¹Œì§€ Convolutional Neural Networkì˜ êµ¬í˜„ ì‹¤ìŠµì„ ë§ˆì¹˜ë„ë¡ í•˜ê² ë‹¤. ë¶€ë”” ë„ì›€ì´ ë˜ì—ˆâ€¦.ì„ê¹Œ?ëŠ” ëª¨ë¥´ê² ì§€ë§Œ ì¬ë°Œê²Œ ë³´ì•˜ìœ¼ë©´ ì¢‹ê² ë‹¤.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kimh060612.github.io/2022/03/05/CNN-A/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Michael Kim">
      <meta itemprop="description" content="Backend Engineering, Deep learning, Algorithm & DS, CS ì „ê³µ ì§€ì‹ ê³µë¶€ ì €ì¥ì†Œ">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Michael's Study Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/05/CNN-A/" class="post-title-link" itemprop="url">Convoltional Neural Network ê°•ì˜ ë‚´ìš© part.A</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-03-05 19:26:40" itemprop="dateCreated datePublished" datetime="2022-03-05T19:26:40+09:00">2022-03-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-05-30 00:00:32" itemprop="dateModified" datetime="2022-05-30T00:00:32+09:00">2022-05-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2022/03/05/CNN-A/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2022/03/05/CNN-A/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><em>ë³¸ í¬ìŠ¤íŠ¸ëŠ” Hands-on Machine learning 2nd Edition, CS231n, Tensorflow ê³µì‹ documentë¥¼ ì°¸ì¡°í•˜ì—¬ ì‘ì„±ë˜ì—ˆìŒì„ ì•Œë¦½ë‹ˆë‹¤.</em></p>
<h3 id="Index"><a href="#Index" class="headerlink" title="Index"></a>Index</h3><ol>
<li>Introduction of Convolution Operation  </li>
<li>Definition of Convolutional Neural Network(CNN)</li>
<li>Back Propagation of CNN</li>
<li>Partice</li>
</ol>
<p>ì—¬ê¸°ì„œëŠ” 1~3ëŠ” Part. Aì´ê³  4ì€ Part. Bì—ì„œ ë‹¤ë£¨ë„ë¡ í•˜ê² ë‹¤.<br><br></p>
<h3 id="Introduction-of-Convolution-Operation"><a href="#Introduction-of-Convolution-Operation" class="headerlink" title="Introduction of Convolution Operation"></a>Introduction of Convolution Operation</h3><hr>
<p>Convolutional Neural NetworkëŠ” Convolution ì—°ì‚°ì„ Neural Networkì— ì ìš©í•œ ê²ƒì´ë‹¤. ë”°ë¼ì„œ ì´ë¥¼ ì•Œê¸° ìœ„í•´ì„œëŠ” Convolution ì—°ì‚°ì„ ë¨¼ì € ì•Œì•„ì•¼í•  í•„ìš”ê°€ ìˆë‹¤. ê´€ë ¨ í•™ê³¼ ëŒ€í•™ìƒì´ë¼ë©´ ì•„ë§ˆë„ ì‹ í˜¸ì™€ ì‹œìŠ¤í…œì„ ë°°ìš°ë©´ì„œ ì´ë¥¼ ì²˜ìŒ ì ‘í–ˆì„ ê²ƒì´ë‹¤. Continuous domain, Discrete domianê¹Œì§€ ì´ ì—°ì‚°ì„ ì •ì˜ë  ìˆ˜ ìˆê³  ê°ê°ì— ë”°ë¼ ê³„ì‚° ë°©ë²• ë˜í•œ ë°°ì› ì„ ê²ƒì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ì„œ 2ì°¨ì›ì˜ Imageì™€ 2ì°¨ì›ì˜ Filterì˜ Convolution ì—°ì‚°ì„ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•´ ë³´ë„ë¡ í•˜ê² ë‹¤.</p>
<ul>
<li>Image í–‰ë ¬ ì •ì˜</li>
</ul>
<script type="math/tex; mode=display">
I(i, j)</script><p>Imageì˜ $i$ì—´, $j$í–‰ì˜ ì„±ë¶„ </p>
<ul>
<li>Filter í–‰ë ¬ ì •ì˜</li>
</ul>
<script type="math/tex; mode=display">
K(i, j)</script><p>Filterì˜ $i$ì—´, $j$í–‰ì˜ ì„±ë¶„. ë†’ì´ë¥¼ $k_1$, ë„ˆë¹„ë¥¼ $k_2$ë¼ê³  ê°€ì •.</p>
<ul>
<li>$I$ì™€ $K$ì˜ Convolution ì—°ì‚°</li>
</ul>
<script type="math/tex; mode=display">
(I*K)_{ij} = \sum_{m = 0}^{k_1 - 1} \sum_{n = 0}^{k_2 - 1}I(i - m, j - n)K(m, n)
\tag{equation (1)}</script><p>ìœ„ ì •ì˜ë¥¼ ì¡°ê¸ˆ í‹€ë©´ ë‹¤ìŒê³¼ ê°™ì´ë„ í‘œí˜„ì´ ê°€ëŠ¥í•˜ë‹¤.</p>
<script type="math/tex; mode=display">
(I*K)_{ij} = \sum_{m = 0}^{k_1 - 1} \sum_{n = 0}^{k_2 - 1}I(i + m, j + n)K(-m, -n)
\tag{equation (2)}</script><p>ìœ„ì™€ ê°™ì€ ì—°ì‚°ì˜ í˜•íƒœë¥¼ Correlationì´ë¼ê³  í•œë‹¤. ì¦‰, Convolution ì—°ì‚°ì˜ Filterë¥¼ $\pi$ë§Œí¼ íšŒì „ì‹œí‚¨ë‹¤ë©´ ê·¸ê²ƒì´ Correlation ì—°ì‚°ì¸ ê²ƒì´ë‹¤. ì´ëŠ” ì•„ì£¼ ì¤‘ìš”í•œ ê´€ê³„ì´ë¯€ë¡œ ê¼­ ê¸°ì–µí•´ ë‘ë„ë¡ í•˜ì.</p>
<h3 id="Definition-of-Convolutional-Neural-Network-CNN"><a href="#Definition-of-Convolutional-Neural-Network-CNN" class="headerlink" title="Definition of Convolutional Neural Network(CNN)"></a>Definition of Convolutional Neural Network(CNN)</h3><hr>
<p>CNNì˜ ì •ì˜ëŠ” ìœ„ì˜ Convolution ì—°ì‚°ì„ ì‚¬ìš©í•˜ì—¬ Weightì™€ Inputì„ ê³„ì‚°í•˜ëŠ” ê²ƒì´ë‹¤. ë§¤ìš° ê°„ë‹¨í•œ ì˜ˆì‹œë¡œ LeNetì´ë¼ëŠ” ê²ƒì„ ë³´ì. ë„ˆë¬´ ìì£¼ ë‚˜ì˜¤ëŠ” ì˜ˆì‹œë¼ì„œ í•˜í’ˆì´ ë‚˜ì˜¬ ê²ƒ ê°™ì§€ë§Œ, ë³¸ë˜ ê¸°ë³¸ì´ë¼ëŠ” ê²ƒì€ â€œì‰¬ìš´â€ê²ƒì´ ì•„ë‹ˆë¼ â€œì¤‘ìš”í•œâ€ê²ƒì´ë‹¤.</p>
<p align="center"><img src="https://kimh060612.github.io/img/LeNet.png" width="100%"></p>

<p>ê·¸ë¦¼ì—ì„œì˜ ê° íŒŒíŠ¸ë¥¼ ë¶„í•´í•´ì„œ ì‚´í´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>
<blockquote>
<p>Image Input -&gt; (Convolution) -&gt; Feature Map -&gt; (Pooling) -&gt; Feature Map -&gt; (Convolution) -&gt; Feature Map -&gt; (Pooling) -&gt; Feature Map -&gt; (Flatten) -&gt; Feature Vector -&gt; (FCNN) -&gt; Feature Vector -&gt; (FCNN) -&gt; Feature Vector -&gt; (Gaussian Connection) -&gt; Output Vector</p>
</blockquote>
<p>ì—¬ê¸°ì„œ ê´„í˜¸ ì•ˆì— ë“¤ì–´ ìˆëŠ” ê²ƒì´ ì—°ì‚°ì˜ ì´ë¦„ì´ë‹¤. FCNNì€ ë‹¤ë¥¸ í¬ìŠ¤íŠ¸ì—ì„œ ë´¤ë‹¤ê³  ê°€ì •í•˜ê³ , ì—¬ê¸°ì„œ ì£¼ëª©í•´ì•¼ í•  ê²ƒì€ Pooling Layerì´ë‹¤. </p>
<p>Poolingì€ ë‹¤ì–‘í•œ ì¢…ë¥˜ê°€ ìˆëŠ”ë° ê°„ë‹¨í•˜ê²Œ í•œê°€ì§€ë§Œ ì†Œê°œí•˜ìë©´ Max Poolingì´ ìˆë‹¤. ìì„¸í•œ ê²ƒì€ Tensorflow 2 <a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D">ê³µì‹ ë¬¸ì„œ</a>ë¥¼ ì°¸ì¡°í•˜ëŠ” ê²ƒì´ ë” ì¢‹ì„ ê²ƒ ê°™ë‹¤.<br>ì—¬ê¸°ì„œëŠ” Poolingê¹Œì§€ ìì„¸íˆ ë‹¤ë£° ì´ìœ ëŠ” ì—†ëŠ” ê²ƒ ê°™ë‹¤.</p>
<p>ì´ì œ ë³¸ê²©ì ìœ¼ë¡œ Convolution ì—°ì‚°ì— ëŒ€í•´ì„œ ì•Œì•„ë³´ë„ë¡ í•˜ê² ë‹¤. ê·¸ ì „ì— FCNN í¬ìŠ¤íŠ¸ì—ì„œë„ ê·¸ë¬ë“¯ì´, ìˆ˜ì‹ í‘œí˜„ì„ í•˜ê¸° ìœ„í•œ ì •ì˜ë¶€í„° í•˜ê³  ì‹œì‘í•˜ì.</p>
<blockquote>
<p><em>Definition 1</em></p>
</blockquote>
<ul>
<li>$u_{ijm}^l$: $l$ë²ˆì§¸ ì¸µì˜ Feature Mapì˜ $m$ë²ˆì§¸ Channel $i$í–‰, $j$ì—´ì˜ ì„±ë¶„</li>
<li>$x_{ijm}$: Inputì˜ $m$ë²ˆì§¸ Channelì˜ $i$í–‰, $j$ì—´ì˜ ì„±ë¶„</li>
<li>$w_{ijmk}^l$: $l$ë²ˆì§¸ ì¸µì˜ Weightì˜ $k$ë²ˆì§¸ Kernel Setì— $m$ë²ˆì§¸ Channel, $i$í–‰, $j$ì—´ì˜ ì„±ë¶„</li>
<li>$b_m^l$: $l$ë²ˆì§¸ ì¸µì˜ $m$ë²ˆì§¸ Channelì˜ Bias</li>
</ul>
<p>ì´ë¥¼ í†µí•´ì„œ Convolution Layerë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ í‘œí˜„í•´ ë³´ìë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>
<script type="math/tex; mode=display">
u_{ijm}^l = \sum^{H - 1}_{p = 0} \sum^{W - 1}_{q = 0} \sum^{K - 1}_{k = 0} z^{l - 1}_{i+sp,j+sq,k}w^l_{p,q,k,m} + b^l_m
\tag{equation (3)}</script><p>ì—¬ê¸°ì„œ $z$í–‰ë ¬ì€ $u$í–‰ë ¬ì— activation functionì„ ì”Œì›Œ ë†“ì€ ê²ƒì´ë¼ê³  ìƒê°í•˜ë©´ í¸í•˜ë‹¤.</p>
<p>ì´ë¥¼ ê·¸ë¦¼ìœ¼ë¡œ í‘œí˜„í•´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. </p>
<p align="center"><img src="https://kimh060612.github.io/img/CNN.png" width="100%"></p>

<p>ìœ„ ìˆ˜ì‹ì—ì„œëŠ” ì•„ì§ ì •ì˜ë˜ì§€ ì•Šì€ ë¶€ë¶„, ì„¤ëª…ë˜ì§€ ì•Šì€ ë¶€ë¶„ì´ ë§ë‹¤. ì²«ë²ˆì§¸ë¡œ $H$, $W$, $K$ì˜ ì˜ë¯¸, ê·¸ë¦¬ê³  index ë¶€ë¶„ì˜ $s$ì˜ ì˜ë¯¸ì´ë‹¤. ë˜í•œ, ìœ„ì˜ ì—°ì‚°ì€ Correlationì¸ë° ì™œ Convolution ì—°ì‚°ì´ë¼ê³  í•˜ëŠ” ê²ƒì¼ê¹Œ? </p>
<p>ì¼ë‹¨ ì²«ë²ˆì§¸ëŠ” $H$, $W$, $K$ì¸ë°, ì´ëŠ” ê°ê° Weightì˜ ë†’ì´, ë„ˆë¹„, ì±„ë„ìˆ˜ì´ë‹¤. ê·¸ë¦¬ê³  index ë¶€ë¶„ì˜ $s$ëŠ” Strideì´ë‹¤. Convolution ì—°ì‚°ì˜ Weightë¥¼ ì˜®ê²¨ê°€ë©´ì„œ ê³±ì…‰ì„ í• ë•Œ ì–¼ë§ˆë‚˜ ì˜®ê¸¸ì§€ë¥¼ ê²°ì •í•œë‹¤. ì´ ê°’ì„ í‚¤ìš¸ìˆ˜ë¡ ê²°ê³¼ ì´ë¯¸ì§€ì˜ í¬ê¸°ê°€ ì‘ì•„ì§„ë‹¤. ìì„¸í•œ ê²ƒì„ í•˜ë‚˜í•˜ë‚˜ ê¹Œë³¼ë ¤ë©´ ì˜¤ë˜ ê±¸ë¦¬ë‹ˆ, ì´ ë¶€ë¶„ì€ í˜¼ìì„œ ì˜ ìƒê°í•´ ë³´ëŠ”ê²Œ ì¢‹ì„ ê²ƒ ê°™ë‹¤. ì–´ë””ê¹Œì§€ë‚˜ ì´ ë¬¸ì„œëŠ” ì…ë¬¸ì„œê°€ ì•„ë‹ˆë¼ëŠ” ì ì„ ì•Œì•„ì£¼ì—ˆìœ¼ë©´ ì¢‹ê² ë‹¤. ê¸°ì¡´ì— Tensorflow/Pytorchë§Œì„ ì‚¬ìš©í•˜ë˜ ì‚¬ëŒë“¤ì—ê²Œ ì´ë¡ ì„ ì œê³µí•˜ê³ ì í•¨ì´ë‹¤.</p>
<h3 id="Back-Propagation-of-CNN"><a href="#Back-Propagation-of-CNN" class="headerlink" title="Back Propagation of CNN"></a>Back Propagation of CNN</h3><hr>
<p>ì. ë³¸ê²©ì ìœ¼ë¡œ ì–´ë ¤ìš´ ë¶€ë¶„ì´ë‹¤. ì•ìœ¼ë¡œ Deep learning ê°•ì˜ë¥¼ ì¨ë‚´ë ¤ê°€ë©´ì„œ ì´ë³´ë‹¤ ì–´ë ¤ìš´ ë¶€ë¶„ì€ ì—†ë‹¤. ê·¸ë¦¬ê³  í•„ìê°€ ìƒê°í•˜ê¸°ì—ë„ ì“¸ëª¨ê°€ ì—†ë‹¤. ë‹¨ì§€ ì§€ì  ìœ í¬ë¥¼ ìœ„í•´ì„œ ì½ì–´ì£¼ê¸°ë¥¼ ë°”ë¼ë©° í‹€ë¦° ë¶€ë¶„ì´ ìˆë‹¤ë©´ ì§€ì í•´ ì£¼ê¸°ë¥¼ ë°”ë€ë‹¤. </p>
<p>ê·¸ ì „ì—, ì™œ í•„ìëŠ” êµ³ì´ ì´ íŒŒíŠ¸ë¥¼ ì¨ë‚´ë ¤ ê°€ëŠ”ê°€ë¥¼ ì ì–´ë³´ë„ë¡ í•˜ê² ë‹¤. (ì¡ë‹´ì´ë‹ˆ êµ³ì´ ì•ˆ ì½ì–´ë„ ìƒê´€ ì—†ë‹¤.) ìµœê·¼ì˜ Deep learning ê°œë°œì€ Auto Grad ê³„ì—´ì˜ ì•Œê³ ë¦¬ì¦˜ë“¤ì„ í™œìš©í•˜ì—¬ ì•ë¨¹ì„ ì—°ì‚°ë§Œì„ ì •ì˜í•˜ë©´ ì•Œì•„ì„œ ì—­ì „íŒŒ ìˆ˜ì‹ì´ ê³„ì‚°ë˜ì–´ BackPropagationì„ í¸ë¦¬í•˜ê²Œ í•  ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ëª¨ë“  ê²ƒì„ ë§¡ê¸°ê³  ê°œë°œë§Œ í•˜ëŠ” ê²ƒì´ ê³¼ì—° ì¢‹ì€ ê°œë°œì/ì—°êµ¬ì› ì´ë¼ê³  í•  ìˆ˜ ìˆì„ê¹Œ? í•„ìš”í•˜ë‹¤ë©´ ë” ê¹Šì€ ì¸ì‚¬ì´íŠ¸ë¥¼ ì–»ì–´ì„œ ë¬¸ì œë¥¼ í•´ê²°í•´ì•¼í•  í•„ìš”ê°€ ìˆë‹¤. ì´ ê¸€ì€ ê·¸ëŸ° ì‚¬ëŒë“¤ì„ ìœ„í•¨ì´ê¸°ë„ í•˜ê³  ë‚˜ì²˜ëŸ¼ í•™ë¬¸ ë³€íƒœë“¤ì„ ìœ„í•œ ê²ƒì´ê¸°ë„ í•˜ë‹¤. ê·¸ëŸ¬ë‹ˆ ì´ íŒŒíŠ¸ê°€ í•„ìš” ì—†ë‹¤ê³  íŒë‹¨ë˜ë©´ ì½ì§€ ì•ŠëŠ” ê²ƒì„ ì¶”ì²œí•˜ê³ , ë§Œì•½ í‹€ë¦° ê²ƒì´ ìˆë‹¤ë©´ ë¶€ë”” ì—°ë½í•´ì„œ ì•Œë ¤ì£¼ì—ˆìœ¼ë©´ ì¢‹ê² ë‹¤. í™˜ì˜í•˜ëŠ” ë§ˆìŒìœ¼ë¡œ ë°›ì•„ë“¤ì´ê³  ìˆ˜ì •í•˜ë„ë¡ í•˜ê² ë‹¤.</p>
<p>ì‚¬ì¡±ì´ ê¸¸ì—ˆëŠ”ë°, ê·¸ë˜ì„œ Back Propagationì´ ì–´ë–»ê²Œ ì •ì˜ë˜ëŠ” ê²ƒì¼ê¹Œ? í° í‹€ì€ FCNNê³¼ ë‹¤ë¥¼ ë°”ê°€ ì—†ë‹¤. Weightë¥¼ ì—…ë°ì´íŠ¸í•¨ì— ìˆì–´ì„œ Chain Ruleì„ í™œìš©í•˜ëŠ” ê²ƒì´ë‹¤. ê·¸ë ‡ë‹¤ë©´ ì–´ë–»ê²Œ ê·¸ê²ƒì„ ì§„í–‰í•  ê²ƒì¸ê°€?<br>ìš°ì„  ì²«ë²ˆì§¸ë¡œ ë¯¸ë¶„ë¶€í„° ì¨ë‚´ë ¤ ê°€ë³´ì. </p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial w^l_{ijmk}}
\tag{def 2}</script><p>ì´ê²ƒì„ êµ¬í•´ì„œ Weightë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ê²ƒì´ Back Propagationì˜ í•µì‹¬ì´ë‹¤. ê·¸ë ‡ë‹¤ë©´ FCNNê³¼ ë˜‘ê°™ì´ ì¼ë°˜í™”ëœ Delta Ruleì„ í™œìš©í•´ ë³´ëŠ” ê²ƒìœ¼ë¡œ ì‹œì‘í•˜ì. ì´ë¥¼ ìœ„í•´ì„œ chain ruleì„ ì ìš©í•´ ë³´ë©´ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤. </p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial w^l_{ijmk}} = \sum_{p=0}^{H'}\sum_{q=0}^{W'} \frac{\partial E}{\partial u^l_{pqm}}\frac{\partial u^l_{pqm}}{\partial w^l_{ijmk}}
\tag{equation (4)}</script><p>ì´ë•Œ, $Hâ€™$, $Wâ€™$ëŠ” Convolution outputì˜ ê²°ê³¼ Feature mapì˜ ë†’ì´ì™€ ë„ˆë¹„ì´ë‹¤.<br>FCNNë•Œì™€ ë˜‘ê°™ì´ í•œë‹¤ë©´ ë‹¤ìŒê³¼ ê°™ì´ Deltaë¥¼ ì •ì˜í•˜ê³  ì‹ì„ ìˆ˜ì •í•  ìˆ˜ ìˆë‹¤. </p>
<script type="math/tex; mode=display">
\delta_{pqm}^l = \frac{\partial E}{\partial u^l_{pqm}}
\tag{def 3}</script><p>ê·¸ë¦¬ê³  ë‹¤ìŒê³¼ ê°™ì´ ì‹ì„ ìœ ë„í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤. </p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial w^l_{ijmk}} = \sum_{p=0}^{H'}\sum_{q=0}^{W'} \delta_{pqm}^l  \frac{\partial u^l_{pqm}}{\partial w^l_{ijmk}} =  \sum_{p=0}^{H'}\sum_{q=0}^{W'} \delta_{pqm}^l z^{l - 1}_{i+sp, j+sq, k}
\tag{equation (5)}</script><p>ê²°êµ­ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ ê°€ëŠ¥í•˜ë‹¤.</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial w^l_{ijmk}} = (\delta_{m}^l * z^{l-1}_k)_{ij}
\tag{equation (6)}</script><p>ì´ì œ deltaë¥¼ ì •ì˜í–ˆìœ¼ë‹ˆ, ì•ì¸µì˜ deltaì™€ ë’·ì¸µì˜ deltaê°„ì˜ ê´€ê³„ë¥¼ ë°í˜€ë‚´ì„œ ì—°ì‚°ì„ íš¨ìœ¨í™” í•˜ë©´ ëœë‹¤. ì´ ê³¼ì •ì„ ì†ìœ¼ë¡œ ìœ ë„í•˜ëŠ” ê²ƒì€ í•„ìê°€ ìƒê°í•´ë„ ì‹¤ìš©ì  ì¸¡ë©´ì—ì„œëŠ” ì •ë§ ì“¸ë°ê°€ ì—†ë‹¤. ì™œëƒí•˜ë©´ í˜„ëŒ€ì˜ ì‹ ê²½ë§ êµ¬ì¡°ëŠ” ë„ˆë¬´ ë³µì¡í•´ì ¸ì„œ ì´ê±¸ ìœ ë„í–ˆë‹¤ ì³ë„ ë‹¤ë¥¸ êµ¬ì¡°ë“¤ì´ ì •ë§ ë§ì´ ë•Œë¬¸ì— ì¨ë¨¹ì„ ìˆ˜ê°€ ì—†ê¸° ë•Œë¬¸ì´ë‹¤. í•˜ì§€ë§Œ ì•„ì£¼ ì œí•œì ì¸ ê²½ìš°ì— ëŒ€í•´ì„œ ì´ê±¸ ìœ ë„í•´ ë³´ë„ë¡ í•˜ê² ë‹¤. </p>
<ol>
<li>Convolution - Convolution layer ì—ì„œì˜ Delta ì í™”ì‹</li>
</ol>
<p>ìš°ì„  ë‹¤ì‹œ í•œë²ˆ deltaì—ì„œ chain ruleì„ ì ìš©í•´ ë³´ë„ë¡ í•˜ê² ë‹¤ . ì´ ê³¼ì •ì€ FCNNì—ì„œë„ í–ˆì„ ê²ƒì´ë‹¤. ë”°ë¼ì„œ ìµœëŒ€í•œ ê°„ê²°í•˜ê²Œ ì§„í–‰í•´ ë³´ë„ë¡ í•˜ê² ë‹¤.  </p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial u^l_{pqm}} = \sum_{x=0}^{H''}\sum_{y=0}^{W''} \sum_{c=0}^{C''}  \frac{\partial E}{\partial u^{l + 1}_{p - sx,q - sy, c}} \frac{\partial u^{l + 1}_{p - sx,q - sy, c}}{\partial u^l_{pqm}}
\tag{equation (7)}</script><p>ì´ë•Œ $Hâ€™â€™$, $Wâ€™â€™$, $Câ€™â€™$ëŠ” ë‹¤ìŒ ì¸µì—ì„œì˜ Feature Mapì˜ í¬ê¸°ì´ë‹¤.</p>
<p>ì´ë¥¼ ì „ê°œí•´ ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial u^l_{pqm}} = \sum_{x=0}^{H''}\sum_{y=0}^{W''} \sum_{c=0}^{C''} \delta_{p-sx,q-sy,c}^{l+1} w^{l+1}_{xycm} f'(u^{l}_{pqm})
\tag{equation (8)}</script><p>ì´ëŠ” ë‹¤ìŒê³¼ ê°™ì´ Convolution ì—°ì‚°ìœ¼ë¡œ í‘œí˜„ë  ìˆ˜ ìˆë‹¤.</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial u^l_{pqm}} = (\delta^{l+1} * w_m^{l+1}) \odot f'(u^l_m)
\tag{equation (9)}</script><p>ìœ„ì˜ $\odot$ì€ ì„±ë¶„ ë¼ë¦¬ì˜ ê³±(element wise multiplication)ì„ ì˜ë¯¸í•œë‹¤. </p>
<ol>
<li>Convolution - Pooling - Convolution ì—ì„œì˜ Delta ì í™”ì‹</li>
</ol>
<p>ìœ„ì—ì„œ deltaë¥¼ ìœ ë„í•¨ì— ìˆì–´ì„œ í•œ ì¸µì´ ë” ì¶”ê°€ë  ë¿ì´ë‹¤. ë‹¤ìŒê³¼ ê°™ì´ ë§ì´ë‹¤. ì—¬ê¸°ì„œëŠ” Max Pooling, Average Poolingì„ ì˜ˆë¡œ ë“¤ì–´ë³´ê² ë‹¤. </p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial u^l_{pqm}} = \sum_{x=0}^{H''}\sum_{y=0}^{W''} \sum_{c=0}^{C''}  \frac{\partial E}{\partial u^{l + 2}_{p' - sx,q' - sy, c}} \frac{\partial u^{l + 2}_{p' - sx,q' - sy, c}}{\partial u^{l + 1}_{p'q'm}} \frac{u^{l + 1}_{p'q'm}}{u^l_{pqm}}
\tag{equation (10)}</script><script type="math/tex; mode=display">
\frac{\partial E}{\partial u^l_{pqm}} = \sum_{x=0}^{H''}\sum_{y=0}^{W''} \sum_{c=0}^{C''}  \delta_{p-sx,q-sy,c}^{l+1} w^{l+1}_{xycm} \frac{\partial u^{l + 2}_{p' - sx,q' - sy, c}}{\partial u^{l + 1}_{p'q'm}} f'(u^{l}_{pqm})
\tag{equation (11)}</script><p>ìœ„ ì‹ì—ì„œ ì¤‘ê°„ì— ìˆëŠ” $l+1$ì¸µì´ Pooling ì¸µì´ë‹¤. ì´ ë¯¸ë¶„ì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤. </p>
<ul>
<li>Max Poolingì˜ ê²½ìš°</li>
</ul>
<script type="math/tex; mode=display">
\frac{\partial u^{l + 2}_{p' - sx,q' - sy, c}}{\partial u^{l + 1}_{p'q'm}} = 
\begin{cases}
    1 \space \space \space \space \space \text{if p, q ì„±ë¶„ì´ ìµœëŒ“ê°’ì´ì—ˆì„ ê²½ìš°} \\
    0 \space \space \space \space \space \text{otherwise}
\end{cases} 
\tag{equation (12)}</script><ul>
<li>Average Poolingì˜ ê²½ìš°</li>
</ul>
<script type="math/tex; mode=display">
\frac{\partial u^{l + 2}_{p' - sx,q' - sy, c}}{\partial u^{l + 1}_{p'q'm}} = \frac{1}{H''' * W'''} 
\tag{equation (12)}</script><p>ì—¬ê¸°ì„œ $Hâ€™â€™â€™$,$Wâ€™â€™â€™$ëŠ” Pooling Layerì˜ í¬ê¸°ì´ë‹¤.</p>
<p>ê²°êµ­ Pooling layerê¹Œì§€ í¬í•¨í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ convolution ì—°ì‚°ìœ¼ë¡œ ì •ì˜í•  ìˆ˜ ìˆë‹¤.</p>
<script type="math/tex; mode=display">
\delta_{pqm}^l = \text{Upsampling}[(\delta^{l + 2} * w^{l + 2}_m)] \odot f'(u^l_m)
\tag{equation (13)}</script><p>ì–´ë””ê¹Œì§€ë‚˜ ì´ë ‡ê²Œ ì—°ì‚°ì„ í•  ìˆ˜ ìˆëŠ” ì´ìœ ëŠ” Pooling layerëŠ” ì—…ë°ì´íŠ¸ë¥¼ í•  í•„ìš”ê°€ ì—†ê¸° ë•Œë¬¸ì´ë‹¤. ë§Œì•½ ì—…ë°ì´íŠ¸ë¥¼ í•  í”¼ë¼ë¯¸í„°ê°€ ìˆë‹¤ë©´ â€œì œëŒ€ë¡œâ€ ë‹¤ì‹œ delta ruleì˜ ë°©ì •ì‹ì„ ìˆ˜ì •í•´ ì¤˜ì•¼ í•œë‹¤.</p>
<ol>
<li>ì´ê²Œ ì •ë§ ì“¸ë° ì—†ëŠ” ì´ìœ </li>
</ol>
<p>í˜„ëŒ€ì˜ ì‹ ê²½ë§ì€ ì—ì‹œë‹¹ì´ˆ Convolution layerì—ì„œ íƒˆê°í•˜ëŠ” ë¶„ìœ„ê¸° ì¸ë°ë‹¤ê°€ Convolution - Feed Forward ê´€ê³„ë‚˜ ResNetê°™ì€ í˜„ëŒ€ì˜ ì‹ ê²½ë§ êµ¬ì¡°ì—ì„œëŠ” ì´ëŸ° ë³µì¡í•œ ìˆ˜ì‹ìœ¼ë¡œ êµ¬í˜„í•˜ëŠ” ê²ƒì€ ì‚¬ì‹¤ìƒ ë¶ˆê°€ëŠ¥ì— ê°€ê¹ë‹¤. ê·¸ëŸ¬ë‹ˆ ìš°ë¦¬ëŠ” Autogradë¥¼ ë¯¿ê³  ì´ëŸ°ê±´ ê·¸ëƒ¥ ì§€ì  ìœ í¬ë¡œë§Œ ì•Œì•„ ë‘ë„ë¡ í•˜ì.</p>
<p>ê·¸ë¦¬ê³  ë§ˆì§€ë§‰ìœ¼ë¡œ Biasì˜ Update ë°©ë²•ì„ ì•Œì•„ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial b_m^l} = \sum^{H_o - 1}_{p = 0} \sum^{W_o - 1}_{q = 0} \frac{\partial E}{\partial u_{pqm}^l} \frac{\partial u_{pqm}^l}{\partial b_m^l} 
\tag{equation (14)}</script><p>ì´ë•Œ, ê³±ì…ˆ termì˜ ë’· í•­ì€ ì „ë¶€ 1ì´ë¯€ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ì‹ì´ ì„±ë¦½í•œë‹¤.</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial b_m^l} = \sum^{H' - 1}_{p = 0} \sum^{W' - 1}_{q = 0} \delta_{pqm}^l
\tag{equation (15)}</script><p>ì´ê±¸ë¡œ CNNì˜ ì´ë¡  ë¶€ë¶„ì€ ëë‚¬ë‹¤. ë‹¤ìŒì—ëŠ” ì‹¤ìŠµ ë¶€ë¶„ìœ¼ë¡œ ì°¾ì•„ì˜¤ë„ë¡ í•˜ê² ë‹¤.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kimh060612.github.io/2022/03/05/FCNN-B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Michael Kim">
      <meta itemprop="description" content="Backend Engineering, Deep learning, Algorithm & DS, CS ì „ê³µ ì§€ì‹ ê³µë¶€ ì €ì¥ì†Œ">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Michael's Study Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/05/FCNN-B/" class="post-title-link" itemprop="url">Fully Connected Neural Network ê°•ì˜ ë‚´ìš© part.B</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-03-05 17:33:29" itemprop="dateCreated datePublished" datetime="2022-03-05T17:33:29+09:00">2022-03-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-05-30 00:00:32" itemprop="dateModified" datetime="2022-05-30T00:00:32+09:00">2022-05-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2022/03/05/FCNN-B/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2022/03/05/FCNN-B/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><em>ë³¸ í¬ìŠ¤íŠ¸ëŠ” Hands-on Machine learning 2nd Edition, CS231n, Tensorflow ê³µì‹ documentë¥¼ ì°¸ì¡°í•˜ì—¬ ì‘ì„±ë˜ì—ˆìŒì„ ì•Œë¦½ë‹ˆë‹¤.</em></p>
<h6 id="Index"><a href="#Index" class="headerlink" title="Index"></a>Index</h6><ol>
<li><del>Basic of Neural Network</del>  </li>
<li><del>Definition of Fully Connected Neural Network(FCNN)</del></li>
<li><del>Feed Forward of FCNN</del></li>
<li><del>Gradient Descent</del></li>
<li><del>Back Propagation of FCNN</del></li>
<li>Partice(+ ë¶€ë¡ Hyper parameter tuning)</li>
</ol>
<p><br></p>
<h4 id="6-Partice-ë¶€ë¡-Hyper-parameter-tuning"><a href="#6-Partice-ë¶€ë¡-Hyper-parameter-tuning" class="headerlink" title="6. Partice(+ ë¶€ë¡ Hyper parameter tuning)"></a>6. Partice(+ ë¶€ë¡ Hyper parameter tuning)</h4><hr>
<p>ì ì‹¤ìŠµ ì‹œê°„ì´ë‹¤. ì™œ ì‹¤ìŠµì„ Part. Bë¡œ ëºëŠëƒ? FCNNì´ ë­ í• ê²Œ ìˆë‹¤ê³ ?<br>ë­ í• ê²Œ ìˆê² ë‹¤. Tensorflow 2ê°€ ëŒ€ì¶© ì–´ë–»ê²Œ ì´ë£¨ì–´ ì¡ŒëŠ”ì§€ ì„¤ëª…í•˜ê¸° ìœ„í•´ ë¶„ëŸ‰ ì¡°ì ˆì„ ìœ„í•´ì„œ ëº€ê²ƒì´ë‹¤.<br>ë¬´ì—‡ë³´ë‹¤ Part. A ì“°ëŠ”ë° ìˆ˜ì‹ì„ ë„ˆë¬´ ë§ì´ ì¨ì„œ í˜ë“¤ì–´ì„œ ë¶„ë¦¬í–ˆë‹¤. <del>Tlqkf</del><br>ì, ìš°ì„  ì‹¤ìŠµì— ë“¤ì–´ê°€ê¸°ì— ì•ì„œ, TF 2ë¥¼ ì• ì •í•˜ëŠ” ë‚˜ë¡œì¨ëŠ” ì•ìœ¼ë¡œ ì´ ìŠ¤í„°ë”” í¬ìŠ¤íŠ¸ì— ì‘ì„±ë  ëŒ€ë¶€ë¶„ì˜ ì†ŒìŠ¤ì½”ë“œë¥¼ ê¿°ëš«ëŠ” êµ¬í˜„ ì²´ê³„ë¥¼ ë¨¼ì € ì„¤ëª…í•˜ê³  ë„˜ì–´ê°€ê² ë‹¤.<br>ë‹¤ìŒ ì‚¬ì§„ì„ ë³´ì.</p>
<p align="center"><img src="http://kimh060612.github.io/img/API.png" width="70%" height="70%"></p>

<p><em>ì¶œì²˜: pyimagesearch blog: <a target="_blank" rel="noopener" href="https://www.pyimagesearch.com/2019/10/28/3-ways-to-create-a-keras-model-with-tensorflow-2-0-sequential-functional-and-model-subclassing/">ë§í¬</a></em></p>
<p>ìœ„ ê·¸ë¦¼ì—ì„œ í•„ìëŠ” ëŒ€ë¶€ë¶„ì˜ ì½”ë“œë¥¼ <strong>Model Subclassing</strong> ë°©ì‹ìœ¼ë¡œ êµ¬í˜„í•  ê²ƒì´ë‹¤. êµ¬í˜„ í•˜ë©´ì„œ ì„¤ëª…í•  í„°ì´ë‹ˆ ì˜ ë”°ë¼ì™€ ì£¼ê¸°ë¥¼ ë°”ë€ë‹¤.<br>ì—¬ê¸°ì„œë¶€í„°ëŠ” ëŒ€í•™êµ ê°•ì˜ ìˆ˜ì¤€ì˜ <strong>ê°ì²´ì§€í–¥í”„ë¡œê·¸ë˜ë°</strong> ì§€ì‹ì„ ê°–ì¶”ì§€ ì•Šìœ¼ë©´ ì½ê¸° í˜ë“¤ ìˆ˜ ìˆë‹¤. â€œìƒì†â€, â€œì˜¤ë²„ë¼ì´ë”©â€ì˜ ê°œë…ì´ë¼ë„ ì‚´í´ë³´ê³  ì˜¤ì.</p>
<h6 id="6-1-Model-Subclassing"><a href="#6-1-Model-Subclassing" class="headerlink" title="6-1. Model Subclassing"></a>6-1. Model Subclassing</h6><hr>
<p>ìš°ì„  ì¤€ë¹„í•œ ì†ŒìŠ¤ë¶€í„° ë³´ê³  ì‹œì‘í•˜ì.<br><em>file: model/FCNN1.py</em><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FCNN</span>(keras.Model):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, _units = [<span class="number">56</span>, <span class="number">56</span>], _activation = [<span class="string">&#x27;relu&#x27;</span>, <span class="string">&#x27;relu&#x27;</span>], **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)</span><br><span class="line">        self.Hidden1 = keras.layers.Dense(units=_units[<span class="number">0</span>], activation=_activation[<span class="number">0</span>], kernel_initializer=<span class="string">&quot;normal&quot;</span>)</span><br><span class="line">        self.Hidden2 = keras.layers.Dense(units=_units[<span class="number">1</span>], activation=_activation[<span class="number">1</span>], kernel_initializer=<span class="string">&quot;normal&quot;</span>)</span><br><span class="line">        self._output = keras.layers.Dense(units=<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, Input</span>):</span><br><span class="line">        hidden1 = self.Hidden1(Input)</span><br><span class="line">        hidden2 = self.Hidden2(hidden1)</span><br><span class="line">        Output = self._output(hidden2)</span><br><span class="line">        <span class="keyword">return</span> Output</span><br></pre></td></tr></table></figure><br>ì, ë³„ê±° ì—†ë‹¤. kerasë¥¼ ì¨ë´¤ë‹¤ë©´ ë­”ì§€ ë°”ë¡œ ê°ì´ ì˜¬ ê²ƒì´ë‹¤.<br>ì´ ë¶€ë¶„ì— ëŒ€í•´ì„œëŠ” í•¨ìˆ˜ì— ëŒ€í•œ ì„¤ëª…ë³´ë‹¤ëŠ” classì— ëŒ€í•œ ì„¤ëª…ì„ í•´ì•¼í•  ê²ƒ ê°™ë‹¤. ë°”ë¡œ FCNN classê°€ ìƒì†ì„ ë°›ì€ ë¶€ëª¨ í´ë˜ìŠ¤ì¸ keras.Model í´ë˜ìŠ¤ì— ê´€í•´ì„œì´ë‹¤.</p>
<p>keras.Model classëŠ” ì¼€ë¼ìŠ¤ì—ì„œ Deep learningì„ ì§„í–‰í•˜ëŠ” ëª¨ë¸ì„ ì •ì˜í•´ì£¼ëŠ” classì´ë‹¤. ìš°ë¦¬ê°€ ì´ë¯¸ ì¡´ì¬í•˜ëŠ” layerë¥¼ ê°€ì ¸ë‹¤ê°€ íŠ¹ì • ìˆœì„œë¡œ ì—°ì‚°ì„ ì§„í–‰í•˜ëŠ” graphë¥¼ ë§Œë“¤ì–´ ë‚´ê¸° ìœ„í•œ classì´ë‹¤. í•˜ì§€ë§Œ ê·¸ë ‡ê²Œ ì–´ë µê²Œ ìƒê°í•˜ì§€ ë§ì. ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ë³´ë©´ ë°”ë¡œ ë‹µì´ ë‚˜ì˜¨ë‹¤.</p>
<p>ì—¬ê¸°ì„œëŠ” ìƒì„±ìì™€ callì´ë¼ëŠ” í•¨ìˆ˜ë¥¼ ì˜¤ë²„ë¼ì´ë”©ì„ í†µí•´ì„œ ì‚¬ìš©ìê°€ ì¬ ì •ì˜ë¥¼ í•´ì„œ ì‚¬ìš©í•œë‹¤. callì€ ìš°ë¦¬ê°€ êµ¬í˜„í•˜ê³ ì í•˜ëŠ” modelì´ feed forwardë¥¼ ì§„í–‰í• ë•Œ í˜¸ì¶œë˜ëŠ” í•¨ìˆ˜ì´ë‹¤. ìƒì„±ìëŠ” ì‚¬ìš©í•  í­ì´ ë„“ë‹¤. ì—¬ê¸°ì„œëŠ” modelì„ êµ¬ì„±í•˜ëŠ” layerë¥¼ ì •ì˜í•˜ëŠ”ë° ì‚¬ìš©í•˜ì˜€ëŠ”ë°, ê¼­ ê·¸ ì—­í• ë§Œ í•  í•„ìš”ëŠ” ì—†ëŠ” ê²ƒì´ë‹¤.</p>
<p>ê°€íƒ€ë¶€íƒ€ ë§ì´ ë§ì•˜ëŠ”ë°, ì‹¤ì œ ì–´ë–»ê²Œ ë™ì‘ì„ ì‹œí‚¤ëŠ”ê°€?</p>
<p><em>file: train_MNIST.py</em><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> model.FCNN1 <span class="keyword">import</span> FCNN</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">mnist = keras.datasets.mnist</span><br><span class="line"></span><br><span class="line">(train_img, train_labels), (test_img, test_labels) = mnist.load_data()</span><br><span class="line"></span><br><span class="line">train_img, test_img = train_img.reshape([-<span class="number">1</span>, <span class="number">784</span>]), test_img.reshape([-<span class="number">1</span>, <span class="number">784</span>])</span><br><span class="line"></span><br><span class="line">train_img = train_img.astype(np.float32) / <span class="number">255.</span></span><br><span class="line">test_img = test_img.astype(np.float32) / <span class="number">255.</span></span><br><span class="line"></span><br><span class="line">model = FCNN()</span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adam&#x27;</span>, loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>, metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br><span class="line">model.fit(train_img, train_labels, batch_size = <span class="number">32</span>, epochs = <span class="number">15</span>, verbose = <span class="number">1</span>, validation_split = <span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line">model.evaluate(test_img, test_labels, verbose=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><br>ê°„ë‹¨í•˜ë‹¤. keras.Model classë¥¼ ìƒì† ë°›ì•˜ìœ¼ë‹ˆ, ê·¸ê³³ì— ìˆëŠ” ê¸°ë³¸ í•¨ìˆ˜ë“¤ì„ ëª¨ë‘ ì‚¬ìš©í•  ìˆ˜ìˆë‹¤. fit methodë¡œ í•™ìŠµì„ ì§„í–‰í•˜ê³  evaluteë¡œ testë°ì´í„°ë¡œ ëª¨ë¸ì„ í‰ê°€í•œë‹¤.<br>ì´ëŠ” ê¸°ì¡´ì— kerasì˜ ì‚¬ìš©ë²•ê³¼ ë³„ë°˜ ë‹¤ë¥¸ê²Œ ì—†ë‹¤.</p>
<p>ì—¬ê¸°ê¹Œì§€ëŠ” ì‰½ë‹¤. í•˜ì§€ë§Œ ì´ëŸ¬ê³  ëë‚¼ê±°ë©´ ì‹œì‘ë„ í•˜ì§€ ì•Šì•˜ë‹¤.</p>
<p>ë‹¤ìŒì„ ì§„ì§œ ìì„¸í•˜ê²Œ ì„¤ëª…í•  ê²ƒì´ë‹¤.</p>
<p><em>file: model/layer.py</em><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"></span><br><span class="line"><span class="comment"># Weight Sumì„ ì§„í–‰í•˜ëŠ” Layerë¥¼ ì •ì˜í•˜ëŠ” ì˜ˆì œ 1</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">WeightSum</span>(keras.layers.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, units=<span class="number">32</span>, input_dim=<span class="number">32</span>, trainable=<span class="literal">True</span>, name=<span class="literal">None</span>, dtype=<span class="literal">None</span>, dynamic=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(trainable=trainable, name=name, dtype=dtype, dynamic=dynamic, **kwargs)</span><br><span class="line">        <span class="comment"># tf.Variableì„ í™œìš©í•˜ëŠ” ì˜ˆì‹œ</span></span><br><span class="line">        <span class="comment"># ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ ì´ˆê¸°í™” í•˜ê¸° ìœ„í•œ ê°ì²´</span></span><br><span class="line">        w_init = tf.random_normal_initializer()</span><br><span class="line">        <span class="comment"># ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ tf.Variableë¡œ ì •ì˜í•¨. ë‹¹ì—°íˆ í•™ìŠµí•´ì•¼í•˜ë‹ˆ training parameterë¥¼ trueë¡œ ë‘ .</span></span><br><span class="line">        self.Weight = tf.Variable(</span><br><span class="line">            initial_value = w_init(shape=(input_dim, units), dtype=<span class="string">&quot;float32&quot;</span>),</span><br><span class="line">            trainable = <span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line">        b_init = tf.zeros_initializer()</span><br><span class="line">        <span class="comment"># ìœ„ë‘ ê°™ìŒ. ë³„ë°˜ ë‹¤ë¥¼ê±° X</span></span><br><span class="line">        self.Bias = tf.Variable(</span><br><span class="line">            initial_value = b_init(shape=(units, ), dtype=<span class="string">&quot;float32&quot;</span>),</span><br><span class="line">            trainable = <span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># add_weightë¥¼ í™œìš©í•˜ëŠ” ì˜ˆì‹œ - Short Cut</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        # ì´ëŠ” keras.layers.Layerì˜ method ì¤‘ì—ì„œ add_weightë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„. </span></span><br><span class="line"><span class="string">        # ì£¼ë¡œ trainingì„ ì‹œí‚¤ê¸° ìœ„í•œ í–‰ë ¬ì„ ì´ë ‡ê²Œ ì„ ì–¸í•´ì„œ ë‚˜ì¤‘ì— í¸í•˜ê²Œ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•œ ëª©ì ì´ í¼. </span></span><br><span class="line"><span class="string">        self.Weight = self.add_weight(</span></span><br><span class="line"><span class="string">            shape=(input_dim, units), initializer=&quot;random_normal&quot;, trainable=True</span></span><br><span class="line"><span class="string">        )</span></span><br><span class="line"><span class="string">        self.Bias = self.add_weight(shape=(units,), initializer=&quot;zeros&quot;, trainable=True)</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, Input</span>):</span><br><span class="line">        <span class="comment"># í–‰ë ¬ ê³±ì„ ìœ„í•œ tf í•¨ìˆ˜ì„. ë³„ê±° ì—†ìŒ</span></span><br><span class="line">        <span class="comment"># ê·¸ëƒ¥ U = WZ + B êµ¬í˜„í•œê±°. </span></span><br><span class="line">        <span class="keyword">return</span> tf.matmul(Input, self.Weight) + self.Bias</span><br><span class="line"></span><br><span class="line"><span class="comment"># Weight Sumì„ ì§„í–‰í•˜ëŠ” Layerë¥¼ ì •ì˜í•˜ëŠ” ì˜ˆì œ 2</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">WeightSumBuild</span>(keras.layers.Layer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, _units = <span class="number">32</span>, trainable=<span class="literal">True</span>, name=<span class="literal">None</span>, dtype=<span class="literal">None</span>, dynamic=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(trainable=trainable, name=name, dtype=dtype, dynamic=dynamic, **kwargs)</span><br><span class="line">        self.units = _units</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build</span>(<span class="params">self, input_dim</span>):</span><br><span class="line">        <span class="comment"># ì´ í•¨ìˆ˜ëŠ” ë°‘ì—ì„œ ìì„¸íˆ ì„¤ëª…í•¨.</span></span><br><span class="line">        self.Weight = self.add_weight(</span><br><span class="line">            shape=(input_dim[-<span class="number">1</span>], self.units),</span><br><span class="line">            initializer = <span class="string">&quot;random_normal&quot;</span>,</span><br><span class="line">            trainable= <span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.Bias = self.add_weight(</span><br><span class="line">            shape = (self.units,),</span><br><span class="line">            initializer = <span class="string">&quot;random_normal&quot;</span>,</span><br><span class="line">            trainable = <span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">call</span>(<span class="params">self, Input</span>):</span><br><span class="line">        <span class="comment"># ìƒê¸° ë™ì¼.</span></span><br><span class="line">        <span class="keyword">return</span> tf.matmul(Input, self.Weight) + self.Bias</span><br></pre></td></tr></table></figure><br>ìœ„ì˜ 2ê°œì˜ classëŠ” í•˜ëŠ” ì§“ê±°ë¦¬ê°€ ì™„ë²½í•˜ê²Œ ë˜‘ê°™ë‹¤. í•˜ì§€ë§Œ í•˜ëŠ” ì§“ê±°ë¦¬ëŠ” ê°™ì€ë° ì•„ì£¼ ì¹˜ëª…ì ì¸ ë¶€ë¶„ì´ ì¡°ê¸ˆ ë‹¤ë¥´ë‹¤. ë°”ë¡œ build í•¨ìˆ˜ì˜ overwrittingì´ë‹¤.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#                ì…ë ¥ ë²¡í„°/í…ì„œì˜ í¬ê¸°ë¥¼ ë°›ìŒ</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build</span>(<span class="params">self, input_dim</span>):</span><br><span class="line">    <span class="comment"># ê·¸ì— ë”°ë¼ì„œ Weightì™€ Biasì˜ ì°¨ì›ì„ ê²°ì •í•¨.</span></span><br><span class="line">    <span class="comment"># ë¬¼ë¡ , BiasëŠ” ì°¨ì´ê°€ ì—†ì„ ì§€ì–¸ì •, WeightëŠ” í¬ê²Œ ì°¨ì´ê°€ ë‚˜ê²Œ ëœë‹¤.</span></span><br><span class="line">    self.Weight = self.add_weight(</span><br><span class="line">        shape=(input_dim[-<span class="number">1</span>], self.units),</span><br><span class="line">        initializer = <span class="string">&quot;random_normal&quot;</span>,</span><br><span class="line">        trainable= <span class="literal">True</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    self.Bias = self.add_weight(</span><br><span class="line">        shape = (self.units,),</span><br><span class="line">        initializer = <span class="string">&quot;random_normal&quot;</span>,</span><br><span class="line">        trainable = <span class="literal">True</span></span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>ì´ í•¨ìˆ˜ëŠ” callì´ í˜¸ì¶œë˜ê¸° ì „ì— ë¬´ì¡°ê±´ ì‹¤í–‰ë˜ëŠ” í•¨ìˆ˜ë¼ê³  ìƒê°í•˜ë©´ ëœë‹¤. ì¦‰, í˜¸ì¶œí•˜ê¸° ì „ì— Weightë¥¼ ì •ì˜í•˜ëŠ” ê²ƒì´ë‹¤. ì¦‰, ì…ë ¥ ë²¡í„°ì˜ í¬ê¸°ì— ë”°ë¼ ëª¨ë¸ì˜ í˜•íƒœê°€ ì•Œì•„ì„œ ë°”ê¾¸ê²Œ í•´ì¤„ ìˆ˜ ìˆëŠ” ê²ƒì´ë‹¤. ì´ëŠ” Imageë¥¼ ì²˜ë¦¬í• ë•Œ ì´ì ì´ ë  ìˆ˜ ìˆë‹¤.<br>Imageë¥¼ í•™ìŠµì‹œí‚¬ë•Œ, ì´ëŸ¬í•œ ì²˜ë¦¬ê°€ ì—†ìœ¼ë©´ ì´ë¯¸ì§€ë¥¼ ì „ë¶€ ë™ì¼í•œ í¬ê¸°ë¡œ ë§Œë“¤ì–´ ì£¼ì–´ì•¼ í•œë‹¤. í•˜ì§€ë§Œ build í•¨ìˆ˜ë¥¼ ì •ì˜í•´ì„œ ê·¸ë•Œ ê·¸ë•Œ ì…ë ¥ ë²¡í„°/í…ì„œì— ë”°ë¼ ì»¤ë„ì„ ìˆ˜ì •í•´ ì£¼ë©´ êµ³ì´ ê·¸ëŸ´ í•„ìš”ê°€ ì—†ë‹¤. ì „ì²˜ë¦¬ ë¹„ìš©ì´ ì¤„ì–´ë“œëŠ” ê²ƒì´ë‹¤. </p>
<p>ê·¸ë¦¬ê³ , ì´ì œ ì´ë¥¼ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•œ ì½”ë“œë¥¼ ë³´ë„ë¡ í•˜ì.</p>
<p><em>file: train_MNIST_2.py</em><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> model.FCNN2 <span class="keyword">import</span> FCNN</span><br><span class="line"></span><br><span class="line">EPOCHS = <span class="number">15</span></span><br><span class="line">LR = <span class="number">0.01</span></span><br><span class="line">BatchSize = <span class="number">32</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸°</span></span><br><span class="line">mnist = keras.datasets.mnist</span><br><span class="line">(train_img, train_labels), (test_img, test_labels) = mnist.load_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># í•„ìš”í•œ ì „ì²˜ë¦¬</span></span><br><span class="line">train_img, test_img = train_img.reshape([-<span class="number">1</span>, <span class="number">784</span>]), test_img.reshape([-<span class="number">1</span>, <span class="number">784</span>])</span><br><span class="line">train_img = train_img.astype(np.float32) / <span class="number">255.</span></span><br><span class="line">test_img = test_img.astype(np.float32) / <span class="number">255.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Train-Validation Split</span></span><br><span class="line">validation_img = train_img[-<span class="number">18000</span>:]</span><br><span class="line">validation_label = train_labels[-<span class="number">18000</span>:]</span><br><span class="line">train_img = train_img[:-<span class="number">18000</span>]</span><br><span class="line">train_labels = train_labels[:-<span class="number">18000</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train Dataì˜ ê·œí•©. &amp; Batch ë³„ë¡œ ìª¼ê°¬</span></span><br><span class="line">train_dataset = tf.data.Dataset.from_tensor_slices((train_img, train_labels))</span><br><span class="line">train_dataset = train_dataset.shuffle(buffer_size=<span class="number">1024</span>).batch(BatchSize)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Validation Dataì˜ ê·œí•© &amp; Batch ë³„ë¡œ ìª¼ê°¬</span></span><br><span class="line">validation_dataset = tf.data.Dataset.from_tensor_slices((validation_img, validation_label))</span><br><span class="line">validation_dataset = validation_dataset.batch(BatchSize)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Optimizer &amp; Loss Function ì •ì˜</span></span><br><span class="line">optimizer = keras.optimizers.Adam(learning_rate=LR)</span><br><span class="line">loss_function = keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># í•™ìŠµì´ ì˜ ë˜ê³  ìˆë‚˜ í™•ì¸í•˜ê¸° ìœ„í•œ ì§€í‘œë¥¼ í™•ì¸í•˜ê¸° ìœ„í•¨.</span></span><br><span class="line">train_accuracy = keras.metrics.SparseCategoricalAccuracy()</span><br><span class="line">val_acc_metric = keras.metrics.SparseCategoricalAccuracy()</span><br><span class="line"></span><br><span class="line">model = FCNN()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Custom Trainingì„ ìœ„í•œ ë°˜ë³µë¬¸</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCHS):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Epoch %d start&quot;</span>%epoch)</span><br><span class="line">    <span class="keyword">for</span> step, (x_batch, y_batch) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_dataset):</span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">            <span class="comment"># Modelì˜ Feed Forward</span></span><br><span class="line">            logits = model(x_batch, training=<span class="literal">True</span>)</span><br><span class="line">            <span class="comment"># Feed Forward ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ Lossë¥¼ êµ¬í•¨</span></span><br><span class="line">            loss_val = loss_function(y_batch, logits)</span><br><span class="line">        <span class="comment"># ìœ„ì˜ ê³¼ì •ì„ ë°”íƒ•ìœ¼ë¡œ gradientë¥¼ êµ¬í•¨</span></span><br><span class="line">        grad = tape.gradient(loss_val, model.trainable_weights)</span><br><span class="line">        <span class="comment"># Optimizerë¥¼ í†µí•´ì„œ Training Variablesë¥¼ ì—…ë°ì´íŠ¸</span></span><br><span class="line">        optimizer.apply_gradients(<span class="built_in">zip</span>(grad, model.trainable_weights))</span><br><span class="line">        <span class="comment"># Batch ë³„ë¡œ Training datasetì— ëŒ€í•œ ì •í™•ë„ë¥¼ êµ¬í•¨.</span></span><br><span class="line">        train_accuracy.update_state(y_batch, logits)</span><br><span class="line">        <span class="keyword">if</span> step % <span class="number">500</span> == <span class="number">0</span> :</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Training loss at step %d: %.4f&quot;</span>%(step, loss_val))</span><br><span class="line">    <span class="comment"># ì •í™•ë„ë¥¼ ê·œí•©í•´ì„œ ì¶œë ¥</span></span><br><span class="line">    train_acc = train_accuracy.result()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Training acc over epoch: %.4f&quot;</span> % (<span class="built_in">float</span>(train_acc),))</span><br><span class="line">    train_accuracy.reset_states()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Validationì„ ì§„í–‰í•¨.</span></span><br><span class="line">    <span class="keyword">for</span> x_batch_val, y_batch_val <span class="keyword">in</span> validation_dataset:</span><br><span class="line">        <span class="comment"># Validationì„ ìœ„í•œ Feed Forward</span></span><br><span class="line">        val_logits = model(x_batch_val, training = <span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># Batch ë³„ë¡œ Validation datasetì— ëŒ€í•œ ì •í™•ë„ë¥¼ êµ¬í•¨</span></span><br><span class="line">        val_acc_metric.update_state(y_batch_val, val_logits)</span><br><span class="line">    <span class="comment"># êµ¬í•œ ì •í™•ë„ë¥¼ ê·œí•©í•˜ì—¬ ì¶œë ¥í•¨.</span></span><br><span class="line">    val_acc = val_acc_metric.result()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Validation acc: %.4f&quot;</span> % (<span class="built_in">float</span>(val_acc),))</span><br><span class="line">    val_acc_metric.reset_states()</span><br></pre></td></tr></table></figure><br>ì´ëŠ” TF2ì—ì„œ ì¶”ê°€ëœ tf.GradientTapeë¥¼ í†µí•´ì„œ ì‚¬ìš©ì ì •ì˜ í•™ìŠµ ë£¨í”„ë¥¼ ë§Œë“  ê²ƒì´ë‹¤. ê° ì¤„ë§ˆë‹¤ ì£¼ì„ì„ ë‹¬ì•„ ë†“ì•˜ìœ¼ë‹ˆ Part. Aì˜ ë‚´ìš©ì„ ìˆ™ì§€í–ˆë‹¤ë©´ ê·¸ë ‡ê²Œ ì–´ë µì§€ ì•Šê²Œ ì•Œì•„ë“¤ì„ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. </p>
<p>ì´ì œ, í•œê°€ì§€ ì˜ë¬¸ì´ ë“ ë‹¤. ê·¸ë ‡ë‹¤ë©´ ìœ„ì˜ Hyper parameterë“¤ì„ ë³€í™”ì‹œì¼œê°€ë©´ì„œ modelì„ ìµœì í™” í•˜ë ¤ë©´ ë…¸ê°€ë‹¤ ë°–ì— ë‹µì´ì—†ëŠ”ê±´ê°€? ë‹µì€ ì•„ë‹ˆë‹¤. ì´ë²ˆì—ëŠ” ë§›ë³´ê¸°ë§Œ ë³´ì—¬ì¤„ ê²ƒì´ë‹¤. ì´ëŠ” scikit learnì˜ RandomizedSearchCVë¥¼ í†µí•´ì„œ í™•ì¸í•  ìˆ˜ ìˆë‹¤.</p>
<p><em>file: RandomSearch.py</em><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> reciprocal</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> RandomizedSearchCV</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">mnist = keras.datasets.mnist</span><br><span class="line">(train_img, train_labels), (test_img, test_labels) = mnist.load_data()</span><br><span class="line">train_img, test_img = train_img.reshape([-<span class="number">1</span>, <span class="number">784</span>]), test_img.reshape([-<span class="number">1</span>, <span class="number">784</span>])</span><br><span class="line">train_img = train_img.astype(np.float32) / <span class="number">255.</span></span><br><span class="line">test_img = test_img.astype(np.float32) / <span class="number">255.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ì—¬ê¸° parameterë“¤ì˜ KeyëŠ” ë¬´ì ê¶Œ ì…ë ¥ í•¨ìˆ˜ì˜ parameterì™€ ê°™ì•„ì•¼ í•œë‹¤. ì•„ë§ˆë„ **kwargsë¡œ í•œë²ˆì— ë³´ë‚´ë²„ë¦¬ëŠ” ê²ƒì¼ê±°ë‹¤.</span></span><br><span class="line">param_distribution = &#123;</span><br><span class="line">    <span class="string">&quot;n_hidden&quot;</span>: [<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],</span><br><span class="line">    <span class="string">&quot;n_neurons&quot;</span>: np.arange(<span class="number">1</span>,<span class="number">100</span>),</span><br><span class="line">    <span class="string">&quot;lr&quot;</span>: reciprocal(<span class="number">3e-4</span>, <span class="number">3e-2</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># ì´ê²ƒì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ëª¨ë¸ì„ ë§Œë“¤ì–´ì¤„ í•¨ìˆ˜ê°€ í•˜ë‚˜ í•„ìš”í•˜ë‹¤.</span></span><br><span class="line"><span class="comment"># ì—¬ê¸°ì„œëŠ” ê·¸ëƒ¥ Sequential APIë¥¼ ì‚¬ìš©í•˜ì˜€ë‹¤. ìƒê°í•˜ê¸° ê·€ì°®ì•˜ë‹¤. ã…‹;; </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Build_model</span>(<span class="params">n_hidden = <span class="number">1</span>, n_neurons=<span class="number">30</span>, lr = <span class="number">3e-3</span>, input_shape=[<span class="number">784</span>]</span>):</span><br><span class="line">    model = keras.models.Sequential()</span><br><span class="line">    model.add(keras.layers.InputLayer(input_shape=input_shape))</span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="built_in">range</span>(n_hidden):</span><br><span class="line">        model.add(keras.layers.Dense(n_neurons, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">    model.add(keras.layers.Dense(units=<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line">    optimizer = keras.optimizers.SGD(learning_rate=lr)</span><br><span class="line">    model.<span class="built_in">compile</span>(optimizer=optimizer, loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>, metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="comment"># ì´ëŠ” keras ëª¨ë¸ì„ scikit learnì—ì„œ ê´€ë¦¬í•˜ê¸° ìœ„í•´ í˜¸ì¶œí•˜ëŠ” í•¨ìˆ˜ì´ë‹¤.</span></span><br><span class="line">keras_classify = keras.wrappers.scikit_learn.KerasClassifier(Build_model)</span><br><span class="line"><span class="comment"># ì—¬ê¸°ì„œë¶€í„° ë³¸ë¡ ì´ë‹¤. parameter_distributionìœ¼ë¡œ ì£¼ì–´ì§„ ì§‘í•© í•œì—ì„œ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì˜ ëª¨ë¸ì„ íƒìƒ‰í•œë‹¤.</span></span><br><span class="line"><span class="comment"># ì—¬ê¸°ì„œëŠ” Cross-Validationì„ ì‚¬ìš©í•œë‹¤. cv í•­ì€ ëª‡ê°œë¡œ Validation-training setì„ ë¶„ë¦¬í• ì§€ ì •í•˜ëŠ” ê²ƒì´ë‹¤. </span></span><br><span class="line">rnd_search_model = RandomizedSearchCV(keras_classify, param_distributions=param_distribution, n_iter = <span class="number">10</span>, cv = <span class="number">3</span>)</span><br><span class="line"><span class="comment"># ì´ì œ ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ê°€ì§€ê³  í•™ìŠµì„ ëŒë©´ì„œ ìµœì ì˜ ëª¨ë¸ì„ íƒìƒ‰í•œë‹¤. </span></span><br><span class="line">rnd_search_model.fit(train_img, train_labels, epochs=<span class="number">10</span>, validation_data=(test_img,test_labels), callbacks=[keras.callbacks.EarlyStopping(patience=<span class="number">10</span>)])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(rnd_search_model.best_params_)</span><br></pre></td></tr></table></figure></p>
<p>ì˜¤ëŠ˜ì€ ì´ê²ƒìœ¼ë¡œ ëë‚´ë„ë¡ í•˜ì.<br>ë‹¤ìŒ í¬ìŠ¤íŠ¸ëŠ” Convolutional Neural Networkë¥¼ ì˜¤ëŠ˜ì²˜ëŸ¼ ë‹¤ë¤„ë³¼ ì˜ˆì •ì´ë‹¤.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://kimh060612.github.io/2022/03/05/FCNN-A/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Michael Kim">
      <meta itemprop="description" content="Backend Engineering, Deep learning, Algorithm & DS, CS ì „ê³µ ì§€ì‹ ê³µë¶€ ì €ì¥ì†Œ">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Michael's Study Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/05/FCNN-A/" class="post-title-link" itemprop="url">Fully Connected Neural Network ê°•ì˜ ë‚´ìš© part.A</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-03-05 17:32:02" itemprop="dateCreated datePublished" datetime="2022-03-05T17:32:02+09:00">2022-03-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-05-30 00:00:32" itemprop="dateModified" datetime="2022-05-30T00:00:32+09:00">2022-05-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2022/03/05/FCNN-A/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2022/03/05/FCNN-A/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><em>ë³¸ í¬ìŠ¤íŠ¸ëŠ” Hands-on Machine learning 2nd Edition, CS231n, Tensorflow ê³µì‹ documentë¥¼ ì°¸ì¡°í•˜ì—¬ ì‘ì„±ë˜ì—ˆìŒì„ ì•Œë¦½ë‹ˆë‹¤.</em></p>
<h3 id="Index"><a href="#Index" class="headerlink" title="Index"></a>Index</h3><ol>
<li>Basic of Neural Network  </li>
<li>Definition of Fully Connected Neural Network(FCNN)</li>
<li>Feed Forward of FCNN</li>
<li>Gradient Descent</li>
<li>Back Propagation of FCNN</li>
<li>Hyper parameter tuning</li>
<li>Partice</li>
</ol>
<p>ì—¬ê¸°ì„œëŠ” 1~5ëŠ” Part. Aì´ê³  6,7ì€ Part. Bì—ì„œ ë‹¤ë£¨ë„ë¡ í•˜ê² ë‹¤.<br><br></p>
<h3 id="Basic-of-Neural-Network"><a href="#Basic-of-Neural-Network" class="headerlink" title="Basic of Neural Network"></a>Basic of Neural Network</h3><hr>
<p>Neural Networkì˜ ê¸°ë³¸ì„ ì„¤ëª…í• ë•Œ í•­ìƒ ë‚˜ì˜¤ëŠ” ê²ƒì´ ë°”ë¡œ ë‰´ëŸ° êµ¬ì¡° ì‚¬ì§„ì´ë‹¤. ì—¬ê¸°ì„œëŠ” ê·¸ë“¤ì„ ìƒëµí•˜ë„ë¡ í•˜ê² ë‹¤. êµ³ì´ ì—¬ê¸°ì„œ ì„¤ëª…í•  í•„ìš”ë¥¼ ëª» ëŠë¼ê² ë‹¤.<br>ê¶ê¸ˆí•˜ë©´ SLP(Single Layer Perceptron)ì™€ MLP(Multi Layer Perceptron)ì— ëŒ€í•´ì„œ êµ¬ê¸€ë§ì„ í•´ë³´ë„ë¡ í•˜ì.</p>
<p>ìš°ì„  FCNNì„ ë³¸ê²©ì ìœ¼ë¡œ ë“¤ì–´ê°€ê¸° ì „ì—, ì§€ë„ í•™ìŠµì— ëŒ€í•œ Deep learningì˜ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ì— ëŒ€í•œ ê°œëµì ì¸ ëª¨ì‹ë„ë¥¼ ë³´ê³  ê°€ì.</p>
<p align="center"><img src="https://kimh060612.github.io/img/DeepProcess_1.png" width="100%"></p>

<p>ì´ëŸ¬í•œ ëª¨ì‹ë„ì— ê°œëµì ì¸ ìƒëµì´ ë“¤ì–´ ê°”ê³ , ë°˜ë¡€ë„ ì¶©ë¶„íˆ ìˆê² ì§€ë§Œ, ì§€ë„ í•™ìŠµì˜ í˜•íƒœë¡œ í•™ìŠµí•˜ëŠ” ëŒ€ë¶€ë¶„ì˜ Neural NetworkëŠ” ì´ëŸ° Processë¡œ í•™ìŠµì„ í•˜ê²Œ ëœë‹¤.<br>ì•ìœ¼ë¡œ ë‚˜ì˜¤ëŠ” ì‹ ê²½ë§ êµ¬ì¡°ë“¤ ì¤‘ì—ì„œ ìœ„ì˜ â€œì¶œë ¥â€, â€œí‰ê°€â€, â€œí•™ìŠµâ€ì˜ 3ê°€ì§€ì˜ ê³¼ì •ì„ ì§‘ì¤‘ì ìœ¼ë¡œ ê¸°ìˆ í•  ìƒê°ì´ë‹¤. </p>
<p>ê·¸ë ‡ë‹¤ë©´ Neural Network êµ¬ì¡°ë¥¼ í™œìš©í•´ì„œ í’€ ìˆ˜ ìˆëŠ” ë¬¸ì œëŠ” ë¬´ì—‡ì´ ìˆì„ê¹Œ?<br>ì—¬ê¸°ì„œëŠ” í¬ê²Œ 2ê°€ì§€ë§Œ ë‹¤ë£° ì˜ˆì •ì´ë‹¤. </p>
<ol>
<li><em>Classification</em>: ì–´ë–¤ ì…ë ¥ì„ íŠ¹ì • ë²”ì£¼ë¡œ ë¶„ë¥˜í•´ë‚´ëŠ” ë¬¸ì œ. </li>
<li><em>Regression</em>: ì–´ë–¤ ë³€ìˆ˜ë“¤ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ë„ì¶œí•´ë‚´ëŠ” ë¬¸ì œ. (Modelingì— ê°€ê¹ë‹¤ê³  ìƒê°í•˜ë©´ í¸í•¨.)</li>
</ol>
<p>ì´ëŸ¬í•œ 2ê°œì˜ ë¬¸ì œì˜ ë²”ì£¼ë¥¼ ì„¤ëª…í•˜ê³  ë„˜ì–´ê°€ëŠ” ì´ìœ ëŠ” Neural Networkë¡œ ì–´ë–¤ ë¬¸ì œë¥¼ í’€ ê²ƒì´ëƒì— ë”°ë¼ì„œ í•™ìŠµ ê³¼ì •ì—ì„œ ë‹¤ì–‘í•œ ë¶€ë¶„ì´ ìƒì´í•˜ê²Œ ë³€í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ì´ëŠ” ë‚˜ì¤‘ì— ë”°ë¡œ ìì„¸í•˜ê²Œ ë‹¤ë£° ì˜ˆì •ì´ë‹¤. ì§€ê¸ˆì€ ì¼ë‹¨ ì´ëŸ° ê²ƒì´ ìˆë‹¤ ë¼ëŠ” ê²ƒë§Œ ì•Œì•„ë‘ê³  ê°€ì.</p>
<p><br></p>
<h3 id="Definition-of-FCNN"><a href="#Definition-of-FCNN" class="headerlink" title="Definition of FCNN"></a>Definition of FCNN</h3><hr>
<p>ì—¬ê¸°ì„œëŠ” Fully Connected Nueral Networkì˜ ì •ì˜ë¥¼ ë¨¼ì € ì•Œì•„ë³´ë„ë¡ í•˜ê² ë‹¤. ê°„ë‹¨í•˜ê²Œ ì´ë¥¼ í‘œí˜„í•˜ìë©´ ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°ë¥¼ ê°€ì§€ëŠ” Neural Networkë¥¼ ì¼ì»«ëŠ”ë‹¤.</p>
<p align="center"><img src="https://kimh060612.github.io/img/FCNN.png" width="100%"></p>

<p><em>Figure 1: FCNNì˜ ëª¨ì‹ë„ 1</em><br>ì´ë ‡ê²Œ ì•ì¸µì˜ ë‰´ëŸ°ê³¼ ë’·ì¸µì˜ ë‰´ëŸ°ì´ ë¹ ì§ì—†ì´ ì „ë¶€ ì—°ê²°ëœ êµ¬ì¡°ë¥¼ FCNNì´ë¼ê³  í•œë‹¤. ì—¬ê¸°ì„œ ë§Œì•½ ë…ìê°€ MLPë¥¼ ëª¨ë¥´ëŠ” ìƒíƒœë¼ë©´ ìœ„ì˜ êµ¬ì¡°ê°€ ìƒë‹¹íˆ ì¶”ìƒì ìœ¼ë¡œ ë‹¤ê°€ì˜¬ ê²ƒì´ë‹¤. ê°„ë‹¨í•˜ê²Œ ìƒê°í•´ì„œ, ê° ë‰´ëŸ°ë“¤ì€ ìˆ«ìë¥¼ ê°€ì§€ê³  ìˆë‹¤ê³  ìƒê°í•˜ë©´ ëœë‹¤. ë‹¤ìŒ ê·¸ë¦¼ì„ ë³´ë©´ í›¨ì”¬ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë‹¤.</p>
<p align="center"><img src="https://kimh060612.github.io/img/FCNN2.png" width="100%"></p>

<p><em>Figure 2: FCNNì˜ ëª¨ì‹ë„ 2</em></p>
<p>ì´ë ‡ê²Œ ì¸µì˜ ë‰´ëŸ°ë§ˆë‹¤ ê°„ì„ ìœ¼ë¡œ ì—°ê²°ë˜ì–´ ìˆê³  ê° ë‰´ëŸ°ë“¤ì€ ë’¤ì˜ ë‰´ëŸ°ë“¤ì—ê²Œ ì–´ë– í•œ ê°€ì¤‘ì¹˜ë¥¼ ë°˜ì˜í•´ì„œ ìì‹ ì˜ ê°’ì„ ê°€ì§€ê³  ìˆë‹¤ê³  ìƒê°í•˜ë©´ ëœë‹¤. ì´ì œë¶€í„° ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ êµ¬ì¡°ë¥¼ ê°€ì§„ Neural Networkê°€ ì–´ë–»ê²Œ ì£¼ì–´ì§„ ë¬¸ì œì— ëŒ€í•´ì„œ ê²°ê³¼ë¥¼ ë„ì¶œí•˜ê³  í•™ìŠµì„ ì§„í–‰í•˜ëŠ”ì§€ ì•Œì•„ë³´ë„ë¡ í•˜ê² ë‹¤. </p>
<p><br></p>
<h3 id="Feed-Forward-of-FCNN"><a href="#Feed-Forward-of-FCNN" class="headerlink" title="Feed Forward of FCNN"></a>Feed Forward of FCNN</h3><hr>
<p>ì´ ê³¼ì •ì€ í•™ìŠµì´ ë˜ì—ˆê±´ ë˜ì§€ ì•Šì•˜ê±´ FCNNì˜ êµ¬ì¡°ì—ì„œ ì…ë ¥ì„ í†µí•´ ì¶œë ¥ì„ ì–»ì–´ë‚´ëŠ” ê³¼ì •ì´ë‹¤. ì¸µì¸µì´ ìŒ“ì¸ Neural Network êµ¬ì¡°ì—ì„œ ì•ìœ¼ë¡œ ê³„ì† ë‚˜ì•„ê°€ë©´ì„œ ê²°ê³¼ë¥¼ ë„ì¶œí•´ ë‚´ëŠ” ëª¨ìŠµì—ì„œ Feed Forward(ì• ë¨¹ì„)ì´ë¼ëŠ” ì´ë¦„ì´ ë¶™ì€ ê²ƒì´ë‹¤.<br>ì•ìœ¼ë¡œ ëŒ€ë¶€ë¶„ì˜ ì˜ˆì‹œëŠ” <em>Figure 2</em>ì™€ ë¹„ìŠ·í•œ í˜•ì‹ìœ¼ë¡œ ë‚˜ì˜¬ ê²ƒì´ë‹¤. ê·¸ë¦¬ê³  ì•ìœ¼ë¡œëŠ” <em>ë‰´ëŸ°</em>ì´ë¼ëŠ” ëª…ì¹­ë³´ë‹¤ëŠ” <em>Unit</em>ì´ë¼ëŠ” ëª…ì¹­ì„ ì‚¬ìš©í•  ê²ƒì´ë‹¤.<br>ì´ì œ, ì´ì „ ìŠ¬ë¼ì´ë“œì— ìˆëŠ” ê²ƒë“¤ì„ ê¸°í˜¸ë¥¼ ëª…í™•í•˜ê²Œ ì •ì˜í•´ ë³´ë„ë¡ í•˜ê² ë‹¤.</p>
<blockquote>
<p><em>Definition 1</em></p>
</blockquote>
<ul>
<li>$x_i$: ì…ë ¥ ë²¡í„°ì˜ $i$ë²ˆì§¸ ì›ì†Œ</li>
<li>$u_i^k$: $k$ë²ˆì§¸ Layerì˜ $i$ë²ˆì§¸ Hidden Unitsì˜ ê°’.</li>
<li>$w^k_{ij}$: $k$ë²ˆì§¸ ì¸µì˜ $i$ë²ˆì§¸ Unitê³¼ $k-1$ë²ˆì§¸ ì¸µì˜ $j$ë²ˆì§¸ Unitì„ ì´ì–´ì£¼ëŠ” ê°€ì¤‘ì¹˜</li>
<li>$y_i$: ì¶œë ¥ì¸µì˜ $i$ë²ˆì§¸ Unit</li>
<li>$n_k$: $k$ë²ˆì§¸ ì¸µì˜ Unitì˜ ê°¯ìˆ˜</li>
</ul>
<p>ê·¸ë ‡ë‹¤ë©´ ì´ì œ Feed Forwardë¥¼ ìœ„í•´ì„œ ê°€ì¥ ì¤‘ìš”í•œ ë¶€ë¶„ì„ ì„¤ëª…í•˜ë„ë¡ í•˜ê² ë‹¤. Weighted Sum(ê°€ì¤‘í•©)ì— ëŒ€í•œ ë¶€ë¶„ì´ë‹¤.<br>ì•ì„  <em>Figure 1</em>ê³¼ <em>Figure 2</em>ì—ì„œ ì•ì¸µê³¼ ë’·ì¸µì˜ ê´€ê³„ë¥¼ ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•œ ê²ƒì´ë‹¤.</p>
<script type="math/tex; mode=display">
u_i^k = \sum_{j=1}^{n^{k-1}}w^k_{ij}u^{k-1}_j
\tag{Equation (2)}</script><p>ì •ë§ ë‹¨ìˆœí•˜ê²Œ ìƒê°í•´ì„œ ì´ì „ ìœ ë‹›ì— í•´ë‹¹í•˜ëŠ” ê°€ì¤‘ì¹˜ë§Œí¼ ì•ì˜ ìœ ë‹›ì— ë°˜ì˜í•´ ì£¼ë©´ ë˜ëŠ” ê²ƒì´ë‹¤. ì´ë ‡ê²Œ ë’¤ì˜ ì¸µì—ì„œ ì•ì˜ ì¸µìœ¼ë¡œ ìˆœì°¨ì ìœ¼ë¡œ ê³„ì‚°í•´ ë‚˜ê°€ë©´ ëœë‹¤.</p>
<p>í•˜ì§€ë§Œ ì—¬ê¸°ì—ëŠ” ì¹˜ëª…ì ì¸ ë¬¸ì œì ì´ ìˆë‹¤. ë°”ë¡œ, ì´ë ‡ê²Œ ëœë‹¤ë©´ Neural Network êµ¬ì¡°ë¡œ Linear Function ë°–ì— í‘œí˜„í•  ìˆ˜ ì—†ë‹¤ëŠ” ê²ƒì´ë‹¤.<br>ìœ„ì˜  $equation (1)$ì˜ í˜•ì‹ìœ¼ë¡œ Feed Fowardë¥¼ ì§„í–‰í•˜ê²Œ ëœë‹¤ë©´ ì¸µì„ ìŒ“ëŠ” ê²ƒì´ ì˜ë¯¸ê°€ ì—†ì–´ì§„ë‹¤. ê·¸ ì´ìœ ëŠ” ë‹¨ìˆœíˆ ì¼ë³€ìˆ˜ ì„ í˜• í•¨ìˆ˜ë¥¼ ìƒê°í•´ë³´ë©´ ì•Œ ìˆ˜ ìˆë‹¤.<br>ìš°ë¦¬ê°€ $y = a_ix$ë¼ëŠ” ì„ì˜ì˜ $N$ê°œì˜ í•¨ìˆ˜ë“¤ì„ ì¸µì¸µì´ ìŒ“ëŠ”ë‹¤ í• ì €ì–¸ì • ê·¸ê²ƒì€ ìƒˆë¡œìš´ ì„ í˜• í•¨ìˆ˜ $y = aâ€™x, \quad aâ€™ = a_1a_2a_3\cdot\cdot\cdot a_N$ ë¥¼ ë§Œë“¤ì–´ ë‚´ëŠ” ê²ƒê³¼ ë³„ë°˜ ë‹¤ë¥¼ ê²ƒì´ ì—†ë‹¤. ì´ë ‡ê²Œ ë˜ë©´ ê°€ì¥ ëŒ€í‘œì ì¸ ë¬¸ì œì ì€ ë°”ë¡œ XORë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ì—†ë‹¤ëŠ” ê²ƒì´ë‹¤. ì´ëŠ” ë„ˆë¬´ ë³´í¸ì ì¸ ë¬¸ì œë¼ì„œ ê·¸ëƒ¥ êµ¬ê¸€ë§ í•˜ë©´ ë‚˜ë³´ë‹¤ ì„¤ëª… ì˜í•´ë†“ì€ ê¸€ì´ ë„ë ¤ìˆë‹¤. ë”°ë¼ì„œ ì—¬ê¸°ì„œëŠ” ìƒëµí•œë‹¤.<br>ë˜‘ê°™ì€ ë…¼ë¦¬ì´ë‹¤. ì´ì— ëŒ€í•œ ìì„¸í•œ ë…¼ì˜ëŠ” Appendix. Aì— ìì„¸íˆ ê¸°ìˆ í•˜ë„ë¡ í•˜ê² ë‹¤.</p>
<p>ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³  ë” ë‹¤ì–‘í•œ í•¨ìˆ˜ë¥¼ ì‹ ê²½ë§ì´ í‘œí˜„í•  ìˆ˜ ìˆê²Œ í•˜ê¸° ìœ„í•´ì„œ Activation functionê³¼ Biasë¥¼ ì ìš©í•˜ê²Œ ëœë‹¤.</p>
<blockquote>
<p>Activation Function(í™œì„± í•¨ìˆ˜)</p>
</blockquote>
<p>ì‹ ê²½ë§ì´ Linearí•œ í•¨ìˆ˜ë§Œì„ ê·¼ì‚¬ì‹œí‚¤ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ì„œ ì¤‘ê°„ì— Non Linear Functionì„ Unitì— ì ìš©í•˜ê²Œ ëœë‹¤. ì´ë•Œ ì´ëŸ¬í•œ í•¨ìˆ˜ë¥¼ Non linear functionì´ë¼ê³  í•œë‹¤. ê·¸ë ‡ë‹¤ê³ , ì „ë¶€ Non linearì¸ ê²ƒì€ ì•„ë‹ˆë‹¤. í•˜ì§€ë§Œ Non linear functionì´ì—¬ì•¼ ì•ì„  ë¬¸ì œê°€ ë°œìƒí•˜ì§€ ì•Šì„ ê²ƒì´ë‹¤. ê·¸ ì¢…ë¥˜ëŠ” ëŒ€ëµ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>
<ol>
<li>Sigmoid: $\sigma(x) = \frac{1}{1+\exp(-x)}$</li>
<li>ReLU(Rectified Linear Unit): $max(0, x)$</li>
<li>Leaky ReLU: $max(0.1x, x)$</li>
<li>Hyperbolic tangent: $\tanh(x) = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(-x)}$</li>
<li>ELU(Exponential Linear Unit): $\sigma(x) = xu(x) + a(\exp(x)-1)u(-x)$</li>
</ol>
<p>ì´ê²ƒ ë¿ë§Œ ì•„ë‹ˆë¼ ì§„ì§œ ê°œ ë§ë‹¤. ë‚˜ë¨¸ì§€ëŠ” Tensorflow ê³µì‹ Documentë‚˜ êµ¬ê¸€ë§ì„ í†µí•´ì„œ ì•Œì•„ë³´ë„ë¡ í•˜ë¼. </p>
<p>ì´ë ‡ê²Œ Activation Functionì„ ì”Œìš´ ê°’ì„ ì•ìœ¼ë¡œëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•˜ë„ë¡ í•˜ê² ë‹¤.</p>
<blockquote>
<p><em>Defintion 2</em>: í™œì„±í•¨ìˆ˜ë¥¼ ì ìš©í•œ Unitì˜ ê°’.</p>
</blockquote>
<script type="math/tex; mode=display">z_i^k = f(u^k_i)</script><blockquote>
<p>Bias (í¸í–¥)</p>
</blockquote>
<p>Biasë¡œì¨ëŠ” Non Linearì„±ì„ ì‹ ê²½ë§ì— ì¶”ê°€í•  ìˆ˜ëŠ” ì—†ë‹¤. í•˜ì§€ë§Œ Activation Functionì„ í‰í–‰ ì´ë™ ì‹œì¼œì£¼ëŠ” ì—­í• ì„ í•˜ê²Œ í•´ì„œ ì…ë ¥ì´ 0ì¸ ì§€ì ì—ì„œì˜ ì‹ ê²½ë§ì˜ ììœ¨ë„ë¥¼ ë†’ì—¬ì£¼ê²Œ ëœë‹¤. ì˜ˆë¥¼ ë“¤ì–´ë³´ì. ìœ„ì— ì˜ˆì‹œë¥¼ ë“  Activation functionë“¤ì—ì„œëŠ” ì˜ˆë¥¼ ë“¤ì–´ì„œ $(0, 32)$ê°™ì€ ì ì„ í‘œí˜„í•  ìˆ˜ ì—†ë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— Biasë¥¼ ì£¼ì–´ì„œ í‰í–‰ ì´ë™ì„ ì‹œì¼œì£¼ëŠ” ê²ƒì´ë‹¤. ì´ê²ƒì˜ ìœ ë¬´ê°€ ì‹ ê²½ë§ì˜ í•™ìŠµ ì–‘ìƒê³¼ ì„±ëŠ¥ì´ í¬ê²Œ ì°¨ì´ë¥¼ ì£¼ëŠ” ê²½ìš°ê°€ ë§ë‹¤. </p>
<p>ì¦‰, ìµœëŒ€í•œ ë§ì€ í•¨ìˆ˜ë¥¼ ê·¼ì‚¬í•  ìˆ˜ ìˆë„ë¡ ìœ„ì™€ ê°™ì€ ì„¤ì •ì„ ì¶”ê°€í•´ ì£¼ëŠ” ê²ƒì´ë‹¤.<br>ì´ëŸ¬ë©´ ì–´ë–¤ ì¼ë¯¼ì´ëŠ” ì´ëŸ° ì§ˆë¬¸ì„ í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.</p>
<blockquote>
<p>???: ì•„ ã…‹ã…‹ã…‹ã…‹ ê·¸ëŸ° ê·¼ì‚¬ ëª» í•˜ëŠ” í•¨ìˆ˜ëŠ” ì–´ì©”ê±´ë°ìš” ã…‹ã…‹ë£¨ì‚¥ë½•</p>
</blockquote>
<p>ê±±ì • ë§ˆë¼. ì´ë¯¸ <em>ë§ŒëŠ¥ ê·¼ì‚¬ ì •ë¦¬</em>(<em>Universal Approximation Theorem</em>, <em>ì‹œë²¤ì½” ì •ë¦¬</em>)ì— ì˜í•´ì„œ ì¦ëª…ë˜ì—ˆë‹¤. ê·¸ëƒ¥ ì•ˆì‹¬í•˜ê³  ì“°ë©´ ëœë‹¤.</p>
<p>ê·¸ë ‡ë‹¤ë©´ Biasë¥¼ ì ìš©í•´ì„œ ë‹¤ì‹œê¸ˆ Unitì˜ ê°’ì„ ì‘ì„±í•´ ë³´ì.</p>
<blockquote>
<p><em>Definition 3</em>: Biasë¥¼ ì ìš©í•œ Unitì˜ ê°’  </p>
</blockquote>
<script type="math/tex; mode=display">
u_i^k = \sum_{j=1}^{n_{k-1}}w^k_{ij}z^{k-1}_j + b^k_i
\tag{Equation (2)}</script><p><br></p>
<p align="center"><img src="https://kimh060612.github.io/img/FCNN3.png" width="100%"></p>

<p><em>Figure 3: Biasë¥¼ ë°˜ì˜í•œ Neural Networkì˜ í‘œí˜„</em></p>
<p>ì´ì œ ì´ë¥¼ í–‰ë ¬ë¡œ í‘œí˜„í•´ë³´ì. í–‰ë ¬ë¡œì¨ í‘œí˜„í•˜ì—¬ ì‹ì„ ì§§ê³  ê°„í¸í•˜ê²Œ ì •ë¦¬í•  ìˆ˜ ìˆìœ¼ë©°, ì»´í“¨í„°ì—ì„œì˜ êµ¬í˜„ì„ ë‹¤ì†Œ ì§ê´€ì ì´ê³  í¸í•˜ê²Œ í•  ìˆ˜ ìˆë‹¤.</p>
<script type="math/tex; mode=display">
U^k = 
\begin{bmatrix}
    u^k_1 && u^k_2 && \dots && u^k_{n_k}
\end{bmatrix} 
^T
\tag{Def. 4}</script><p><br></p>
<script type="math/tex; mode=display">
W^k = \begin{bmatrix}
    w^k_{11} && w^k_{12} && \cdots && w^k_{1n_{k-1}} \\
    w^k_{21} && w^k_{22} && \cdots && w^k_{2n_{k-1}} \\
    \vdots && \ddots && \ddots && \vdots \\
    w^k_{n_k1} && w^k_{n_k2} && \cdots && w^k_{n_kn_{k-1}} \\
\end{bmatrix}
\tag{Def. 5}</script><p><br></p>
<script type="math/tex; mode=display">
Z^{k-1} = 
\begin{bmatrix}
    z^{k-1}_1 && z^{k-1}_2 && \dots && z^{k-1}_{n_{k-1}}
\end{bmatrix}^T
\tag{Def. 6}</script><p><br></p>
<script type="math/tex; mode=display">
B^k = 
\begin{bmatrix}
    b^k_1 && b^k_2 && \dots && b^k_{n_k}
\end{bmatrix}^T
\tag{Def. 7}</script><p><br></p>
<script type="math/tex; mode=display">
\therefore U^k = W^kZ^{k-1} + B^k
\quad
(Z^k = f(U^k), \quad where \space f: \mathbb{R}^{n^k} \to \mathbb{R}^{n^k})</script><p><br></p>
<h3 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h3><hr>
<p>ì´ë²ˆ ì¥ì—ì„œëŠ” Gradient Descentì— ëŒ€í•´ì„œ ë‹¤ë£° ì˜ˆì •ì´ë‹¤. í•˜ì§€ë§Œ ë„ˆë¬´ ê¹Šê²ŒëŠ” ì•ˆ ë‹¤ë£° ìƒê°ì´ë‹¤. ê·¸ë ‡ë‹¤ê³  ê±±ì •í•  í•„ìš”ëŠ” ì—†ë‹¤. ë‚˜ì¤‘ì— ì¡´ë‚˜ ìì„¸í•˜ê²Œ ë‹¤ë£°ê±°ë‹ˆê¹ ê±±ì •ì€ ë¶™ë“¤ì–´ ë§¤ë„ë¡ í•˜ì. </p>
<p align="center"><img src="https://kimh060612.github.io/img/GradientDescent.gif" width="100%"></p>

<p><em>Figure 4. Gradient Descentì„ ë¬˜ì‚¬í•˜ëŠ” Figure</em><br>Gradient DescentëŠ” ê°„ë‹¨í•˜ê²Œ ì„¤ëª…í•´ì„œ Gradientë¥¼ í™œìš©í•´ì„œ Objective Function(ëª©ì  í•¨ìˆ˜)ì˜ ìµœì €ì ì„ ì°¾ëŠ” ê²ƒì´ ëª©í‘œì´ë‹¤. ìœ„ì˜ ì‚¬ì§„ì²˜ëŸ¼ ë§ì´ë‹¤. ì´ì˜ ëª…í™•í•œ í‘œí˜„ì„ ì§ê´€ì ìœ¼ë¡œ ë‚©ë“ì‹œí‚¤ê¸° ìœ„í•´ì„œ ë‹¤ìŒì˜ ì‚¬ì§„ì„ ë³´ì.</p>
<p align="center"><img src="https://kimh060612.github.io/img/GD.jpg" width="70%"></p>

<p><em>Figure 5. Gradient Descentë¥¼ ì§ê´€ì ìœ¼ë¡œ ì„¤ëª…í•˜ê¸° ìœ„í•œ ê·¸ë¦¼</em></p>
<p>ì´ì²˜ëŸ¼ ëª©ì  í•¨ìˆ˜ì˜ ë¯¸ë¶„ê°’ì„ í™œìš©í•˜ì—¬ ëª©ì  í•¨ìˆ˜ì˜ ìµœì €ì ì„ ì°¾ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤. ì´ì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ì€ ì§„ì§œ ë‚˜ì¤‘ì— ì§ˆë¦¬ë„ë¡ í•´ì£¼ë„ë¡ í•˜ê² ë‹¤. ê±±ì •í•˜ì§€ ë§ì•„ë‹¬ë¼.</p>
<p>ê·¸ë ‡ë‹¤ë©´ ì´ì œ Neural Networkì—ì„œ ëª©ì  í•¨ìˆ˜ëŠ” ì–´ë–¤ ê²ƒì„ ì‚¬ìš©í•˜ëŠ”ì§€ ì•Œì•„ë´ì•¼í•œë‹¤. ì´ê²ƒë„ ì§€ê¸ˆì€ ê·¸ëƒ¥ â€œê·¸ëŸ° ê²ƒì´ ìˆë‹¤â€ë¼ê³ ë§Œ ìƒê°í•˜ì. ì´ íŒŒíŠ¸ëŠ” ì‹ ê²½ë§ì„ ì´í•´í•˜ëŠ”ë° ìˆì–´ì„œ ì¤‘ìš”í•˜ê³  ì‹¬ë„ìˆëŠ” ë‚´ìš©ì´ê¸° ë•Œë¬¸ì— ë‚˜ì¤‘ì— Post í•˜ë‚˜ë¥¼ í†µìœ¼ë¡œ ì—´ì–´ì„œ ì§‘ì¤‘ì ìœ¼ë¡œ ì„¤ëª…í•  ê²ƒì´ë‹¤.</p>
<p>ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” ëª©ì í•¨ìˆ˜ë¡œì¨ ì£¼ë¡œ 2ê°€ì§€ë¥¼ ë‹¤ë£° ê²ƒì´ë‹¤.</p>
<ol>
<li>Mean Squared Error</li>
</ol>
<script type="math/tex; mode=display">
E = \frac{1}{2N}\sum_i(y_i - t_i)^2
\tag{Equation (3)}</script><ol>
<li>Cross-Entropy</li>
</ol>
<script type="math/tex; mode=display">
E = -\sum_c t_c\ln y_c
\tag{Equation (4)}</script><p>$Equation 3$ì€ ì£¼ë¡œ regression ë¬¸ì œì—ì„œ ëª©ì  í•¨ìˆ˜ë¡œ ì“°ì´ê³ , $Equation 4$ëŠ” ì£¼ë¡œ classification ë¬¸ì œì—ì„œ ì“°ì¸ë‹¤.<br>ì´ë ‡ê²Œ Error functionì´ ìˆëŠ”ë° Updateë¥¼ ì–´ë–»ê²Œ í•œë‹¤ê³  í•˜ëŠ” ê²ƒì¼ê¹Œ? ë‹¤ìŒê³¼ ê°™ì€ ê³µì‹ìœ¼ë¡œ Update í•˜ë©´ ëœë‹¤</p>
<script type="math/tex; mode=display">
w_{t+1} \gets w_t - \alpha \frac{\partial E}{\partial w_t}
\quad
\alpha: learning \space rate
\tag{Equation (5)}</script><p>ì§ê´€ì ìœ¼ë¡œ ìƒê°í•´ë³´ë©´, ê·¸ë˜í”„ì˜ ë¯¸ë¶„ê°’ì´ ìŒìˆ˜ì´ë©´ ì•„ë˜ë¡œ ë³¼ë¡í•œ ì´ì°¨ í•¨ìˆ˜ì—ì„œ ê·¹ì†Œì ì´ ì˜¤ë¥¸ìª½ì— ìˆë‹¤ëŠ” ê²ƒì´ë‹¤. ê·¸ë ‡ë‹¤ë©´ ë³€ìˆ˜ì— ì–‘ìˆ˜ë¥¼ ë”í•´ì¤˜ì•¼ ê·¹ì†Œì ì— ë‹¤ê°€ê°ˆ ê²ƒì´ë‹¤. ë°˜ëŒ€ë„ ë§ˆì°¬ê°€ì§€ì´ë‹¤. ê·¸ë¦¬ê³  í•œë²ˆ Updateë¥¼ í• ë•Œ ë„ˆë¬´ ë³€í™”í­ì„ ê¸‰í•˜ê²Œ í•˜ì§€ ì•Šê¸° ìœ„í•´ì„œ learning rateë¥¼ ê³±í•´ì¤˜ì„œ updateë¥¼ í•´ì¤€ë‹¤. ê·¸ë ‡ê²Œ ì ì ˆí•œ learning rateë¥¼ ì§€ì •í•˜ì—¬ ëª‡ë²ˆ ë°˜ë³µí•´ì£¼ë©´ ìµœì €ì ì— ìˆ˜ë ´í•´ ìˆì„ ê²ƒì´ë‹¤.</p>
<p>ì§‘ìš”í•˜ì§€ë§Œ, ì´ë²ˆ ì£¼ì œì— ê´€í•œ ë‚´ìš©ì€ ë‚˜ì¤‘ì— â€œOptimization, Error function, and Problemâ€ì— ê´€í•œ í¬ìŠ¤íŠ¸ì—ì„œ ìì„¸í•˜ê²Œ ë‹¤ë£° ê²ƒì´ë‹¤. ê·¸ëŸ¬ë‹ˆ ì§€ê¸ˆì€ ì§ê´€ì ìœ¼ë¡œë§Œ í•™ìŠµì˜ Processë¥¼ ì´í•´í•˜ëŠ”ë° ì‚¬ë ¥ì„ ë‹¤í•´ì£¼ê¸°ë¥¼ ë°”ë€ë‹¤.</p>
<h3 id="Back-propagation-of-FCNN"><a href="#Back-propagation-of-FCNN" class="headerlink" title="Back-propagation of FCNN"></a>Back-propagation of FCNN</h3><hr>
<p>ì´ë²ˆì—ëŠ” ì–´ë–»ê²Œ ì‹ ê²½ë§ì— Gradient Descentë¥¼ ì ìš©í•  ê²ƒì¸ì§€ì— ëŒ€í•œ ë‚´ìš©ì´ë‹¤. ìœ„ì˜ Gradient Descentë¥¼ ë°°ìš´ ì‚¬ëŒì´ë¼ë©´ <script type="math/tex">Equation 5</script>ë¥¼ í™œìš©í•˜ì—¬ ê°€ì¤‘ì¹˜ì˜ Updateë¥¼ ë‹¤ìŒê³¼ ê°™ì´ í•´ì¤˜ì•¼ í•  ê²ƒì´ë¼ëŠ”ë° ì´ê²¬ì„ ê°€ì§€ì§€ëŠ” ì•Šì„ ê²ƒì´ë‹¤.</p>
<script type="math/tex; mode=display">
w^m_{ij} \gets w^m_{ij} - \alpha \frac{\partial E}{\partial w^m_{ij}}
\tag{Equation (6)}
\quad
, m \in [1, K] \cap \N</script><p>ì—¬ê¸°ì„œ $K$ëŠ” ì¸µì˜ ê°¯ìˆ˜ì´ë‹¤. </p>
<p>ì´ë•Œ ìˆ˜ì¹˜í•´ì„ì„ ê³µë¶€í•œ ì‚¬ëŒì´ë¼ë©´, ë‹¤ìŒê³¼ ê°™ì€ ë§ì„ í•  ìˆ˜ ìˆë‹¤.</p>
<blockquote>
<p>ì ë‹¹í•œ $\epsilon$ì„ ì„ íƒí•˜ê³ , ìˆ˜ì¹˜ ë¯¸ë¶„ì„ í•´ì£¼ë©´ ë˜ì§€ ì•Šì„ê¹Œ?</p>
</blockquote>
<p>ë­, í‹€ë¦° ë§ì€ ì•„ë‹ˆë‹¤. ì‹¤ì œë¡œë„ ì¢‹ì€ ë°©ë²•ì´ ë  ìˆ˜ ìˆë‹¤.(<em>ì‹ ê²½ë§ì´ ì•„ë‹ˆë¼ë©´ ë§ì´ì§€</em>) í•˜ì§€ë§Œ ì´ê±´ Neural Networkì´ë‹¤. ë¯¸ì¹œ ì—°ì‚°ëŸ‰ì„ ìë‘í•˜ëŠ” ì´ìª½ ë°”ë‹¥ì—ì„œ ì•ˆì´í•˜ê²Œ ì ‘ê·¼í–ˆë‹¤ê°€ëŠ” í”¼ë¥¼ ë³¼ ìˆ˜ ìˆë‹¤.<br>ìš”ì¦˜ì€ ë§ì€ ì•Œê³ ë¦¬ì¦˜ë“¤ì´ ë°œë‹¬í•´ì„œ ì–´ë–¨ì§€ëŠ” ëª¨ë¥´ì§€ë§Œ, ê¸°ë³¸ì ì¸ ìˆ˜ì¹˜ ë¯¸ë¶„ì€ ì—¬ê¸°ì„œ ì¢‹ì€ ë°©ë²•ì´ ì•„ë‹ˆë‹¤. ìš°ì„  ìˆ˜ì¹˜ ë¯¸ë¶„ì„ ì§„í–‰í•˜ëŠ” ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>
<script type="math/tex; mode=display">
    \frac{df}{dx} \approx \frac{f(x+\epsilon) - f(x)}{\epsilon}</script><p>ìš°ì„  ì´ê±¸ ì‹ ê²½ë§ì— ì ìš©ì‹œí‚¤ê¸° ìœ„í•´ ê°€ì¥ ë¨¼ì € ë– ì˜¤ë¥´ëŠ” ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ì„ ê²ƒì´ë‹¤.</p>
<ol>
<li>$f(x+\epsilon)$ë¥¼ ê³„ì‚°í•œë‹¤.</li>
<li>$f(x)$ë¥¼ ê³„ì‚°í•œë‹¤.</li>
<li>ë¯¸ë¶„ì„ ê³„ì‚°í•œë‹¤.</li>
</ol>
<p>ì, 1,2ë²ˆì˜ ê³¼ì •ì„ ì‹ ê²½ë§ì— ì ìš©í•´ ì£¼ë ¤ë©´ ë‹¹ì‹ ì€ 2ë²ˆì˜ í•¨ìˆ˜ ê°’ì„ ê³„ì‚°í•´ ì¤˜ì•¼í•œë‹¤. ë§Œì•½, ê³„ì‚°ëŸ‰ì´ í¬ì§€ ì•Šì€ í•¨ìˆ˜ë¼ë©´ ë¹¨ë¦¬ ë˜ê² ì§€ë§Œ, ì´ê±´ ì‹ ê²½ë§ì´ë‹¤. <em>Figure 3</em>ì„ ë³´ì. ì €ê¸° ê·¸ë ¤ì ¸ ìˆëŠ” ëª¨ë“  ê°„ì„ ì— ëŒ€í•´ì„œ Updateë¥¼ í•´ì¤˜ì•¼í•˜ëŠ”ë° ì§„ì§œ ì–´ë¦¼ë„ ì—†ëŠ” ë°©ë²•ì´ ì•„ë‹ ìˆ˜ ì—†ë‹¤. ì´ê±¸ parameterë§ˆë‹¤ í•´ì¤€ë‹¤? ì§„ì§œ ì–´ë¦¼ë„ ì—†ëŠ” ì†Œë¦¬ê°€ ëœë‹¤.</p>
<p>ê·¸ë ‡ë‹¤ë©´ ë‹¤ìŒìœ¼ë¡œ íŒ¨ê¸° ë„˜ì¹˜ëŠ” ì¼ë¯¼ì´ê°€ ë‹¤ìŒê³¼ ê°™ì´ ë§í•œë‹¤.</p>
<blockquote>
<p>ê·¸ëƒ¥ ê°„ë‹¨í•œ í•¨ìˆ˜ë§Œ ì“°ê³  ì†ìœ¼ë¡œ ê³„ì‚°í•˜ë©´ ì•ˆë¨? ê·¸ê²ƒë„ ëª»í•˜ëˆ„ ã…‹ã…‹ë£¨ì‚¥ë½•</p>
</blockquote>
<p>^^ ì—´ì‹¬íˆ í•´ë³´ì‹­ì…” ì¼ë¯¼ë‹˜^^(<em>ë§ë„ ì•ˆë˜ëŠ” ì†Œë¦¬ëŠ” ì§‘ì–´ ì¹˜ìš°ì</em>)</p>
<p>ì—¬ê¸°ì„œ tensorflowì˜ ìë™ ë¯¸ë¶„(Autograd)ê°€ ë‚˜ì˜¤ëŠ”ë°, ìš°ë¦¬ëŠ” ì´ ì „ì— ìˆ˜í•™ì ì¸ ë¶€ë¶„ìœ¼ë¡œ ì´ë¥¼ ì ‘ê·¼í•´ ë³´ë„ë¡í•˜ì.</p>
<p>ê°€ì¥ ë¨¼ì €, ì´ëŠ” ì—„ì²­ë‚˜ê²Œ ê³‚ê³‚ì´ ìŒ“ì¸ í•¨ìˆ˜ì´ë‹¤. ì¼ë³€ìˆ˜ Scalar í•¨ìˆ˜ë¡œ ì˜ˆì‹œë¥¼ ë“¤ì–´ë³´ë©´ ë‹¤ìŒê³¼ ë¹„ìŠ·í•œ ê²ƒì´ë¼ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤.</p>
<script type="math/tex; mode=display">y = f(f(f( \cdots f(x) \cdots)))</script><p>ì´ë¥¼ ë¯¸ë¶„í•´ì•¼í•œë‹¤ê³  ìƒê°í•´ë³´ì. Calculusë¥¼ ë°°ì› ë‹¤ë©´ ê°€ì¥ ë¨¼ì € ë– ì˜¤ë¥´ëŠ” ê²ƒì€ Chain Rule ì¼ ê²ƒì´ë‹¤.</p>
<p>í•˜ì§€ë§Œ, ì–´ë””ì„œë¶€í„° ì–´ë–»ê²Œ Chain Ruleì„ ì ìš©í•´ì•¼í•˜ëŠ”ì§€ ë§‰ë§‰í•˜ë‹¤. ê·¸ ì „ì— ìš°ì„  <strong>ìƒëŒ€ì ìœ¼ë¡œ êµ¬íˆê¸° ì‰¬ìš¸ ê²ƒ ê°™ì€</strong> ì¶œë ¥ì¸µ ë°”ë¡œ ì´ì „ì˜ ê°€ì¤‘ì¹˜ì˜ ë¯¸ë¶„ì„ Chaine Ruleì„ í†µí•´ì„œ êµ¬í•´ë³´ì.</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial w^K_{ij}} = \frac{\partial E}{\partial y_i}\frac{\partial y_i}{\partial u^K_i}\frac{\partial u^K_i}{\partial w^K_{ij}}
\tag{Equation (6)}</script><p>ì´ëŠ” ì¶œë ¥ì¸µ ë°”ë¡œ ì´ì „ì˜ ì¸µì˜ ê°€ì¤‘ì¹˜ì´ê¸°ì—, Chain Ruleì„ í†µí•´ì„œ <strong>ë¹„êµì  ì‰½ê²Œ</strong> êµ¬í•  ìˆ˜ ìˆë‹¤. ë‹¤ìŒê³¼ ê°™ì´ ë§ì´ë‹¤.<br>(Error function(ëª©ì  í•¨ìˆ˜)ëŠ” MSEë¥¼ ì‚¬ìš©í•˜ì˜€ë‹¤ê³  ìƒê°í•˜ì.)</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial y_i} = \frac{1}{N}(y_i - t_i)
\quad  
\frac{\partial y_i}{\partial u^K_i} = f'(u^K_i)
\quad
\frac{\partial u^K_i}{\partial w^K_{ij}} = z^{K-1}_j
\tag{Equation (7)}</script><script type="math/tex; mode=display">
\therefore \frac{\partial E}{\partial w^K_{ij}} = \frac{1}{N}(y_i - t_i)f'(u^K_i)z^{K-1}_j
\tag{Equation (8)}</script><p>ì´ë ‡ê²Œ ì„±ê³µì ìœ¼ë¡œ $W^K$ì˜ ë¯¸ë¶„ì€ êµ¬í–ˆë‹¤ê³  ì¹˜ê³ , ì–´ë–»ê²Œ ê·¸ ë’¤ì˜ ì¸µì„ êµ¬í•  ê²ƒì¸ê°€?<br>ìš°ì„ , ë‹¤ìŒ ê·¸ë¦¼ì„ ë³´ì.</p>
<p align="center"><img src="https://kimh060612.github.io/img/Layer.png" width="70%"></p>

<p><em>Figure 7. Neural Network êµ¬ì¡°ì—ì„œ ì¼ë¶€ë¥¼ ë–¼ì–´ì„œ í‘œí˜„í•œ ê·¸ë¦¼</em><br>ì´ëŠ” Neural Network êµ¬ì¡°ì—ì„œ ì¼ë¶€ë¥¼ ë–¼ì–´ë‚¸ ê²ƒì„ í‘œí˜„í•œ ê·¸ë¦¼ì´ë‹¤. ìœ„ì˜ $\vdots$ê°€ ê·¸ë ¤ì§„ Unitì€ ê·¸ëƒ¥ ê±°ê¸°ì— ë§ì€ ìˆ˜ì˜ Unitë“¤ì´ ìˆë‹¤ê³  ìƒê°í•˜ë©´ ë  ê²ƒì´ë‹¤.<br>ìš°ë¦¬ëŠ” ì—¬ê¸°ì„œ $W^k$ì˜ $E$ì— ëŒ€í•œ ë¯¸ë¶„ì„ êµ¬í•˜ëŠ” ê²ƒì´ ëª©í‘œì´ë‹¤.<br>ì¦‰, $Equation (9)$ë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ ëª©í‘œì´ë‹¤.</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial w^k_{ip}} \space = \space ?</script><p>ê·¸ê²ƒì„ ìœ„í•´ì„œ ìš°ì„ , Chaine Ruleì„ ì§„í–‰í•´ ë³´ì.</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial w^k_{ip}} = \frac{\partial E}{\partial u^k_i}\frac{\partial u^k_i}{\partial w^k_{ip}}
\tag{Equation (9)}</script><p>ì—¬ê¸°ê¹Œì§€ë§Œ ë´ì„œëŠ” ì˜ ëª¨ë¥´ê² ë‹¤. í•˜ì§€ë§Œ ë‹¤ìŒì˜ ê·¸ë¦¼ì„ ë³´ê³ , $\frac{\partial E}{\partial u^k_i}$ ì— í•œë²ˆ ë” Chain Ruleì„ ì ìš©í•´ ë³´ì.</p>
<p align="center"><img src="https://kimh060612.github.io/img/Layer2.png" width="70%"></p>

<p><em>Figure 6. $u^k_i$ê°€ ì˜í–¥ì„ ì£¼ëŠ” ì• ì¸µì˜ Unitë“¤</em><br>FCNNì˜ ì •ì˜ì— ì˜í•´, $u^k_i$ëŠ” ì•ì¸µì˜ ëª¨ë“  ë‰´ëŸ°ì— ì—°ê²°ë˜ì–´ ìˆë‹¤. ê·¸ë¦¬ê³  ê·¸ ì• ì¸µì˜ ë‰´ëŸ°ë“¤ì€ ë˜ ê·¸ ë‹¤ìŒì¸µìœ¼ë¡œ ê°’ì„ ì „ë‹¬í•˜ë©°, ì ì  ì¶œë ¥ì¸µì— ê°€ê¹Œì›Œ ì§€ê³ , Errorì— ì˜í–¥ì„ ë¯¸ì¹˜ê²Œ ëœë‹¤. í•œë§ˆë””ë¡œ, ë‹¤ìŒê³¼ ê°™ì´ Chain Ruleì„ í¼ì¹  ìˆ˜ ìˆë‹¤.</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial u^k_i} = \sum_{j=1}^{n_{k+1}} \frac{\partial E}{\partial u^{k+1}_j}\frac{\partial u^{k+1}_j}{\partial u^k_i}
\tag{Equation (10)}</script><p>ì´ë•Œ ìš°ë¦¬ëŠ” ìœ„ì˜ $Equation (10)$ì—ì„œ ìœ ì‚¬í•œ ë¶€ë¶„ì„ ì°¾ì„ ìˆ˜ ìˆë‹¤.</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial u^k_i} \quad and \quad \frac{\partial E}{\partial u^{k+1}_j}</script><p>ê·¸ë ‡ë‹¤ë©´ ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ë¥¼ í•´ë³´ì.</p>
<script type="math/tex; mode=display">
\delta^k_i = \frac{\partial E}{\partial u^k_i}
\tag{Def. 8}</script><p>ê·¸ë ‡ë‹¤ë©´ $Equation (10)$ì„ ë‹¤ìŒê³¼ ê°™ì´ ì‘ì„±í•  ìˆ˜ ìˆë‹¤.</p>
<script type="math/tex; mode=display">
\delta^k_i = \sum_{j=1}^{n_{k+1}} \delta^{k+1}_j\frac{\partial u^{k+1}_j}{\partial u^k_i}
\tag{Equation (11)}</script><p><strong>ì´ ì í™”ì‹ì´ ì´ë²ˆ í¬ìŠ¤íŠ¸ì˜ í•µì‹¬ì´ë‹¤.</strong><br>ê·¸ë ‡ë‹¤ë©´ ì‹ì„ ë‹¤ì‹œ ì •ë¦¬í•´ì„œ, ìµœì¢…ì ì¸ ë¯¸ë¶„ ê°’ì„ êµ¬í•´ë³´ì.</p>
<script type="math/tex; mode=display">
\frac{\partial u^{k+1}_j}{\partial u^k_i} = \frac{\partial u^{k+1}_j}{\partial z^k_i}\frac{\partial z^{k}_i}{\partial u^k_i} = w^{k+1}_{ji}f'(u^k_i)
\tag{Equation (12)}</script><p>ê·¸ë¦¬ê³ , $Equation (12)$ë¥¼ í†µí•´ì„œ $Equation (10)$ì„ ë‹¤ì‹œ ì¨ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>
<script type="math/tex; mode=display">
\delta^k_i = \sum_{j=1}^{n_{k+1}} \delta^{k+1}_jw^{k+1}_{ji}f'(u^k_i)
\tag{Equation (13)}</script><p>ì! ë‹¤ ì™”ë‹¤. ì´ì œ ìµœì¢… ë¯¸ë¶„ì‹ì„ ë‹¤ì‹œ ì¨ë³´ì.</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial w^k_{ip}} = \delta^k_i\frac{\partial u^k_i}{\partial w^k_{ip}} = \left(\sum_{j=1}^{n_{k+1}} \delta^{k+1}_jw^{k+1}_{ji}f'(u^k_i)\right )\frac{\partial u^k_i}{\partial w^k_{ip}}</script><script type="math/tex; mode=display">
\therefore \frac{\partial E}{\partial w^k_{ip}} = \left(\sum_{j=1}^{n_{k+1}} \delta^{k+1}_jw^{k+1}_{ji}f'(u^k_i)\right )z^{k-1}_p
\tag{Equation (14)}</script><p>ì´ í¬ìŠ¤íŠ¸ì˜ í•µì‹¬ì€ ì´ê²ƒì´ë‹¤.<br>ê²°êµ­ ìš°ë¦¬ëŠ” <strong>ìƒëŒ€ì ìœ¼ë¡œ êµ¬í•˜ê¸° ì‰¬ìš´ ì¶œë ¥ì¸µì˜ ë¯¸ë¶„ì„ êµ¬í•˜ëŠ” ê³¼ì •ì—ì„œ ë¯¸ë¶„ì„ êµ¬í•˜ê¸° ì–´ë ¤ìš´ ì¸µì˜ ë¯¸ë¶„ì„ êµ¬í•  ìˆ˜ ìˆëŠ” ì‹¤ë§ˆë¦¬ë¥¼ ì–»ì€ ê²ƒì´ë‹¤.</strong><br>ë¬´ìŠ¨ ë§ì´ëƒ? ìš°ë¦¬ëŠ” $\delta^k_i$ë¥¼ ì •ì˜í–ˆë‹¤ë©´, ì¶œë ¥ì¸µì˜ $\delta^K$ë„ êµ¬í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. ê·¸ë ‡ë‹¤ë©´ $K-1$ì¸µì˜ $\delta^{K-1}$ëŠ” $\delta^K$ë¥¼ í†µí•´ì„œ êµ¬í•  ìˆ˜ ìˆìœ¼ë‹ˆ, ê²°êµ­ $W^{K-1}$ì¸µì˜ ë¯¸ë¶„ë„ êµ¬í•  ìˆ˜ ìˆë‹¤ëŠ” ëœ»ì´ ëœë‹¤.</p>
<p>ë³¸ê²©ì ìœ¼ë¡œ $\delta^K$ë¥¼ êµ¬í•´ë³´ì.</p>
<script type="math/tex; mode=display">
\delta^K_i = \frac{\partial E}{\partial y_i}\frac{\partial y_i}{\partial u^K_i} = \frac{1}{N}(y_i - t_i)f'(u^K_i)</script><p>ê·¸ë ‡ë‹¤. ë°”ë¡œ êµ¬í•  ìˆ˜ ìˆë‹¤. ê·¸ë ‡ë‹¤ë©´ ì´ë¥¼ í†µí•´ì„œ ë’¤ì— ìˆëŠ” ì¸µì˜ ë¯¸ë¶„ì„ ì°¨ë¡€ë¡œ êµ¬í•  ìˆ˜ ìˆëŠ” ê²ƒì´ë‹¤.<br>Feed Forwardì™€ëŠ” ë‹¬ë¦¬, ì´ ê³¼ì •ì€ ë°©í–¥ì´ ë°˜ëŒ€ë¡œ ì§„í–‰ëœë‹¤. ë”°ë¼ì„œ ì´ëŠ” Back Propagationì´ë¼ê³  í•˜ë©°, ì´ë ‡ê²Œ $\delta$ë¥¼ ì •ì˜í•˜ì—¬ ë¯¸ë¶„ì„ êµ¬í•˜ëŠ” ë°©ì‹ì„ <strong>ì¼ë°˜í™”ëœ ë¸íƒ€ ê·œì¹™</strong>ì´ë¼ê³  í•œë‹¤. </p>
<p>ì, Weightì˜ ë¯¸ë¶„ì€ êµ¬í–ˆìœ¼ë‹ˆ, Biasì˜ ë¯¸ë¶„ë„ êµ¬í•´ì•¼í•œë‹¤. ì´ëŠ” ë§¤ìš° ê°„ë‹¨í•˜ë‹¤.</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial b^k_i} = \frac{\partial E}{\partial u^k_i}\frac{\partial u^k_i}{\partial b^k_i} = \delta^k_i\cdot 1
\tag{Equation (15)}</script><p>ì´ë ‡ê²Œ ìš°ë¦¬ëŠ” $\delta$ì— ê´€í•œ ê°’ë§Œ êµ¬í•˜ë©´ ì‹ ê²½ë§ì„ Update í•˜ê¸° ìœ„í•œ ë¯¸ë¶„ì„ ì „ë¶€ êµ¬í•  ìˆ˜ ìˆë‹¤. ê·¸ë ‡ê¸°ì—, <em>í•µì‹¬</em>ì´ë¼ê³  í•  ìˆ˜ ìˆëŠ” ë°©ì •ì‹ $Def. 8$, $Equation (13)$, $Equation (15)$, $Equation (14)$ì— ëŒ€í•´ì„œ í–‰ë ¬ë¡œì¨ í‘œí˜„í•´ ë³´ì.</p>
<p><em>Definition of Delta</em></p>
<script type="math/tex; mode=display">
\Delta^k = \nabla_UE = \nabla_ZE \odot f'(U^k)
\tag{Def. 8}</script><p><em>Delta Matrix Relation</em></p>
<script type="math/tex; mode=display">
\Delta^k = \left( (W^{k+1})^T\Delta^k \right) \odot f'(U^k)
\tag{Equation (13)}</script><p><em>Bias Update Equation</em></p>
<script type="math/tex; mode=display">
\nabla_B E = \Delta^k
\tag{Equation (15)}</script><p><em>Weight Update Equation</em></p>
<script type="math/tex; mode=display">
\nabla_WE = \Delta^kZ^{k-1}
\tag{Equation (14)}</script><p>ì´ê±¸ë¡œ FCNNì˜ Feed Forward, Back Propagationì˜ ì´ë¡ ì ì¸ ë¶€ë¶„ì´ ëë‚¬ë‹¤. ì´ì œ ì‹¤ìŠµì€ Part. Bì—ì„œ ë§ˆì € ë‹¤ë£° ì˜ˆì •ì´ë‹¤.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Michael Kim</p>
  <div class="site-description" itemprop="description">Backend Engineering, Deep learning, Algorithm & DS, CS ì „ê³µ ì§€ì‹ ê³µë¶€ ì €ì¥ì†Œ</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">19</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/kimh060612" title="GitHub â†’ https:&#x2F;&#x2F;github.com&#x2F;kimh060612" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:kimh060612@khu.ac.kr" title="E-Mail â†’ mailto:kimh060612@khu.ac.kr" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Michael Kim</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://michael-blog-1.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>

</body>
</html>
